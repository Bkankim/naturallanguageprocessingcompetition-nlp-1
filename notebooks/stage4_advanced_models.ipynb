{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 4: Advanced Models\n",
    "\n",
    "고급 모델 실험 및 최적화 기법 적용\n",
    "\n",
    "## 목표\n",
    "1. 다양한 사전학습 모델 실험 (mBART, Qwen 등)\n",
    "2. LoRA/QLoRA 파라미터 효율적 미세조정\n",
    "3. 앙상블 기법 적용\n",
    "4. 최종 제출 파일 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load best config from Stage 3\n",
    "with open('../configs/best_config_stage3.json', 'r') as f:\n",
    "    BEST_CONFIG = json.load(f)\n",
    "\n",
    "CONFIG = {\n",
    "    **BEST_CONFIG,\n",
    "    \n",
    "    # Experiment\n",
    "    \"exp_num\": \"004\",\n",
    "    \"exp_name\": \"advanced-model\",\n",
    "    \n",
    "    # Model options\n",
    "    \"models_to_test\": [\n",
    "        \"gogamza/kobart-base-v2\",\n",
    "        \"facebook/mbart-large-50\",\n",
    "        # \"Qwen/Qwen2.5-7B-Instruct\",  # Requires more memory\n",
    "    ],\n",
    "    \n",
    "    # LoRA configuration\n",
    "    \"use_lora\": True,\n",
    "    \"lora_r\": 8,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"lora_target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "    \n",
    "    # Ensemble\n",
    "    \"ensemble_method\": \"voting\",  # voting, averaging, weighted\n",
    "    \"ensemble_weights\": None,  # Auto-calculated from validation scores\n",
    "    \n",
    "    # Paths\n",
    "    \"checkpoint_dir\": \"../checkpoints/advanced\",\n",
    "    \"submission_dir\": \"../submissions\",\n",
    "}\n",
    "\n",
    "print(\"✅ Config loaded\")\n",
    "print(f\"📊 Best ROUGE from Stage 3: {BEST_CONFIG.get('best_rouge_sum', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Custom utilities\n",
    "from utils import (\n",
    "    set_seed,\n",
    "    setup_wandb,\n",
    "    compute_rouge,\n",
    "    auto_git_backup\n",
    ")\n",
    "\n",
    "# Set seed\n",
    "set_seed(CONFIG[\"seed\"])\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CONFIG[\"checkpoint_dir\"], exist_ok=True)\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"🖥️ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "train_df = pd.read_csv(os.path.join(CONFIG[\"data_dir\"], \"train_processed.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(CONFIG[\"data_dir\"], \"test_processed.csv\"))\n",
    "\n",
    "# Use cleaned text\n",
    "train_df = train_df[['fname', 'dialogue_clean', 'summary_clean']].rename(\n",
    "    columns={'dialogue_clean': 'dialogue', 'summary_clean': 'summary'}\n",
    ")\n",
    "test_df = test_df[['fname', 'dialogue_clean']].rename(\n",
    "    columns={'dialogue_clean': 'dialogue'}\n",
    ")\n",
    "\n",
    "# Train/validation split\n",
    "train_data, val_data = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.1,\n",
    "    random_state=CONFIG[\"seed\"]\n",
    ")\n",
    "\n",
    "print(f\"📊 Train: {len(train_data):,}, Val: {len(val_data):,}, Test: {len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lora_model(base_model, config):\n",
    "    \"\"\"LoRA 설정으로 모델을 래핑합니다.\"\"\"\n",
    "    lora_config = LoraConfig(\n",
    "        r=config[\"lora_r\"],\n",
    "        lora_alpha=config[\"lora_alpha\"],\n",
    "        target_modules=config[\"lora_target_modules\"],\n",
    "        lora_dropout=config[\"lora_dropout\"],\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"✅ LoRA helper function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, config, train_dataset, val_dataset, tokenizer):\n",
    "    \"\"\"단일 모델을 학습합니다.\"\"\"\n",
    "    print(f\"\\n🚀 Training: {model_name}\")\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Apply LoRA if enabled\n",
    "    if config[\"use_lora\"]:\n",
    "        model = create_lora_model(model, config)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    model_short_name = model_name.split(\"/\")[-1]\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=os.path.join(config[\"checkpoint_dir\"], model_short_name),\n",
    "        \n",
    "        # Use best config from Stage 3\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        per_device_train_batch_size=config[\"batch_size\"],\n",
    "        per_device_eval_batch_size=config[\"batch_size\"] * 2,\n",
    "        gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
    "        num_train_epochs=config[\"num_train_epochs\"],\n",
    "        warmup_ratio=config[\"warmup_ratio\"],\n",
    "        \n",
    "        # Optimization\n",
    "        fp16=config[\"fp16\"],\n",
    "        gradient_checkpointing=True,\n",
    "        \n",
    "        # Evaluation\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=config[\"eval_steps\"],\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=config[\"save_steps\"],\n",
    "        logging_steps=config[\"logging_steps\"],\n",
    "        \n",
    "        # Prediction\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=config[\"max_target_length\"],\n",
    "        generation_num_beams=config[\"num_beams\"],\n",
    "        \n",
    "        # Saving\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_rouge_sum\",\n",
    "        greater_is_better=True,\n",
    "        \n",
    "        # WandB\n",
    "        report_to=\"wandb\",\n",
    "        run_name=f\"{config['exp_name']}-{model_short_name}\",\n",
    "        \n",
    "        # Misc\n",
    "        seed=config[\"seed\"],\n",
    "    )\n",
    "    \n",
    "    # Compute metrics\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        result = compute_rouge(\n",
    "            predictions=decoded_preds,\n",
    "            references=decoded_labels,\n",
    "            use_korean_tokenizer=True\n",
    "        )\n",
    "        return result\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_results = trainer.evaluate()\n",
    "    rouge_sum = eval_results['eval_rouge_sum']\n",
    "    \n",
    "    # Save model\n",
    "    final_model_path = os.path.join(config[\"checkpoint_dir\"], f\"{model_short_name}_final\")\n",
    "    trainer.save_model(final_model_path)\n",
    "    \n",
    "    print(f\"✅ {model_name} training complete\")\n",
    "    print(f\"📊 ROUGE sum: {rouge_sum:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"model_path\": final_model_path,\n",
    "        \"rouge_sum\": rouge_sum,\n",
    "        \"eval_results\": eval_results\n",
    "    }\n",
    "\n",
    "print(\"✅ Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and special tokens\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"models_to_test\"][0])\n",
    "\n",
    "special_tokens_path = os.path.join(CONFIG[\"config_dir\"], \"special_tokens.json\")\n",
    "if os.path.exists(special_tokens_path):\n",
    "    with open(special_tokens_path, 'r', encoding='utf-8') as f:\n",
    "        special_tokens_config = json.load(f)\n",
    "    base_tokenizer.add_special_tokens({\n",
    "        'additional_special_tokens': special_tokens_config['additional_special_tokens']\n",
    "    })\n",
    "\n",
    "# Preprocess datasets\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = base_tokenizer(\n",
    "        examples['dialogue'],\n",
    "        max_length=CONFIG[\"max_input_length\"],\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    labels = base_tokenizer(\n",
    "        examples['summary'],\n",
    "        max_length=CONFIG[\"max_target_length\"],\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data.reset_index(drop=True))\n",
    "val_dataset = Dataset.from_pandas(val_data.reset_index(drop=True))\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=['dialogue', 'summary'])\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True, remove_columns=['dialogue', 'summary'])\n",
    "\n",
    "print(\"✅ Datasets preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "trained_models = []\n",
    "\n",
    "for model_name in CONFIG[\"models_to_test\"]:\n",
    "    try:\n",
    "        # Train model\n",
    "        result = train_model(\n",
    "            model_name=model_name,\n",
    "            config=CONFIG,\n",
    "            train_dataset=train_dataset,\n",
    "            val_dataset=val_dataset,\n",
    "            tokenizer=base_tokenizer\n",
    "        )\n",
    "        trained_models.append(result)\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to train {model_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✅ Trained {len(trained_models)} models successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": result[\"model_name\"].split(\"/\")[-1],\n",
    "        \"ROUGE-1\": result[\"eval_results\"][\"eval_rouge1\"],\n",
    "        \"ROUGE-2\": result[\"eval_results\"][\"eval_rouge2\"],\n",
    "        \"ROUGE-L\": result[\"eval_results\"][\"eval_rougeL\"],\n",
    "        \"ROUGE Sum\": result[\"rouge_sum\"]\n",
    "    }\n",
    "    for result in trained_models\n",
    "])\n",
    "\n",
    "results_df = results_df.sort_values(\"ROUGE Sum\", ascending=False)\n",
    "\n",
    "print(\"\\n📊 Model Comparison:\")\n",
    "display(results_df)\n",
    "\n",
    "# Best model\n",
    "best_model_result = trained_models[0]\n",
    "print(f\"\\n🏆 Best Model: {best_model_result['model_name']}\")\n",
    "print(f\"📊 ROUGE Sum: {best_model_result['rouge_sum']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Summary\n",
    "\n",
    "**완료된 작업**:\n",
    "- ✅ Stage 4 노트북 기본 구조 생성\n",
    "- ✅ LoRA/QLoRA 파라미터 효율적 미세조정 지원\n",
    "- ✅ 다중 모델 학습 파이프라인\n",
    "- ✅ 모델 비교 및 평가\n",
    "\n",
    "**다음 단계**:\n",
    "- Task #2: 고급 모델 실험 (context7 활용)\n",
    "- Task #5: 최종 앙상블 및 제출"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
