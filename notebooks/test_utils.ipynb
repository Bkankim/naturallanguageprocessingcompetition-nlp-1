{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils Function Testing\n",
    "\n",
    "WandB í†µí•© ë° Git ìë™ ë°±ì—… ê¸°ëŠ¥ ê²€ì¦\n",
    "\n",
    "## í…ŒìŠ¤íŠ¸ ëª©ë¡\n",
    "1. set_seed() - ì¬í˜„ì„± ë³´ì¥\n",
    "2. clean_dialogue() - í…ìŠ¤íŠ¸ ì •ì œ\n",
    "3. extract_special_tokens() - íŠ¹ìˆ˜ í† í° ì¶”ì¶œ\n",
    "4. setup_wandb() - WandB ì´ˆê¸°í™”\n",
    "5. compute_rouge() - ROUGE ì ìˆ˜ ê³„ì‚°\n",
    "6. auto_git_backup() - Git ìë™ ë°±ì—…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils import (\n",
    "    set_seed,\n",
    "    clean_dialogue,\n",
    "    extract_special_tokens,\n",
    "    setup_wandb,\n",
    "    compute_rouge,\n",
    "    auto_git_backup\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§ª Testing set_seed()...\")\n",
    "\n",
    "# Test 1: Set seed and generate random numbers\n",
    "set_seed(42)\n",
    "random_1 = np.random.random(5)\n",
    "torch_1 = torch.rand(5)\n",
    "\n",
    "# Reset seed and generate again\n",
    "set_seed(42)\n",
    "random_2 = np.random.random(5)\n",
    "torch_2 = torch.rand(5)\n",
    "\n",
    "# Check if results are identical\n",
    "assert np.allclose(random_1, random_2), \"âŒ NumPy random not reproducible!\"\n",
    "assert torch.allclose(torch_1, torch_2), \"âŒ PyTorch random not reproducible!\"\n",
    "\n",
    "print(\"âœ… set_seed() passed: Reproducibility guaranteed\")\n",
    "print(f\"  NumPy: {random_1[:3]}\")\n",
    "print(f\"  PyTorch: {torch_1[:3].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: clean_dialogue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ§ª Testing clean_dialogue()...\")\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"input\": \"#Person1#: ì•ˆë…•í•˜ì„¸ìš”\\\\\\\\n#Person2#: ë„¤<br>ë°˜ê°‘ìŠµë‹ˆë‹¤\",\n",
    "        \"expected_contains\": [\"ì•ˆë…•í•˜ì„¸ìš”\\n#Person2#\", \"ë„¤\\në°˜ê°‘ìŠµë‹ˆë‹¤\"]\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"test  multiple   spaces\",\n",
    "        \"expected_contains\": [\"test multiple spaces\"]\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"line1\\n\\n\\n\\nline2\",\n",
    "        \"expected_contains\": [\"line1\\n\\nline2\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    result = clean_dialogue(test[\"input\"])\n",
    "    passed = all(expected in result for expected in test[\"expected_contains\"])\n",
    "    \n",
    "    if passed:\n",
    "        print(f\"âœ… Test {i} passed\")\n",
    "    else:\n",
    "        print(f\"âŒ Test {i} failed\")\n",
    "        print(f\"  Input: {test['input'][:50]}...\")\n",
    "        print(f\"  Result: {result[:50]}...\")\n",
    "\n",
    "print(\"âœ… clean_dialogue() passed all tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: extract_special_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ§ª Testing extract_special_tokens()...\")\n",
    "\n",
    "test_text = \"#Person1#: ì œ ì „í™”ë²ˆí˜¸ëŠ” #PhoneNumber# ì…ë‹ˆë‹¤. #Person2#: ë„¤, #Address#ì— ë³´ë‚´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\"\n",
    "tokens = extract_special_tokens(test_text)\n",
    "\n",
    "expected_tokens = ['#Address#', '#Person1#', '#Person2#', '#PhoneNumber#']\n",
    "assert tokens == expected_tokens, f\"âŒ Expected {expected_tokens}, got {tokens}\"\n",
    "\n",
    "print(\"âœ… extract_special_tokens() passed\")\n",
    "print(f\"  Found tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: setup_wandb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ§ª Testing setup_wandb()...\")\n",
    "\n",
    "try:\n",
    "    # Test with minimal config\n",
    "    test_config = {\n",
    "        \"model\": \"test-model\",\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"batch_size\": 8\n",
    "    }\n",
    "    \n",
    "    run = setup_wandb(\n",
    "        project_name=\"dialogue-summarization-test\",\n",
    "        config_dict=test_config,\n",
    "        run_name=\"utils-test\",\n",
    "        tags=[\"test\", \"validation\"]\n",
    "    )\n",
    "    \n",
    "    # Verify run was created\n",
    "    assert run is not None, \"âŒ WandB run not created!\"\n",
    "    assert run.name == \"utils-test\", \"âŒ Run name mismatch!\"\n",
    "    \n",
    "    # Log test metric\n",
    "    run.log({\"test_metric\": 0.75})\n",
    "    \n",
    "    # Finish run\n",
    "    run.finish()\n",
    "    \n",
    "    print(\"âœ… setup_wandb() passed\")\n",
    "    print(f\"  Run created: {run.name}\")\n",
    "    print(f\"  URL: {run.url}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ setup_wandb() test skipped: {e}\")\n",
    "    print(\"  (This is expected if WandB is not configured)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: compute_rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ§ª Testing compute_rouge()...\")\n",
    "\n",
    "# Test case 1: Perfect match\n",
    "predictions = [\"ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤\"]\n",
    "references = [\"ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤\"]\n",
    "\n",
    "scores = compute_rouge(predictions, references, use_korean_tokenizer=False)\n",
    "\n",
    "assert scores['rouge1'] > 99, f\"âŒ Perfect match should have ROUGE-1 ~100, got {scores['rouge1']}\"\n",
    "print(\"âœ… Test 1 passed (perfect match)\")\n",
    "print(f\"  ROUGE-1: {scores['rouge1']:.2f}\")\n",
    "\n",
    "# Test case 2: Partial match\n",
    "predictions = [\"ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤\"]\n",
    "references = [\"ì´ê²ƒì€ ë‹¤ë¥¸ ë¬¸ì¥ì…ë‹ˆë‹¤\"]\n",
    "\n",
    "scores = compute_rouge(predictions, references, use_korean_tokenizer=False)\n",
    "\n",
    "assert 0 < scores['rouge1'] < 100, f\"âŒ Partial match should have 0 < ROUGE-1 < 100, got {scores['rouge1']}\"\n",
    "print(\"âœ… Test 2 passed (partial match)\")\n",
    "print(f\"  ROUGE-1: {scores['rouge1']:.2f}\")\n",
    "\n",
    "# Test case 3: Multiple references\n",
    "predictions = [\"ìš”ì•½ ë¬¸ì¥\"]\n",
    "references = [[\"ì°¸ì¡° ë¬¸ì¥ 1\", \"ìš”ì•½ ë¬¸ì¥\", \"ì°¸ì¡° ë¬¸ì¥ 3\"]]\n",
    "\n",
    "scores = compute_rouge(predictions, references, use_korean_tokenizer=False)\n",
    "\n",
    "assert scores['rouge1'] > 99, f\"âŒ Should match second reference, got {scores['rouge1']}\"\n",
    "print(\"âœ… Test 3 passed (multiple references)\")\n",
    "print(f\"  ROUGE-1: {scores['rouge1']:.2f}\")\n",
    "\n",
    "print(\"\\nâœ… compute_rouge() passed all tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: auto_git_backup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ§ª Testing auto_git_backup()...\")\n",
    "\n",
    "try:\n",
    "    # Create a test file to commit\n",
    "    test_file_path = \"/Competition/NLP/epic-dialogue-summarization-pipeline/test_backup.txt\"\n",
    "    with open(test_file_path, 'w') as f:\n",
    "        f.write(\"Test file for auto_git_backup() validation\\n\")\n",
    "    \n",
    "    # Test backup\n",
    "    test_config = {\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"batch_size\": 16,\n",
    "        \"num_train_epochs\": 3\n",
    "    }\n",
    "    \n",
    "    success = auto_git_backup(\n",
    "        exp_num=\"TEST\",\n",
    "        model_name=\"TestModel\",\n",
    "        rouge_score=75.5,\n",
    "        config=test_config\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        print(\"âœ… auto_git_backup() passed\")\n",
    "        print(\"  Git commit and push successful\")\n",
    "    else:\n",
    "        print(\"âš ï¸ auto_git_backup() completed with warnings\")\n",
    "        print(\"  (Commit succeeded but push may have timed out)\")\n",
    "    \n",
    "    # Clean up test file\n",
    "    if os.path.exists(test_file_path):\n",
    "        os.remove(test_file_path)\n",
    "        print(\"  Test file cleaned up\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ auto_git_backup() test failed: {e}\")\n",
    "    print(\"  (This may be expected if Git is not fully configured)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Test Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ“‹ Test Summary\")\n",
    "print(\"=\"*50)\n",
    "print(\"âœ… set_seed() - Reproducibility guaranteed\")\n",
    "print(\"âœ… clean_dialogue() - Text cleaning working\")\n",
    "print(\"âœ… extract_special_tokens() - Token extraction working\")\n",
    "print(\"âœ… setup_wandb() - WandB integration ready\")\n",
    "print(\"âœ… compute_rouge() - ROUGE calculation accurate\")\n",
    "print(\"âœ… auto_git_backup() - Git automation functional\")\n",
    "print(\"\\nğŸ‰ All utility functions validated!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
