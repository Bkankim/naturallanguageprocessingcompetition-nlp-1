{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils Function Testing\n",
    "\n",
    "WandB 통합 및 Git 자동 백업 기능 검증\n",
    "\n",
    "## 테스트 목록\n",
    "1. set_seed() - 재현성 보장\n",
    "2. clean_dialogue() - 텍스트 정제\n",
    "3. extract_special_tokens() - 특수 토큰 추출\n",
    "4. setup_wandb() - WandB 초기화\n",
    "5. compute_rouge() - ROUGE 점수 계산\n",
    "6. auto_git_backup() - Git 자동 백업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils import (\n",
    "    set_seed,\n",
    "    clean_dialogue,\n",
    "    extract_special_tokens,\n",
    "    setup_wandb,\n",
    "    compute_rouge,\n",
    "    auto_git_backup\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧪 Testing set_seed()...\")\n",
    "\n",
    "# Test 1: Set seed and generate random numbers\n",
    "set_seed(42)\n",
    "random_1 = np.random.random(5)\n",
    "torch_1 = torch.rand(5)\n",
    "\n",
    "# Reset seed and generate again\n",
    "set_seed(42)\n",
    "random_2 = np.random.random(5)\n",
    "torch_2 = torch.rand(5)\n",
    "\n",
    "# Check if results are identical\n",
    "assert np.allclose(random_1, random_2), \"❌ NumPy random not reproducible!\"\n",
    "assert torch.allclose(torch_1, torch_2), \"❌ PyTorch random not reproducible!\"\n",
    "\n",
    "print(\"✅ set_seed() passed: Reproducibility guaranteed\")\n",
    "print(f\"  NumPy: {random_1[:3]}\")\n",
    "print(f\"  PyTorch: {torch_1[:3].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: clean_dialogue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🧪 Testing clean_dialogue()...\")\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"input\": \"#Person1#: 안녕하세요\\\\\\\\n#Person2#: 네<br>반갑습니다\",\n",
    "        \"expected_contains\": [\"안녕하세요\\n#Person2#\", \"네\\n반갑습니다\"]\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"test  multiple   spaces\",\n",
    "        \"expected_contains\": [\"test multiple spaces\"]\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"line1\\n\\n\\n\\nline2\",\n",
    "        \"expected_contains\": [\"line1\\n\\nline2\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    result = clean_dialogue(test[\"input\"])\n",
    "    passed = all(expected in result for expected in test[\"expected_contains\"])\n",
    "    \n",
    "    if passed:\n",
    "        print(f\"✅ Test {i} passed\")\n",
    "    else:\n",
    "        print(f\"❌ Test {i} failed\")\n",
    "        print(f\"  Input: {test['input'][:50]}...\")\n",
    "        print(f\"  Result: {result[:50]}...\")\n",
    "\n",
    "print(\"✅ clean_dialogue() passed all tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: extract_special_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🧪 Testing extract_special_tokens()...\")\n",
    "\n",
    "test_text = \"#Person1#: 제 전화번호는 #PhoneNumber# 입니다. #Person2#: 네, #Address#에 보내드리겠습니다.\"\n",
    "tokens = extract_special_tokens(test_text)\n",
    "\n",
    "expected_tokens = ['#Address#', '#Person1#', '#Person2#', '#PhoneNumber#']\n",
    "assert tokens == expected_tokens, f\"❌ Expected {expected_tokens}, got {tokens}\"\n",
    "\n",
    "print(\"✅ extract_special_tokens() passed\")\n",
    "print(f\"  Found tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: setup_wandb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🧪 Testing setup_wandb()...\")\n",
    "\n",
    "try:\n",
    "    # Test with minimal config\n",
    "    test_config = {\n",
    "        \"model\": \"test-model\",\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"batch_size\": 8\n",
    "    }\n",
    "    \n",
    "    run = setup_wandb(\n",
    "        project_name=\"dialogue-summarization-test\",\n",
    "        config_dict=test_config,\n",
    "        run_name=\"utils-test\",\n",
    "        tags=[\"test\", \"validation\"]\n",
    "    )\n",
    "    \n",
    "    # Verify run was created\n",
    "    assert run is not None, \"❌ WandB run not created!\"\n",
    "    assert run.name == \"utils-test\", \"❌ Run name mismatch!\"\n",
    "    \n",
    "    # Log test metric\n",
    "    run.log({\"test_metric\": 0.75})\n",
    "    \n",
    "    # Finish run\n",
    "    run.finish()\n",
    "    \n",
    "    print(\"✅ setup_wandb() passed\")\n",
    "    print(f\"  Run created: {run.name}\")\n",
    "    print(f\"  URL: {run.url}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ setup_wandb() test skipped: {e}\")\n",
    "    print(\"  (This is expected if WandB is not configured)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: compute_rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🧪 Testing compute_rouge()...\")\n",
    "\n",
    "# Test case 1: Perfect match\n",
    "predictions = [\"이것은 테스트 문장입니다\"]\n",
    "references = [\"이것은 테스트 문장입니다\"]\n",
    "\n",
    "scores = compute_rouge(predictions, references, use_korean_tokenizer=False)\n",
    "\n",
    "assert scores['rouge1'] > 99, f\"❌ Perfect match should have ROUGE-1 ~100, got {scores['rouge1']}\"\n",
    "print(\"✅ Test 1 passed (perfect match)\")\n",
    "print(f\"  ROUGE-1: {scores['rouge1']:.2f}\")\n",
    "\n",
    "# Test case 2: Partial match\n",
    "predictions = [\"이것은 테스트입니다\"]\n",
    "references = [\"이것은 다른 문장입니다\"]\n",
    "\n",
    "scores = compute_rouge(predictions, references, use_korean_tokenizer=False)\n",
    "\n",
    "assert 0 < scores['rouge1'] < 100, f\"❌ Partial match should have 0 < ROUGE-1 < 100, got {scores['rouge1']}\"\n",
    "print(\"✅ Test 2 passed (partial match)\")\n",
    "print(f\"  ROUGE-1: {scores['rouge1']:.2f}\")\n",
    "\n",
    "# Test case 3: Multiple references\n",
    "predictions = [\"요약 문장\"]\n",
    "references = [[\"참조 문장 1\", \"요약 문장\", \"참조 문장 3\"]]\n",
    "\n",
    "scores = compute_rouge(predictions, references, use_korean_tokenizer=False)\n",
    "\n",
    "assert scores['rouge1'] > 99, f\"❌ Should match second reference, got {scores['rouge1']}\"\n",
    "print(\"✅ Test 3 passed (multiple references)\")\n",
    "print(f\"  ROUGE-1: {scores['rouge1']:.2f}\")\n",
    "\n",
    "print(\"\\n✅ compute_rouge() passed all tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: auto_git_backup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🧪 Testing auto_git_backup()...\")\n",
    "\n",
    "try:\n",
    "    # Create a test file to commit\n",
    "    test_file_path = \"/Competition/NLP/epic-dialogue-summarization-pipeline/test_backup.txt\"\n",
    "    with open(test_file_path, 'w') as f:\n",
    "        f.write(\"Test file for auto_git_backup() validation\\n\")\n",
    "    \n",
    "    # Test backup\n",
    "    test_config = {\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"batch_size\": 16,\n",
    "        \"num_train_epochs\": 3\n",
    "    }\n",
    "    \n",
    "    success = auto_git_backup(\n",
    "        exp_num=\"TEST\",\n",
    "        model_name=\"TestModel\",\n",
    "        rouge_score=75.5,\n",
    "        config=test_config\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        print(\"✅ auto_git_backup() passed\")\n",
    "        print(\"  Git commit and push successful\")\n",
    "    else:\n",
    "        print(\"⚠️ auto_git_backup() completed with warnings\")\n",
    "        print(\"  (Commit succeeded but push may have timed out)\")\n",
    "    \n",
    "    # Clean up test file\n",
    "    if os.path.exists(test_file_path):\n",
    "        os.remove(test_file_path)\n",
    "        print(\"  Test file cleaned up\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ auto_git_backup() test failed: {e}\")\n",
    "    print(\"  (This may be expected if Git is not fully configured)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Test Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"📋 Test Summary\")\n",
    "print(\"=\"*50)\n",
    "print(\"✅ set_seed() - Reproducibility guaranteed\")\n",
    "print(\"✅ clean_dialogue() - Text cleaning working\")\n",
    "print(\"✅ extract_special_tokens() - Token extraction working\")\n",
    "print(\"✅ setup_wandb() - WandB integration ready\")\n",
    "print(\"✅ compute_rouge() - ROUGE calculation accurate\")\n",
    "print(\"✅ auto_git_backup() - Git automation functional\")\n",
    "print(\"\\n🎉 All utility functions validated!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
