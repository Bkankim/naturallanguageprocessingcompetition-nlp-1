{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Baseline Training (KoBART)\n",
    "\n",
    "KoBART ëª¨ë¸ì„ ì‚¬ìš©í•œ ëŒ€í™” ìš”ì•½ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í•™ìŠµ\n",
    "\n",
    "## ëª©í‘œ\n",
    "1. KoBART ëª¨ë¸ ë¡œë”© ë° ì„¤ì •\n",
    "2. ë°ì´í„°ì…‹ ì¤€ë¹„ (Hugging Face Dataset)\n",
    "3. ëª¨ë¸ í•™ìŠµ (Seq2SeqTrainer)\n",
    "4. ROUGE í‰ê°€\n",
    "5. ì²« ì œì¶œ íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Experiment\n",
    "    \"exp_num\": \"001\",\n",
    "    \"exp_name\": \"kobart-baseline\",\n",
    "    \"seed\": 42,\n",
    "    \n",
    "    # Model\n",
    "    \"model_name\": \"gogamza/kobart-base-v2\",\n",
    "    \"max_input_length\": 512,\n",
    "    \"max_target_length\": 128,\n",
    "    \n",
    "    # Training\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 2,  # Effective batch size = 16\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"fp16\": True,  # Mixed precision\n",
    "    \n",
    "    # Evaluation\n",
    "    \"eval_steps\": 500,\n",
    "    \"save_steps\": 500,\n",
    "    \"logging_steps\": 100,\n",
    "    \n",
    "    # Generation\n",
    "    \"num_beams\": 4,\n",
    "    \"length_penalty\": 1.0,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \n",
    "    # Paths\n",
    "    \"data_dir\": \"../data/processed\",\n",
    "    \"config_dir\": \"../configs\",\n",
    "    \"checkpoint_dir\": \"../checkpoints/baseline\",\n",
    "    \"submission_dir\": \"../submissions\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# Custom utilities\n",
    "from utils import (\n",
    "    set_seed,\n",
    "    setup_wandb,\n",
    "    compute_rouge,\n",
    "    auto_git_backup\n",
    ")\n",
    "\n",
    "# Set seed\n",
    "set_seed(CONFIG[\"seed\"])\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CONFIG[\"checkpoint_dir\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"submission_dir\"], exist_ok=True)\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ–¥ï¸ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¬ WandB Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WandB\n",
    "run = setup_wandb(\n",
    "    project_name=\"dialogue-summarization-competition\",\n",
    "    config_dict=CONFIG,\n",
    "    run_name=f\"{CONFIG['exp_name']}-exp{CONFIG['exp_num']}\",\n",
    "    tags=[\"baseline\", \"kobart\", \"stage2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "train_df = pd.read_csv(os.path.join(CONFIG[\"data_dir\"], \"train_processed.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(CONFIG[\"data_dir\"], \"test_processed.csv\"))\n",
    "\n",
    "# Use cleaned text\n",
    "train_df = train_df[['fname', 'dialogue_clean', 'summary_clean']].rename(\n",
    "    columns={'dialogue_clean': 'dialogue', 'summary_clean': 'summary'}\n",
    ")\n",
    "test_df = test_df[['fname', 'dialogue_clean']].rename(\n",
    "    columns={'dialogue_clean': 'dialogue'}\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š Train: {len(train_df):,} samples\")\n",
    "print(f\"ğŸ“Š Test: {len(test_df):,} samples\")\n",
    "\n",
    "# Train/validation split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.1, random_state=CONFIG[\"seed\"])\n",
    "\n",
    "print(f\"ğŸ“Š Split - Train: {len(train_data):,}, Val: {len(val_data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "print(f\"âœ… Tokenizer loaded: {CONFIG['model_name']}\")\n",
    "\n",
    "# Load special tokens\n",
    "special_tokens_path = os.path.join(CONFIG[\"config_dir\"], \"special_tokens.json\")\n",
    "if os.path.exists(special_tokens_path):\n",
    "    with open(special_tokens_path, 'r', encoding='utf-8') as f:\n",
    "        special_tokens_config = json.load(f)\n",
    "    \n",
    "    # Add special tokens to tokenizer\n",
    "    num_added = tokenizer.add_special_tokens({\n",
    "        'additional_special_tokens': special_tokens_config['additional_special_tokens']\n",
    "    })\n",
    "    print(f\"âœ… Added {num_added} special tokens\")\n",
    "else:\n",
    "    print(\"âš ï¸ No special tokens file found, skipping...\")\n",
    "    num_added = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(CONFIG[\"model_name\"])\n",
    "\n",
    "# Resize token embeddings if we added special tokens\n",
    "if num_added > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"âœ… Resized token embeddings to {len(tokenizer)}\")\n",
    "\n",
    "# Model to device\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"âœ… Model loaded: {CONFIG['model_name']}\")\n",
    "print(f\"ğŸ“Š Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace Dataset\n",
    "train_dataset = Dataset.from_pandas(train_data.reset_index(drop=True))\n",
    "val_dataset = Dataset.from_pandas(val_data.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "print(\"âœ… Datasets created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"í† í¬ë‚˜ì´ì§• ë° íŒ¨ë”©.\"\"\"\n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        examples['dialogue'],\n",
    "        max_length=CONFIG[\"max_input_length\"],\n",
    "        truncation=True,\n",
    "        padding=False  # Dynamic padding handled by data collator\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets (if exists)\n",
    "    if 'summary' in examples:\n",
    "        labels = tokenizer(\n",
    "            examples['summary'],\n",
    "            max_length=CONFIG[\"max_target_length\"],\n",
    "            truncation=True,\n",
    "            padding=False\n",
    "        )\n",
    "        model_inputs['labels'] = labels['input_ids']\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"ğŸ”„ Tokenizing datasets...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=['dialogue', 'summary']\n",
    ")\n",
    "val_dataset = val_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=['dialogue', 'summary']\n",
    ")\n",
    "test_dataset = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=['dialogue']\n",
    ")\n",
    "\n",
    "print(\"âœ… Tokenization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "print(\"âœ… Data collator created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=CONFIG[\"checkpoint_dir\"],\n",
    "    \n",
    "    # Training\n",
    "    num_train_epochs=CONFIG[\"num_train_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"batch_size\"] * 2,\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    \n",
    "    # Optimization\n",
    "    fp16=CONFIG[\"fp16\"],\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Evaluation & Logging\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=CONFIG[\"eval_steps\"],\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    \n",
    "    # Prediction\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=CONFIG[\"max_target_length\"],\n",
    "    generation_num_beams=CONFIG[\"num_beams\"],\n",
    "    \n",
    "    # Saving\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    \n",
    "    # WandB\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"{CONFIG['exp_name']}-exp{CONFIG['exp_num']}\",\n",
    "    \n",
    "    # Misc\n",
    "    seed=CONFIG[\"seed\"],\n",
    "    dataloader_num_workers=4,\n",
    ")\n",
    "\n",
    "print(\"âœ… Training arguments configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"ROUGE ë©”íŠ¸ë¦­ ê³„ì‚°.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Decode predictions\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Decode labels (replace -100 with pad token)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute ROUGE\n",
    "    result = compute_rouge(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_korean_tokenizer=True\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ… Metrics function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"ğŸš€ Starting training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"âœ… Training complete!\")\n",
    "print(f\"ğŸ“Š Final train loss: {train_result.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"ğŸ“Š Evaluating on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nâœ… Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Calculate total ROUGE score\n",
    "rouge_sum = eval_results['eval_rouge1'] + eval_results['eval_rouge2'] + eval_results['eval_rougeL']\n",
    "print(f\"\\nğŸ“ˆ Total ROUGE Score: {rouge_sum:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample predictions\n",
    "print(\"\\nğŸ“ Sample Predictions:\")\n",
    "sample_indices = np.random.choice(len(val_data), 3, replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    sample = val_data.iloc[idx]\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        sample['dialogue'],\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=CONFIG[\"max_input_length\"],\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=CONFIG[\"max_target_length\"],\n",
    "        num_beams=CONFIG[\"num_beams\"],\n",
    "        length_penalty=CONFIG[\"length_penalty\"],\n",
    "        no_repeat_ngram_size=CONFIG[\"no_repeat_ngram_size\"],\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\n--- Sample {idx} ---\")\n",
    "    print(f\"Dialogue: {sample['dialogue'][:100]}...\")\n",
    "    print(f\"Reference: {sample['summary']}\")\n",
    "    print(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "final_model_path = os.path.join(CONFIG[\"checkpoint_dir\"], \"final_model\")\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"âœ… Model saved to {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Test Predictions & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for test set\n",
    "print(\"ğŸ¯ Generating test predictions...\")\n",
    "\n",
    "test_predictions = []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in tqdm(range(len(test_df))):\n",
    "        dialogue = test_df.iloc[idx]['dialogue']\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            dialogue,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=CONFIG[\"max_input_length\"],\n",
    "            truncation=True\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=CONFIG[\"max_target_length\"],\n",
    "            num_beams=CONFIG[\"num_beams\"],\n",
    "            length_penalty=CONFIG[\"length_penalty\"],\n",
    "            no_repeat_ngram_size=CONFIG[\"no_repeat_ngram_size\"],\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        # Decode\n",
    "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        test_predictions.append(prediction)\n",
    "\n",
    "print(f\"âœ… Generated {len(test_predictions)} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'fname': test_df['fname'],\n",
    "    'summary': test_predictions\n",
    "})\n",
    "\n",
    "submission_path = os.path.join(CONFIG[\"submission_dir\"], \"submission_baseline.csv\")\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"âœ… Submission file created: {submission_path}\")\n",
    "print(f\"ğŸ“Š Shape: {submission.shape}\")\n",
    "display(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Git Auto-Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto backup to Git\n",
    "backup_success = auto_git_backup(\n",
    "    exp_num=CONFIG[\"exp_num\"],\n",
    "    model_name=CONFIG[\"model_name\"].split(\"/\")[-1],\n",
    "    rouge_score=rouge_sum,\n",
    "    config=CONFIG\n",
    ")\n",
    "\n",
    "if backup_success:\n",
    "    print(\"âœ… Git backup successful!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Git backup failed (see logs above)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Summary\n",
    "\n",
    "**ì™„ë£Œëœ ì‘ì—…**:\n",
    "- âœ… KoBART ëª¨ë¸ ë¡œë”© ë° ì„¤ì •\n",
    "- âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ (HuggingFace Dataset)\n",
    "- âœ… ëª¨ë¸ í•™ìŠµ (3 epochs)\n",
    "- âœ… ROUGE í‰ê°€ ì™„ë£Œ\n",
    "- âœ… ì²« ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "- âœ… Git ìë™ ë°±ì—…\n",
    "\n",
    "**ë‹¤ìŒ ë‹¨ê³„**: Stage 3 - Hyperparameter Optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
