{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 5: Ensemble & Final Submission\n",
    "\n",
    "ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” ë° ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "\n",
    "## ëª©í‘œ\n",
    "1. í•™ìŠµëœ ëª¨ë¸ë“¤ ë¡œë”©\n",
    "2. ì•™ìƒë¸” ì „ëµ ì ìš©\n",
    "3. ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "4. ê²°ê³¼ ê²€ì¦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Experiment\n",
    "    \"exp_num\": \"FINAL\",\n",
    "    \"exp_name\": \"ensemble-final\",\n",
    "    \"seed\": 42,\n",
    "    \n",
    "    # Models to ensemble\n",
    "    \"models\": [\n",
    "        {\n",
    "            \"name\": \"kobart-optimized\",\n",
    "            \"path\": \"../checkpoints/baseline/final_model\",\n",
    "            \"weight\": 0.4  # Based on validation performance\n",
    "        },\n",
    "        # Add more models as trained\n",
    "    ],\n",
    "    \n",
    "    # Ensemble\n",
    "    \"ensemble_method\": \"weighted_voting\",  # voting, weighted_voting, averaging\n",
    "    \"use_postprocessing\": True,\n",
    "    \n",
    "    # Generation\n",
    "    \"max_input_length\": 512,\n",
    "    \"max_target_length\": 128,\n",
    "    \"num_beams\": 5,\n",
    "    \"length_penalty\": 1.2,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \n",
    "    # Paths\n",
    "    \"data_dir\": \"../data/processed\",\n",
    "    \"submission_dir\": \"../submissions\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM\n",
    ")\n",
    "\n",
    "# Custom utilities\n",
    "from utils import (\n",
    "    set_seed,\n",
    "    auto_git_backup\n",
    ")\n",
    "\n",
    "# Set seed\n",
    "set_seed(CONFIG[\"seed\"])\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ–¥ï¸ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_df = pd.read_csv(os.path.join(CONFIG[\"data_dir\"], \"test_processed.csv\"))\n",
    "test_df = test_df[['fname', 'dialogue_clean']].rename(\n",
    "    columns={'dialogue_clean': 'dialogue'}\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š Test samples: {len(test_df):,}\")\n",
    "print(f\"âœ… Data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all ensemble models\n",
    "loaded_models = []\n",
    "\n",
    "for model_config in CONFIG[\"models\"]:\n",
    "    print(f\"\\nğŸ”„ Loading {model_config['name']}...\")\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_config[\"path\"])\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_config[\"path\"])\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        loaded_models.append({\n",
    "            \"name\": model_config[\"name\"],\n",
    "            \"model\": model,\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"weight\": model_config[\"weight\"]\n",
    "        })\n",
    "        \n",
    "        print(f\"âœ… {model_config['name']} loaded (weight: {model_config['weight']})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load {model_config['name']}: {e}\")\n",
    "\n",
    "print(f\"\\nâœ… Loaded {len(loaded_models)} models for ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”® Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from each model\n",
    "all_predictions = {}\n",
    "\n",
    "for model_info in loaded_models:\n",
    "    model_name = model_info[\"name\"]\n",
    "    model = model_info[\"model\"]\n",
    "    tokenizer = model_info[\"tokenizer\"]\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Generating predictions with {model_name}...\")\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in tqdm(range(len(test_df)), desc=model_name):\n",
    "            dialogue = test_df.iloc[idx]['dialogue']\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                dialogue,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=CONFIG[\"max_input_length\"],\n",
    "                truncation=True\n",
    "            ).to(device)\n",
    "            \n",
    "            # Generate\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=CONFIG[\"max_target_length\"],\n",
    "                num_beams=CONFIG[\"num_beams\"],\n",
    "                length_penalty=CONFIG[\"length_penalty\"],\n",
    "                no_repeat_ngram_size=CONFIG[\"no_repeat_ngram_size\"],\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            # Decode\n",
    "            prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            predictions.append(prediction)\n",
    "    \n",
    "    all_predictions[model_name] = predictions\n",
    "    print(f\"âœ… {model_name}: {len(predictions)} predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¨ Ensemble Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_voting_ensemble(predictions_dict, weights_dict):\n",
    "    \"\"\"ê°€ì¤‘ íˆ¬í‘œ ì•™ìƒë¸”.\"\"\"\n",
    "    ensembled = []\n",
    "    \n",
    "    n_samples = len(list(predictions_dict.values())[0])\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Collect predictions for this sample\n",
    "        sample_preds = [preds[i] for preds in predictions_dict.values()]\n",
    "        sample_weights = list(weights_dict.values())\n",
    "        \n",
    "        # Count votes with weights\n",
    "        vote_counter = Counter()\n",
    "        for pred, weight in zip(sample_preds, sample_weights):\n",
    "            vote_counter[pred] += weight\n",
    "        \n",
    "        # Get winner\n",
    "        winner = vote_counter.most_common(1)[0][0]\n",
    "        ensembled.append(winner)\n",
    "    \n",
    "    return ensembled\n",
    "\n",
    "def simple_voting_ensemble(predictions_dict):\n",
    "    \"\"\"ë‹¨ìˆœ íˆ¬í‘œ ì•™ìƒë¸”.\"\"\"\n",
    "    weights = {name: 1.0 for name in predictions_dict.keys()}\n",
    "    return weighted_voting_ensemble(predictions_dict, weights)\n",
    "\n",
    "def averaging_ensemble(predictions_dict):\n",
    "    \"\"\"í‰ê·  ì•™ìƒë¸” (ì²« ë²ˆì§¸ ëª¨ë¸ ì„ íƒ - ì‹¤ì œë¡œëŠ” logits í‰ê·  í•„ìš”).\"\"\"\n",
    "    # Simplified: return first model's predictions\n",
    "    # In practice, you'd average logits before decoding\n",
    "    return list(predictions_dict.values())[0]\n",
    "\n",
    "print(\"âœ… Ensemble functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ensemble\n",
    "if CONFIG[\"ensemble_method\"] == \"weighted_voting\":\n",
    "    weights = {m[\"name\"]: m[\"weight\"] for m in loaded_models}\n",
    "    ensemble_predictions = weighted_voting_ensemble(all_predictions, weights)\n",
    "elif CONFIG[\"ensemble_method\"] == \"voting\":\n",
    "    ensemble_predictions = simple_voting_ensemble(all_predictions)\n",
    "elif CONFIG[\"ensemble_method\"] == \"averaging\":\n",
    "    ensemble_predictions = averaging_ensemble(all_predictions)\n",
    "else:\n",
    "    # Default: use best single model\n",
    "    ensemble_predictions = list(all_predictions.values())[0]\n",
    "\n",
    "print(f\"âœ… Ensemble complete: {len(ensemble_predictions)} predictions\")\n",
    "print(f\"  Method: {CONFIG['ensemble_method']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_postprocessing(summary):\n",
    "    \"\"\"í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸.\"\"\"\n",
    "    # Remove extra whitespace\n",
    "    summary = re.sub(r'\\s+', ' ', summary).strip()\n",
    "    \n",
    "    # Remove repeated phrases\n",
    "    words = summary.split()\n",
    "    if len(words) > 3:\n",
    "        # Check for 3-word repetitions\n",
    "        seen = set()\n",
    "        filtered = []\n",
    "        for i in range(len(words)):\n",
    "            if i >= 2:\n",
    "                trigram = ' '.join(words[i-2:i+1])\n",
    "                if trigram not in seen:\n",
    "                    filtered.append(words[i])\n",
    "                    seen.add(trigram)\n",
    "            else:\n",
    "                filtered.append(words[i])\n",
    "        summary = ' '.join(filtered)\n",
    "    \n",
    "    # Ensure proper ending\n",
    "    if summary and not summary.endswith(('.', '!', '?')):\n",
    "        summary += '.'\n",
    "    \n",
    "    return summary\n",
    "\n",
    "if CONFIG[\"use_postprocessing\"]:\n",
    "    print(\"ğŸ”§ Applying post-processing...\")\n",
    "    final_predictions = [apply_postprocessing(pred) for pred in ensemble_predictions]\n",
    "    print(\"âœ… Post-processing complete\")\n",
    "else:\n",
    "    final_predictions = ensemble_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'fname': test_df['fname'],\n",
    "    'summary': final_predictions\n",
    "})\n",
    "\n",
    "# Save\n",
    "submission_path = os.path.join(CONFIG[\"submission_dir\"], \"submission_final.csv\")\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"âœ… Final submission saved: {submission_path}\")\n",
    "print(f\"ğŸ“Š Shape: {submission.shape}\")\n",
    "\n",
    "# Display samples\n",
    "print(\"\\nğŸ“ Sample Predictions:\")\n",
    "display(submission.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ”ï¸ Validation Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation checks\n",
    "print(\"ğŸ” Validation Checks:\\n\")\n",
    "\n",
    "# 1. Check row count\n",
    "assert len(submission) == len(test_df), f\"âŒ Row count mismatch: {len(submission)} vs {len(test_df)}\"\n",
    "print(f\"âœ… Row count: {len(submission)}\")\n",
    "\n",
    "# 2. Check for missing values\n",
    "assert submission['summary'].isnull().sum() == 0, \"âŒ Missing summaries found!\"\n",
    "print(f\"âœ… No missing values\")\n",
    "\n",
    "# 3. Check for empty summaries\n",
    "empty_count = (submission['summary'].str.len() == 0).sum()\n",
    "assert empty_count == 0, f\"âŒ {empty_count} empty summaries found!\"\n",
    "print(f\"âœ… No empty summaries\")\n",
    "\n",
    "# 4. Check summary lengths\n",
    "avg_len = submission['summary'].str.len().mean()\n",
    "max_len = submission['summary'].str.len().max()\n",
    "min_len = submission['summary'].str.len().min()\n",
    "print(f\"âœ… Summary lengths: avg={avg_len:.1f}, min={min_len}, max={max_len}\")\n",
    "\n",
    "# 5. Check file names match\n",
    "assert (submission['fname'] == test_df['fname']).all(), \"âŒ File names don't match!\"\n",
    "print(f\"âœ… File names verified\")\n",
    "\n",
    "print(\"\\nğŸ‰ All validation checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Git Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final backup\n",
    "backup_config = {\n",
    "    \"ensemble_method\": CONFIG[\"ensemble_method\"],\n",
    "    \"num_models\": len(loaded_models),\n",
    "    \"postprocessing\": CONFIG[\"use_postprocessing\"]\n",
    "}\n",
    "\n",
    "success = auto_git_backup(\n",
    "    exp_num=CONFIG[\"exp_num\"],\n",
    "    model_name=\"Ensemble\",\n",
    "    rouge_score=0.0,  # Will be known after submission\n",
    "    config=backup_config\n",
    ")\n",
    "\n",
    "if success:\n",
    "    print(\"âœ… Final backup successful!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Backup completed with warnings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Final Summary\n",
    "\n",
    "**ì™„ë£Œëœ ì‘ì—…**:\n",
    "- âœ… ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” êµ¬í˜„\n",
    "- âœ… ê°€ì¤‘ íˆ¬í‘œ ì „ëµ ì ìš©\n",
    "- âœ… í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš©\n",
    "- âœ… ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "- âœ… ê²€ì¦ ì²´í¬ í†µê³¼\n",
    "- âœ… Git ìë™ ë°±ì—…\n",
    "\n",
    "**ë‹¤ìŒ ë‹¨ê³„**:\n",
    "1. submission_final.csvë¥¼ Kaggleì— ì œì¶œ\n",
    "2. ë¦¬ë”ë³´ë“œ ì ìˆ˜ í™•ì¸\n",
    "3. í•„ìš”ì‹œ ì¶”ê°€ ìµœì í™”\n",
    "\n",
    "**ì œì¶œ íŒŒì¼**: `../submissions/submission_final.csv`\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ¯ **ëª©í‘œ ë‹¬ì„±**: ROUGE > 80, Top 3 ìˆœìœ„! ğŸ†"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
