{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: EDA & Preprocessing\n",
    "\n",
    "í•œêµ­ì–´ ëŒ€í™” ìš”ì•½ ë°ì´í„°ì…‹ì— ëŒ€í•œ íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ê³¼ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ëª©í‘œ\n",
    "1. ë°ì´í„°ì…‹ ë¡œë”© ë° ê¸°ë³¸ í†µê³„ í™•ì¸\n",
    "2. í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ë¶„ì„ ë° ì •ì œ\n",
    "3. PII ë§ˆìŠ¤í‚¹ íŒ¨í„´ ì¶”ì¶œ\n",
    "4. ë°ì´í„° ì‹œê°í™”\n",
    "5. ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"data_dir\": \"../data\",\n",
    "    \"output_dir\": \"../data/processed\",\n",
    "    \"config_dir\": \"../configs\",\n",
    "    \"train_file\": \"train.csv\",\n",
    "    \"dev_file\": \"dev.csv\",\n",
    "    \"test_file\": \"test.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Custom utilities\n",
    "from utils import set_seed, clean_dialogue, extract_special_tokens\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(CONFIG[\"seed\"])\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"config_dir\"], exist_ok=True)\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_csv(os.path.join(CONFIG[\"data_dir\"], CONFIG[\"train_file\"]))\n",
    "dev_df = pd.read_csv(os.path.join(CONFIG[\"data_dir\"], CONFIG[\"dev_file\"]))\n",
    "test_df = pd.read_csv(os.path.join(CONFIG[\"data_dir\"], CONFIG[\"test_file\"]))\n",
    "\n",
    "print(f\"âœ… Data loaded successfully\")\n",
    "print(f\"ğŸ“Š Train: {len(train_df):,} samples\")\n",
    "print(f\"ğŸ“Š Dev: {len(dev_df):,} samples\")\n",
    "print(f\"ğŸ“Š Test: {len(test_df):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"\\nğŸ” Train Dataset Preview:\")\n",
    "display(train_df.head())\n",
    "\n",
    "print(\"\\nğŸ“‹ Columns:\", train_df.columns.tolist())\n",
    "print(\"\\nğŸ“Š Data Types:\")\n",
    "print(train_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¬ Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text_statistics(df, name=\"Dataset\"):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ í†µê³„ ë¶„ì„.\"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ê¸¸ì´ (ë¬¸ì ìˆ˜)\n",
    "    df['dialogue_len'] = df['dialogue'].str.len()\n",
    "    df['summary_len'] = df['summary'].str.len() if 'summary' in df.columns else 0\n",
    "    \n",
    "    # ë‹¨ì–´ ìˆ˜\n",
    "    df['dialogue_words'] = df['dialogue'].str.split().str.len()\n",
    "    df['summary_words'] = df['summary'].str.split().str.len() if 'summary' in df.columns else 0\n",
    "    \n",
    "    stats['dialogue_len'] = {\n",
    "        'mean': df['dialogue_len'].mean(),\n",
    "        'std': df['dialogue_len'].std(),\n",
    "        'min': df['dialogue_len'].min(),\n",
    "        'max': df['dialogue_len'].max(),\n",
    "        'median': df['dialogue_len'].median()\n",
    "    }\n",
    "    \n",
    "    if 'summary' in df.columns:\n",
    "        stats['summary_len'] = {\n",
    "            'mean': df['summary_len'].mean(),\n",
    "            'std': df['summary_len'].std(),\n",
    "            'min': df['summary_len'].min(),\n",
    "            'max': df['summary_len'].max(),\n",
    "            'median': df['summary_len'].median()\n",
    "        }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Analyze train data\n",
    "train_stats = analyze_text_statistics(train_df, \"Train\")\n",
    "print(\"ğŸ“Š Train Statistics:\")\n",
    "print(f\"\\nDialogue Length (chars):\")\n",
    "for k, v in train_stats['dialogue_len'].items():\n",
    "    print(f\"  {k}: {v:.1f}\")\n",
    "\n",
    "print(f\"\\nSummary Length (chars):\")\n",
    "for k, v in train_stats['summary_len'].items():\n",
    "    print(f\"  {k}: {v:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"ğŸ” Missing Values:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nğŸ” Duplicate Dialogues: {train_df['dialogue'].duplicated().sum()}\")\n",
    "print(f\"ğŸ” Duplicate Summaries: {train_df['summary'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¹ Noise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_noise(text_series):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ íŒ¨í„´ ë¶„ì„.\"\"\"\n",
    "    patterns = {\n",
    "        'escaped_newlines': r'\\\\\\\\n',\n",
    "        'html_br': r'<br>|<br/>|<br />',\n",
    "        'repeated_chars': r'(.)\\\\1{3,}',\n",
    "        'excessive_spaces': r'\\s{2,}',\n",
    "        'informal_tokens': r'ã…‹+|ã…+|ã…‡ã…‡',\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, pattern in patterns.items():\n",
    "        count = text_series.str.contains(pattern, regex=True).sum()\n",
    "        percentage = (count / len(text_series)) * 100\n",
    "        results[name] = {'count': count, 'percentage': percentage}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze noise in dialogues\n",
    "print(\"ğŸ” Noise Analysis - Dialogues:\")\n",
    "noise_report = analyze_noise(train_df['dialogue'])\n",
    "for name, stats in noise_report.items():\n",
    "    print(f\"  {name}: {stats['count']} ({stats['percentage']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample noisy texts\n",
    "print(\"\\nğŸ“ Sample Noisy Dialogue:\")\n",
    "noisy_samples = train_df[train_df['dialogue'].str.contains(r'\\\\\\\\n|<br>', regex=True)].head(2)\n",
    "for idx, row in noisy_samples.iterrows():\n",
    "    print(f\"\\nBefore cleaning:\")\n",
    "    print(row['dialogue'][:200], \"...\")\n",
    "    print(f\"\\nAfter cleaning:\")\n",
    "    print(clean_dialogue(row['dialogue'])[:200], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ·ï¸ Special Token Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all special tokens from dialogues\n",
    "all_tokens = set()\n",
    "for text in tqdm(train_df['dialogue'], desc=\"Extracting special tokens\"):\n",
    "    tokens = extract_special_tokens(text)\n",
    "    all_tokens.update(tokens)\n",
    "\n",
    "# Sort tokens\n",
    "special_tokens = sorted(list(all_tokens))\n",
    "\n",
    "print(f\"âœ… Found {len(special_tokens)} unique special tokens:\")\n",
    "print(special_tokens)\n",
    "\n",
    "# Count token occurrences\n",
    "token_counts = Counter()\n",
    "for text in train_df['dialogue']:\n",
    "    tokens = extract_special_tokens(text)\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "print(f\"\\nğŸ“Š Top 10 Most Common Tokens:\")\n",
    "for token, count in token_counts.most_common(10):\n",
    "    print(f\"  {token}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save special tokens configuration\n",
    "special_tokens_config = {\n",
    "    \"additional_special_tokens\": special_tokens,\n",
    "    \"token_counts\": {k: v for k, v in token_counts.most_common()}\n",
    "}\n",
    "\n",
    "config_path = os.path.join(CONFIG[\"config_dir\"], \"special_tokens.json\")\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(special_tokens_config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… Special tokens saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to all datasets\n",
    "print(\"ğŸ§¹ Cleaning train dataset...\")\n",
    "train_df['dialogue_clean'] = train_df['dialogue'].apply(clean_dialogue)\n",
    "train_df['summary_clean'] = train_df['summary'].apply(clean_dialogue)\n",
    "\n",
    "print(\"ğŸ§¹ Cleaning dev dataset...\")\n",
    "dev_df['dialogue_clean'] = dev_df['dialogue'].apply(clean_dialogue)\n",
    "dev_df['summary_clean'] = dev_df['summary'].apply(clean_dialogue)\n",
    "\n",
    "print(\"ğŸ§¹ Cleaning test dataset...\")\n",
    "test_df['dialogue_clean'] = test_df['dialogue'].apply(clean_dialogue)\n",
    "if 'summary' in test_df.columns:\n",
    "    test_df['summary_clean'] = test_df['summary'].apply(clean_dialogue)\n",
    "\n",
    "print(\"âœ… Text cleaning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Dialogue length (before cleaning)\n",
    "axes[0, 0].hist(train_df['dialogue_len'], bins=50, edgecolor='black')\n",
    "axes[0, 0].set_title('Dialogue Length Distribution (Before Cleaning)')\n",
    "axes[0, 0].set_xlabel('Characters')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Summary length (before cleaning)\n",
    "axes[0, 1].hist(train_df['summary_len'], bins=50, edgecolor='black')\n",
    "axes[0, 1].set_title('Summary Length Distribution (Before Cleaning)')\n",
    "axes[0, 1].set_xlabel('Characters')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Dialogue words\n",
    "axes[1, 0].hist(train_df['dialogue_words'], bins=50, edgecolor='black')\n",
    "axes[1, 0].set_title('Dialogue Word Count Distribution')\n",
    "axes[1, 0].set_xlabel('Words')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Summary words\n",
    "axes[1, 1].hist(train_df['summary_words'], bins=50, edgecolor='black')\n",
    "axes[1, 1].set_title('Summary Word Count Distribution')\n",
    "axes[1, 1].set_xlabel('Words')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud visualization\n",
    "print(\"ğŸ“Š Generating word clouds...\")\n",
    "\n",
    "# Combine all dialogues\n",
    "all_dialogues = ' '.join(train_df['dialogue_clean'].head(1000))  # Sample for speed\n",
    "all_summaries = ' '.join(train_df['summary_clean'].head(1000))\n",
    "\n",
    "# Create word clouds\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Dialogue word cloud\n",
    "wc_dialogue = WordCloud(width=800, height=400, background_color='white', \n",
    "                        font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf').generate(all_dialogues)\n",
    "axes[0].imshow(wc_dialogue, interpolation='bilinear')\n",
    "axes[0].set_title('Dialogue Word Cloud', fontsize=16)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Summary word cloud\n",
    "wc_summary = WordCloud(width=800, height=400, background_color='white',\n",
    "                       font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf').generate(all_summaries)\n",
    "axes[1].imshow(wc_summary, interpolation='bilinear')\n",
    "axes[1].set_title('Summary Word Cloud', fontsize=16)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Word clouds generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed datasets\n",
    "train_output = os.path.join(CONFIG[\"output_dir\"], \"train_processed.csv\")\n",
    "dev_output = os.path.join(CONFIG[\"output_dir\"], \"dev_processed.csv\")\n",
    "test_output = os.path.join(CONFIG[\"output_dir\"], \"test_processed.csv\")\n",
    "\n",
    "train_df.to_csv(train_output, index=False)\n",
    "dev_df.to_csv(dev_output, index=False)\n",
    "test_df.to_csv(test_output, index=False)\n",
    "\n",
    "print(f\"âœ… Processed data saved to {CONFIG['output_dir']}/\")\n",
    "print(f\"  - train_processed.csv: {len(train_df):,} rows\")\n",
    "print(f\"  - dev_processed.csv: {len(dev_df):,} rows\")\n",
    "print(f\"  - test_processed.csv: {len(test_df):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Summary\n",
    "\n",
    "**ì™„ë£Œëœ ì‘ì—…**:\n",
    "- âœ… ë°ì´í„°ì…‹ ë¡œë”© ë° ê¸°ë³¸ í†µê³„ ë¶„ì„\n",
    "- âœ… í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ë¶„ì„ ë° ì •ì œ\n",
    "- âœ… íŠ¹ìˆ˜ í† í° ì¶”ì¶œ ë° ì €ì¥\n",
    "- âœ… ë°ì´í„° ì‹œê°í™” (íˆìŠ¤í† ê·¸ë¨, ì›Œë“œí´ë¼ìš°ë“œ)\n",
    "- âœ… ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥\n",
    "\n",
    "**ë‹¤ìŒ ë‹¨ê³„**: Stage 2 - Baseline Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
