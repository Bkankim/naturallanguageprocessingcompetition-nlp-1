{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: EDA & Preprocessing\n",
    "\n",
    "한국어 대화 요약 데이터셋에 대한 탐색적 데이터 분석과 전처리를 수행합니다.\n",
    "\n",
    "## 목표\n",
    "1. 데이터셋 로딩 및 기본 통계 확인\n",
    "2. 텍스트 노이즈 분석 및 정제\n",
    "3. PII 마스킹 패턴 추출\n",
    "4. 데이터 시각화\n",
    "5. 전처리된 데이터 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"data_dir\": \"../data\",\n",
    "    \"output_dir\": \"../data/processed\",\n",
    "    \"config_dir\": \"../configs\",\n",
    "    \"train_file\": \"train.csv\",\n",
    "    \"dev_file\": \"dev.csv\",\n",
    "    \"test_file\": \"test.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Custom utilities\n",
    "from utils import set_seed, clean_dialogue, extract_special_tokens\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(CONFIG[\"seed\"])\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"config_dir\"], exist_ok=True)\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_csv(os.path.join(CONFIG[\"data_dir\"], CONFIG[\"train_file\"]))\n",
    "dev_df = pd.read_csv(os.path.join(CONFIG[\"data_dir\"], CONFIG[\"dev_file\"]))\n",
    "test_df = pd.read_csv(os.path.join(CONFIG[\"data_dir\"], CONFIG[\"test_file\"]))\n",
    "\n",
    "print(f\"✅ Data loaded successfully\")\n",
    "print(f\"📊 Train: {len(train_df):,} samples\")\n",
    "print(f\"📊 Dev: {len(dev_df):,} samples\")\n",
    "print(f\"📊 Test: {len(test_df):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"\\n🔍 Train Dataset Preview:\")\n",
    "display(train_df.head())\n",
    "\n",
    "print(\"\\n📋 Columns:\", train_df.columns.tolist())\n",
    "print(\"\\n📊 Data Types:\")\n",
    "print(train_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔬 Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text_statistics(df, name=\"Dataset\"):\n",
    "    \"\"\"텍스트 통계 분석.\"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    # 텍스트 길이 (문자 수)\n",
    "    df['dialogue_len'] = df['dialogue'].str.len()\n",
    "    df['summary_len'] = df['summary'].str.len() if 'summary' in df.columns else 0\n",
    "    \n",
    "    # 단어 수\n",
    "    df['dialogue_words'] = df['dialogue'].str.split().str.len()\n",
    "    df['summary_words'] = df['summary'].str.split().str.len() if 'summary' in df.columns else 0\n",
    "    \n",
    "    stats['dialogue_len'] = {\n",
    "        'mean': df['dialogue_len'].mean(),\n",
    "        'std': df['dialogue_len'].std(),\n",
    "        'min': df['dialogue_len'].min(),\n",
    "        'max': df['dialogue_len'].max(),\n",
    "        'median': df['dialogue_len'].median()\n",
    "    }\n",
    "    \n",
    "    if 'summary' in df.columns:\n",
    "        stats['summary_len'] = {\n",
    "            'mean': df['summary_len'].mean(),\n",
    "            'std': df['summary_len'].std(),\n",
    "            'min': df['summary_len'].min(),\n",
    "            'max': df['summary_len'].max(),\n",
    "            'median': df['summary_len'].median()\n",
    "        }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Analyze train data\n",
    "train_stats = analyze_text_statistics(train_df, \"Train\")\n",
    "print(\"📊 Train Statistics:\")\n",
    "print(f\"\\nDialogue Length (chars):\")\n",
    "for k, v in train_stats['dialogue_len'].items():\n",
    "    print(f\"  {k}: {v:.1f}\")\n",
    "\n",
    "print(f\"\\nSummary Length (chars):\")\n",
    "for k, v in train_stats['summary_len'].items():\n",
    "    print(f\"  {k}: {v:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"🔍 Missing Values:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\n🔍 Duplicate Dialogues: {train_df['dialogue'].duplicated().sum()}\")\n",
    "print(f\"🔍 Duplicate Summaries: {train_df['summary'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧹 Noise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_noise(text_series):\n",
    "    \"\"\"텍스트 노이즈 패턴 분석.\"\"\"\n",
    "    patterns = {\n",
    "        'escaped_newlines': r'\\\\\\\\n',\n",
    "        'html_br': r'<br>|<br/>|<br />',\n",
    "        'repeated_chars': r'(.)\\\\1{3,}',\n",
    "        'excessive_spaces': r'\\s{2,}',\n",
    "        'informal_tokens': r'ㅋ+|ㅎ+|ㅇㅇ',\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, pattern in patterns.items():\n",
    "        count = text_series.str.contains(pattern, regex=True).sum()\n",
    "        percentage = (count / len(text_series)) * 100\n",
    "        results[name] = {'count': count, 'percentage': percentage}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze noise in dialogues\n",
    "print(\"🔍 Noise Analysis - Dialogues:\")\n",
    "noise_report = analyze_noise(train_df['dialogue'])\n",
    "for name, stats in noise_report.items():\n",
    "    print(f\"  {name}: {stats['count']} ({stats['percentage']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample noisy texts\n",
    "print(\"\\n📝 Sample Noisy Dialogue:\")\n",
    "noisy_samples = train_df[train_df['dialogue'].str.contains(r'\\\\\\\\n|<br>', regex=True)].head(2)\n",
    "for idx, row in noisy_samples.iterrows():\n",
    "    print(f\"\\nBefore cleaning:\")\n",
    "    print(row['dialogue'][:200], \"...\")\n",
    "    print(f\"\\nAfter cleaning:\")\n",
    "    print(clean_dialogue(row['dialogue'])[:200], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏷️ Special Token Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all special tokens from dialogues\n",
    "all_tokens = set()\n",
    "for text in tqdm(train_df['dialogue'], desc=\"Extracting special tokens\"):\n",
    "    tokens = extract_special_tokens(text)\n",
    "    all_tokens.update(tokens)\n",
    "\n",
    "# Sort tokens\n",
    "special_tokens = sorted(list(all_tokens))\n",
    "\n",
    "print(f\"✅ Found {len(special_tokens)} unique special tokens:\")\n",
    "print(special_tokens)\n",
    "\n",
    "# Count token occurrences\n",
    "token_counts = Counter()\n",
    "for text in train_df['dialogue']:\n",
    "    tokens = extract_special_tokens(text)\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "print(f\"\\n📊 Top 10 Most Common Tokens:\")\n",
    "for token, count in token_counts.most_common(10):\n",
    "    print(f\"  {token}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save special tokens configuration\n",
    "special_tokens_config = {\n",
    "    \"additional_special_tokens\": special_tokens,\n",
    "    \"token_counts\": {k: v for k, v in token_counts.most_common()}\n",
    "}\n",
    "\n",
    "config_path = os.path.join(CONFIG[\"config_dir\"], \"special_tokens.json\")\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(special_tokens_config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Special tokens saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to all datasets\n",
    "print(\"🧹 Cleaning train dataset...\")\n",
    "train_df['dialogue_clean'] = train_df['dialogue'].apply(clean_dialogue)\n",
    "train_df['summary_clean'] = train_df['summary'].apply(clean_dialogue)\n",
    "\n",
    "print(\"🧹 Cleaning dev dataset...\")\n",
    "dev_df['dialogue_clean'] = dev_df['dialogue'].apply(clean_dialogue)\n",
    "dev_df['summary_clean'] = dev_df['summary'].apply(clean_dialogue)\n",
    "\n",
    "print(\"🧹 Cleaning test dataset...\")\n",
    "test_df['dialogue_clean'] = test_df['dialogue'].apply(clean_dialogue)\n",
    "if 'summary' in test_df.columns:\n",
    "    test_df['summary_clean'] = test_df['summary'].apply(clean_dialogue)\n",
    "\n",
    "print(\"✅ Text cleaning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Dialogue length (before cleaning)\n",
    "axes[0, 0].hist(train_df['dialogue_len'], bins=50, edgecolor='black')\n",
    "axes[0, 0].set_title('Dialogue Length Distribution (Before Cleaning)')\n",
    "axes[0, 0].set_xlabel('Characters')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Summary length (before cleaning)\n",
    "axes[0, 1].hist(train_df['summary_len'], bins=50, edgecolor='black')\n",
    "axes[0, 1].set_title('Summary Length Distribution (Before Cleaning)')\n",
    "axes[0, 1].set_xlabel('Characters')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Dialogue words\n",
    "axes[1, 0].hist(train_df['dialogue_words'], bins=50, edgecolor='black')\n",
    "axes[1, 0].set_title('Dialogue Word Count Distribution')\n",
    "axes[1, 0].set_xlabel('Words')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Summary words\n",
    "axes[1, 1].hist(train_df['summary_words'], bins=50, edgecolor='black')\n",
    "axes[1, 1].set_title('Summary Word Count Distribution')\n",
    "axes[1, 1].set_xlabel('Words')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud visualization\n",
    "print(\"📊 Generating word clouds...\")\n",
    "\n",
    "# Combine all dialogues\n",
    "all_dialogues = ' '.join(train_df['dialogue_clean'].head(1000))  # Sample for speed\n",
    "all_summaries = ' '.join(train_df['summary_clean'].head(1000))\n",
    "\n",
    "# Create word clouds\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Dialogue word cloud\n",
    "wc_dialogue = WordCloud(width=800, height=400, background_color='white', \n",
    "                        font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf').generate(all_dialogues)\n",
    "axes[0].imshow(wc_dialogue, interpolation='bilinear')\n",
    "axes[0].set_title('Dialogue Word Cloud', fontsize=16)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Summary word cloud\n",
    "wc_summary = WordCloud(width=800, height=400, background_color='white',\n",
    "                       font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf').generate(all_summaries)\n",
    "axes[1].imshow(wc_summary, interpolation='bilinear')\n",
    "axes[1].set_title('Summary Word Cloud', fontsize=16)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Word clouds generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed datasets\n",
    "train_output = os.path.join(CONFIG[\"output_dir\"], \"train_processed.csv\")\n",
    "dev_output = os.path.join(CONFIG[\"output_dir\"], \"dev_processed.csv\")\n",
    "test_output = os.path.join(CONFIG[\"output_dir\"], \"test_processed.csv\")\n",
    "\n",
    "train_df.to_csv(train_output, index=False)\n",
    "dev_df.to_csv(dev_output, index=False)\n",
    "test_df.to_csv(test_output, index=False)\n",
    "\n",
    "print(f\"✅ Processed data saved to {CONFIG['output_dir']}/\")\n",
    "print(f\"  - train_processed.csv: {len(train_df):,} rows\")\n",
    "print(f\"  - dev_processed.csv: {len(dev_df):,} rows\")\n",
    "print(f\"  - test_processed.csv: {len(test_df):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Summary\n",
    "\n",
    "**완료된 작업**:\n",
    "- ✅ 데이터셋 로딩 및 기본 통계 분석\n",
    "- ✅ 텍스트 노이즈 분석 및 정제\n",
    "- ✅ 특수 토큰 추출 및 저장\n",
    "- ✅ 데이터 시각화 (히스토그램, 워드클라우드)\n",
    "- ✅ 전처리된 데이터 저장\n",
    "\n",
    "**다음 단계**: Stage 2 - Baseline Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
