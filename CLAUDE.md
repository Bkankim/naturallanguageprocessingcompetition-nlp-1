# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## .md Update Guidelines
- After completing each task, always prioritize updating all documents to ensure they are up to date.

## Git Updates
- After completing each task, update the .gitignore file and always synchronize with the GitHub repository (as a backup practice).

## code agents Guidelines
- ê° ëª…ë ¹ì„ ìˆ˜í–‰í•  ë•Œ, ê°€ëŠ¥í•œ SubAgentsë¥¼ ì ê·¹ í™œìš©í•  ê²ƒ (ê°ê°ì˜ ì„ë¬´ë¥¼ ì •ì˜í•´ë‘” ì„œë¸Œ ì—ì´ì „íŠ¸ë“¤ì´ ì¡´ì¬).
**ì£¼ì˜** : ë‹¨ë… ìˆ˜í–‰í•  ë•Œ ì„±ëŠ¥ì´ ë” ì¢‹ì„ ê²ƒ ê°™ë‹¤ëŠ” íŒë‹¨ì´ ì„œë©´ ì„œë¸Œì—ì´ì „íŠ¸ ì‚¬ìš©ê¸ˆì§€. ì–´ë””ê¹Œì§€ë‚˜ ë„ˆì˜ íŒë‹¨ í•˜ì— ë³‘ë ¬ ìˆ˜í–‰ì„ í•  ê²ƒ.

## âš ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡° ë° Git ì‘ì—… ë””ë ‰í† ë¦¬ (ì ˆëŒ€ í—·ê°ˆë¦¬ì§€ ë§ ê²ƒ!)

**í”„ë¡œì íŠ¸ ë£¨íŠ¸**: `/Competition/NLP`
**Git ì‘ì—… ë””ë ‰í† ë¦¬**: `/Competition/NLP/naturallanguageprocessingcompetition-nlp-1`

### ì¤‘ìš” ê·œì¹™:
1. **ëª¨ë“  Git ëª…ë ¹ì€ ë°˜ë“œì‹œ `naturallanguageprocessingcompetition-nlp-1/` ë””ë ‰í† ë¦¬ ì•ˆì—ì„œ ì‹¤í–‰**
   ```bash
   # ì˜¬ë°”ë¥¸ ì˜ˆì‹œ
   cd /Competition/NLP/naturallanguageprocessingcompetition-nlp-1
   git add .
   git commit -m "message"
   git push origin main

   # ì˜ëª»ëœ ì˜ˆì‹œ - ì ˆëŒ€ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰í•˜ì§€ ë§ ê²ƒ
   cd /Competition/NLP
   git add .  # âŒ ì˜ëª»ë¨!
   ```

2. **íŒŒì¼ ì‘ì—…ì€ ë£¨íŠ¸(`/Competition/NLP`)ì—ì„œ í•˜ë˜, Git ì‘ì—…ì€ ì„œë¸Œë””ë ‰í† ë¦¬ì—ì„œ**
   - Epic/Task íŒŒì¼: `/Competition/NLP/.claude/`
   - ë°ì´í„°: `/Competition/NLP/data/`
   - ë¬¸ì„œ: `/Competition/NLP/docs/`
   - Git ì €ì¥ì†Œ: `/Competition/NLP/naturallanguageprocessingcompetition-nlp-1/.git`

3. **GitHub ì €ì¥ì†Œ**: `https://github.com/Bkankim/naturallanguageprocessingcompetition-nlp-1.git`

4. **ì‘ì—… íë¦„**:
   - í”„ë¡œì íŠ¸ íŒŒì¼ í¸ì§‘: ë£¨íŠ¸ ë˜ëŠ” í•˜ìœ„ ë””ë ‰í† ë¦¬ ì–´ë””ì„œë‚˜ ê°€ëŠ¥
   - Git ëª…ë ¹ ì‹¤í–‰: **ë°˜ë“œì‹œ** `cd /Competition/NLP/naturallanguageprocessingcompetition-nlp-1` í›„ ì‹¤í–‰
   - ì»¤ë°‹í•  íŒŒì¼ì„ Git ì €ì¥ì†Œ ì•ˆìœ¼ë¡œ ë³µì‚¬/ì´ë™ í•„ìš”

## MCP Usage Criteria
- sequential-thinking: Use freely when tasks require deep reasoning or involve complex processes.
- context7: Must be used when dependency issues arise or when downloading new packages is necessary.

## Prompt Language
- Always respond in Korean, and represent all reasoning steps in Korean as well.

## Code Writing Guidelines
- Every Class and def definition must be preceded by a simple comment explaining its functionality.

## Jupyter Notebook Guidelines
- **ì‘ì—… ì™„ë£Œ ì‹œ ì¬í˜„ ë…¸íŠ¸ë¶ ìƒì„± í•„ìˆ˜**: ëª¨ë“ˆí™”ëœ ì½”ë“œë‚˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í•œ ê²½ìš°, ì‚¬ìš©ìê°€ ì‰½ê²Œ ì¬í˜„í•  ìˆ˜ ìˆë„ë¡ Jupyter Notebookì„ ë°˜ë“œì‹œ í•¨ê»˜ ì œê³µí•  ê²ƒ.
- **ë…¸íŠ¸ë¶ êµ¬ì¡°**:
  1. í™˜ê²½ ì„¤ì • (imports, path ì„¤ì •)
  2. Config ë¡œë“œ ë° ì£¼ìš” ì„¤ì • í™•ì¸
  3. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
  4. ëª¨ë¸ í•™ìŠµ/ì¶”ë¡ 
  5. ê²°ê³¼ í‰ê°€ ë° ì‹œê°í™”
  6. ìƒ˜í”Œ ì˜ˆì¸¡ í™•ì¸
- **ë…¸íŠ¸ë¶ ìœ„ì¹˜**: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë˜ëŠ” ê´€ë ¨ ë””ë ‰í† ë¦¬ (ì˜ˆ: `dialogue-summarization/train_demo.ipynb`)
- **ë§ˆí¬ë‹¤ìš´ ì…€ í™œìš©**: ê° ì„¹ì…˜ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ê³  ì„¤ëª… ì¶”ê°€
- **ì‹¤í–‰ ê°€ëŠ¥ì„± ë³´ì¥**: ëª¨ë“  ì…€ì´ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•´ì•¼ í•¨
- **ì˜ˆì œ**:
  - í•™ìŠµ í”„ë¡œì íŠ¸: `train_demo.ipynb` ë˜ëŠ” `reproduction_notebook.ipynb`
  - ì¶”ë¡  í”„ë¡œì íŠ¸: `inference_demo.ipynb`
  - EDA í”„ë¡œì íŠ¸: `eda_notebook.ipynb`

## Storage Management Guidelines
- Before saving any run results, check disk usage with:
```bash
du -sh / 2>/dev/null
```
- **âš ï¸ CRITICAL**: Ensure that the root (/) disk usage does not exceed the **150GB limit** - server resets if exceeded
- **Safe zone**: Keep usage below 110GB for safety margin
- Use `du -sh /` for accurate measurement (not `df -h`)
- Clean up old checkpoints and cache files regularly

## Project Overview

This is a **Dialogue Summarization Competition** project focused on building models that generate summaries from Korean conversational data. The competition uses the DialogSum dataset (translated to Korean) where models must generate summaries of multi-turn dialogues between 2-7 participants.

**Key Task**: Given dialogue text, generate a concise summary that captures the main points of the conversation.

**Evaluation**: Model performance is measured using the average of ROUGE-1-F1, ROUGE-2-F1, and ROUGE-L-F1 scores across 3 reference summaries per dialogue. Korean morphological tokenization is used for scoring.

## Data Structure

### Dataset Files (`data/`)
- `train.csv` - 12,457 training samples (dialogue + summary)
- `dev.csv` - 499 development samples
- `test.csv` - 250 test samples (dialogue + 3 reference summaries)
- `sample_submission.csv` - Submission format template

### Data Format
- **fname**: Unique dialogue identifier
- **dialogue**: Multi-turn conversation with speakers labeled as `#Person1#`, `#Person2#`, etc.
  - Turns separated by `\n` (note: some noise includes `\\n` or `<br>` tags)
  - Contains masked PII: `#PhoneNumber#`, `#Address#`, `#DateOfBirth#`, `#PassportNumber#`, `#SSN#`, `#CardNumber#`, `#CarNumber#`, `#Email#`
- **summary**: Target summary text (ë¬¸ì–´ì²´/written style vs dialogue êµ¬ì–´ì²´/spoken style)

### Data Noise
The dataset contains noise that must be handled:
- Escaped newlines: `\\n` instead of `\n`
- HTML tags: `<br>` instead of newlines
- Informal tokens: `ã…‹ã…‹`, `ã…‡ã…‡` (Korean onomatopoeia)

## Development Commands

### Package Management
Use the PM CLI for unified package management:
- `/pm:install` - Install all dependencies
- `/pm:add <package>` - Add a new package
- `/pm:run <script>` - Run package scripts
- `/pm:test` - Run tests

### Python Environment
```bash
# Activate virtual environment
source .venv/bin/activate

# Install dependencies
uv pip install -e .
```

### Working with Notebooks
Baseline implementations are in Jupyter notebooks:
- `naturallanguageprocessingcompetition-nlp-1/code/baseline.ipynb` - Main baseline model
- `naturallanguageprocessingcompetition-nlp-1/code/solar_api.ipynb` - Solar API integration

## Competition Rules & Constraints

### Prohibited
- **DialogSum dataset** (original or derived) - this is the source of test data
- Using test data for training (analysis is allowed)
- Pretrained weights trained on DialogSum
- Paid APIs (exception: **Solar model is allowed**)

### Allowed
- External datasets (except DialogSum)
- Free APIs
- Pretrained weights (not trained on DialogSum)
- Solar model API

### Submission
- Maximum 12 submissions per day per team (resets at midnight KST)
- Final submission: select up to 2 results before deadline
- Output format: CSV with 249 dialogue summaries

## Model Development Guidelines

### Preprocessing Considerations
1. **Text Cleaning**: Handle noise (`\\n`, `<br>`, informal tokens)
2. **Special Tokens**: Add PII masking tokens and person markers to tokenizer:
   ```python
   special_tokens = ['#Person1#', '#Person2#', ..., '#PhoneNumber#', '#Address#', ...]
   tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})
   ```
3. **Length Analysis**: Check dialogue/summary length distributions to set appropriate `max_length` parameters
4. **Speaker Extraction**: Use regex `r"#\w+\d#"` to identify speakers (`#Person1#`, etc.)

### Key Hyperparameters (Seq2SeqTrainingArguments)
- `learning_rate`: Default 5e-5 (AdamW)
- `per_device_train_batch_size`: Default 8
- `num_train_epochs`: Default 20 (watch for overfitting)
- `gradient_accumulation_steps`: For effective larger batch sizes
- `warmup_ratio`: Learning rate warmup (default 0.0)
- `lr_scheduler_type`: `linear` or `cosine`
- `fp16`/`bf16`: Mixed precision training

### Hyperparameter Tuning
The project supports Optuna for automated hyperparameter search:
```python
def optuna_hp_space(trial):
    return {
        "learning_rate": trial.suggest_float("lr", 1e-4, 5e-3, log=True),
        "per_device_train_batch_size": trial.suggest_categorical("batch", [16, 32]),
        # ... other parameters
    }

best_run = trainer.hyperparameter_search(
    direction="maximize",
    backend="optuna",
    hp_space=optuna_hp_space,
    n_trials=20
)
```

## Documentation

### Competition Overview (`docs/Competition_Overview/`)
- `introduction.md` - Competition background and objectives
- `data_overview.md` - Detailed data structure and format
- `evaluation_method.md` - ROUGE metric calculation details
- `competition_rule.md` - Full rules and restrictions

### Advanced Topics (`docs/Competition_Advanced/`)
- `textdata_eda.md` - Text EDA techniques: word clouds, TF-IDF, regex patterns, PII masking analysis
- `hyperparameter_tuning_tip.md` - Grid/Random/Bayesian optimization with Optuna

## Expected Baseline Performance

Random reference selection from 3 gold summaries achieves approximately **70 points** (sum of ROUGE-1, ROUGE-2, ROUGE-L F1 scores). This provides a baseline expectation for model performance.

## Architecture Notes

This is primarily a **Seq2Seq summarization task** using Korean language models. Common approaches:
- Fine-tuning encoder-decoder models (e.g., KoBART, mBART)
- Using Hugging Face Trainer with Seq2SeqTrainingArguments
- Implementing custom metrics using Korean morphological tokenizers for ROUGE scoring

## Critical Technical Learnings

### 1. Chat Template Tokens for LLM Models
LLM ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹œ ë°˜ë“œì‹œ Chat Template í† í°ì„ ëª…ì‹œì ìœ¼ë¡œ í† í¬ë‚˜ì´ì €ì— ì¶”ê°€í•´ì•¼ í•¨:

**Llama ëª¨ë¸ (Llama-3, Llama-3.1, Llama-3.2 ë“±):**
```python
chat_template_tokens = ["<|start_header_id|>", "<|end_header_id|>", "<|eot_id|>"]
tokenizer.add_special_tokens({'additional_special_tokens': chat_template_tokens})
model.resize_token_embeddings(len(tokenizer))
```

**Qwen ëª¨ë¸ (Qwen2, Qwen2.5 ë“±):**
```python
chat_template_tokens = ["<|im_start|>", "<|im_end|>"]
tokenizer.add_special_tokens({'additional_special_tokens': chat_template_tokens})
model.resize_token_embeddings(len(tokenizer))
```

**ì£¼ì˜ì‚¬í•­:**
- Chat template í† í°ì„ ì¶”ê°€í•˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµ ì¤‘ í† í° ì¸ì½”ë”© ì˜¤ë¥˜ ë°œìƒ
- ë°˜ë“œì‹œ `model.resize_token_embeddings()` í˜¸ì¶œí•˜ì—¬ ì„ë² ë”© í¬ê¸° ì¡°ì •

### 2. QLoRA Configuration: compute_dtype Matching
QLoRA ì„¤ì • ì‹œ `compute_dtype`ì€ ë°˜ë“œì‹œ ëª¨ë¸ì˜ dtypeê³¼ ì¼ì¹˜í•´ì•¼ í•¨:

**Llama ëª¨ë¸ (bf16 ì‚¬ìš©):**
```python
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16  # bf16 ì‚¬ìš©
)

training_args = TrainingArguments(
    bf16=True,  # bf16 í™œì„±í™”
    fp16=False,
    ...
)
```

**Qwen ëª¨ë¸ (fp16 ì‚¬ìš©):**
```python
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16  # fp16 ì‚¬ìš©
)

training_args = TrainingArguments(
    fp16=True,  # fp16 í™œì„±í™”
    bf16=False,
    ...
)
```

**ì£¼ì˜ì‚¬í•­:**
- compute_dtype ë¶ˆì¼ì¹˜ ì‹œ í•™ìŠµ ì¤‘ ìˆ˜ì¹˜ì  ë¶ˆì•ˆì •ì„± ë˜ëŠ” ì˜¤ë¥˜ ë°œìƒ
- í•­ìƒ ëª¨ë¸ ë¬¸ì„œì—ì„œ ê¶Œì¥ dtype í™•ì¸

### 3. metric_for_best_model: Model Typeë³„ ì°¨ì´

**Encoder-Decoder ëª¨ë¸ (KoBART, KoT5 ë“±):**
```python
training_args = Seq2SeqTrainingArguments(
    metric_for_best_model="rouge_sum",  # ROUGE í•©ê³„ ì‚¬ìš©
    greater_is_better=True,
    load_best_model_at_end=True,
    ...
)
```

**Causal LM ëª¨ë¸ (Llama, Qwen ë“±):**
```python
training_args = TrainingArguments(
    metric_for_best_model="eval_loss",  # Loss ì‚¬ìš©
    greater_is_better=False,  # LossëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
    load_best_model_at_end=True,
    ...
)
```

**ì´ìœ :**
- Encoder-Decoder: `generate()` ë©”ì„œë“œê°€ ê¸°ë³¸ ì œê³µë˜ë¯€ë¡œ ROUGE ì§ì ‘ ê³„ì‚° ê°€ëŠ¥
- Causal LM: generation config ì„¤ì •ì´ ë³µì¡í•˜ì—¬ eval_loss ì‚¬ìš©ì´ ì•ˆì •ì 

### 4. Model Architecture ì„ íƒ ê°€ì´ë“œ

**Encoder-Decoder (Seq2Seq) ëª¨ë¸:**
- **ì¥ì **: ìš”ì•½ íƒœìŠ¤í¬ì— ìµœì í™”, ìƒì„± í’ˆì§ˆ ìš°ìˆ˜
- **ë‹¨ì **: ëª¨ë¸ í¬ê¸° ì œì•½ (ëŒ€ë¶€ë¶„ 1B ì´í•˜)
- **ì¶”ì²œ ëª¨ë¸**: KoBART, KoT5, mBART-50

**Causal LM (Decoder-only) ëª¨ë¸:**
- **ì¥ì **: ëŒ€ê·œëª¨ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ (3B~8B), ìµœì‹  ì•„í‚¤í…ì²˜
- **ë‹¨ì **: í•™ìŠµì´ ê¹Œë‹¤ë¡œì›€, Chat template ì²˜ë¦¬ í•„ìš”
- **ì¶”ì²œ ëª¨ë¸**: Llama-3.2-3B, Qwen2.5-3B, EXAONE-3.5-2.4B

### 5. Prompt Truncation ë¬¸ì œ (LLM ëª¨ë¸ì˜ ì¹˜ëª…ì  ì´ìŠˆ)

Causal LM íŒŒì¸íŠœë‹ ì‹œ max_length ì„¤ì •ì´ ë¶€ì ì ˆí•˜ë©´ **assistant í—¤ë”ê°€ ì˜ë ¤ë‚˜ê°€** ëª¨ë¸ì´ ìƒì„± ìœ„ì¹˜ë¥¼ ì¸ì‹í•˜ì§€ ëª»í•˜ëŠ” ì¹˜ëª…ì  ë¬¸ì œ ë°œìƒ.

**ë¬¸ì œ ë°œìƒ ë©”ì»¤ë‹ˆì¦˜:**

Chat template êµ¬ì¡°:
```
<|start_header_id|>system<|end_header_id|>
ë‹¹ì‹ ì€ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
<|eot_id|><|start_header_id|>user<|end_header_id|>
ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ì„¸ìš”:
---
[ê¸´ ëŒ€í™” ë‚´ìš©...]
---
<|eot_id|><|start_header_id|>assistant<|end_header_id|>  â† ì—¬ê¸°ì„œë¶€í„° ìƒì„±!
[ìš”ì•½...]
```

ë§Œì•½ max_length=512ë¡œ ì˜ë¦¬ë©´:
```
<|start_header_id|>system<|end_header_id|>
ë‹¹ì‹ ì€ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
<|eot_id|><|start_header_id|>user<|end_header_id|>
ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ì„¸ìš”:
---
[ê¸´ ëŒ€í™”... (ì—¬ê¸°ì„œ ëŠê¹€)]
```

â†’ **assistant í—¤ë” ì—†ìŒ** â†’ ëª¨ë¸ì€ "userê°€ ë§í•˜ëŠ” ì¤‘"ìœ¼ë¡œ ì¸ì‹ â†’ ë¹ˆ ë¬¸ìì—´ ë˜ëŠ” ëŒ€í™” ì´ì–´ê°€ê¸°

**ì‹¤ì¸¡ ë°ì´í„° (ë³¸ í”„ë¡œì íŠ¸):**

ê¸°ì¡´ ì„¤ì • (encoder_max_len=512, decoder_max_len=100):
- í•™ìŠµ Prompt ì˜ë¦¼: **6.07%** (756/12,457ê°œ)
- í•™ìŠµ Full text ì˜ë¦¼: **4.50%** (560/12,457ê°œ)
- Dev Prompt ì˜ë¦¼: **6.81%** (34/499ê°œ)

ê°œì„  ì„¤ì • (encoder_max_len=1024, decoder_max_len=200):
- í•™ìŠµ Prompt ì˜ë¦¼: **0.11%** (14/12,457ê°œ)
- í•™ìŠµ Full text ì˜ë¦¼: **0.07%** (9/12,457ê°œ)
- Dev Prompt ì˜ë¦¼: **0.00%** (0/499ê°œ)

**í•´ê²°ì±…:**

```python
# 1. Config ì„¤ì • (finetune_config.yaml)
tokenizer:
  encoder_max_len: 1024  # 512 â†’ 1024 (LLMìš© ì¦ê°€)
  decoder_max_len: 200   # 100 â†’ 200 (ì—¬ìœ  í™•ë³´)

# 2. í•™ìŠµ ì‹œ: Right truncation ìœ ì§€ (Label ê³„ì‚° ì•ˆì •ì„±)
full_ids = tokenizer(
    full_text,
    max_length=1024 + 200,  # 1224
    truncation=True,  # right truncation (ê¸°ë³¸ê°’)
    ...
)

# 3. ì¶”ë¡  ì‹œ: Left truncation ì‚¬ìš© (Assistant í—¤ë” ë³´ì¡´)
if template_type:
    tokenizer.padding_side = "left"
    tokenizer.truncation_side = "left"  # â† í•µì‹¬!

inputs = tokenizer(
    batch_prompts,
    max_length=1024,
    truncation=True,
    ...
)

# 4. ìƒì„± ì—¬ìœ  í™•ë³´
outputs = model.generate(
    **inputs,
    max_new_tokens=150,  # 100 â†’ 150
    ...
)
```

**ì£¼ì˜ì‚¬í•­:**
- **í•™ìŠµ ì‹œ left truncation ê¸ˆì§€**: Label ê³„ì‚° ë¡œì§ì´ í‹€ì–´ì§ (prompt ê¸¸ì´ ë¶ˆì¼ì¹˜)
- **ì¶”ë¡  ì‹œ left truncation í•„ìˆ˜**: Assistant í—¤ë” ë³´ì¡´ì´ ìµœìš°ì„ 
- **ì‹¤ì¸¡ ë°ì´í„° ê¸°ë°˜ ì„¤ì •**: í† í¬ë‚˜ì´ì €ë¡œ ì‹¤ì œ ê¸¸ì´ ì¸¡ì • í›„ max_length ê²°ì •

**ì˜ˆìƒ ì„±ëŠ¥ ì˜í–¥:**
- Prompt truncation 6% ë°œìƒ ì‹œ: **-20~30 ROUGE points**
- í•´ê²° ì‹œ ì¦‰ê°ì ì¸ ì„±ëŠ¥ íšŒë³µ

## Fine-tuning Progress & Results

### ì™„ë£Œëœ ëª¨ë¸ (Completed)
| ëª¨ë¸ | ROUGE ì ìˆ˜ | ë¹„ê³  |
|------|------------|------|
| **KoBART** | **94.51** | ê°€ì¥ ë†’ì€ ì„±ëŠ¥, Encoder-Decoder ìµœê³  |

### ì œì™¸ëœ ëª¨ë¸ (Excluded)
| ëª¨ë¸ | ìƒíƒœ | ì´ìœ  |
|------|------|------|
| KoT5 | âŒ Excluded | Overflow error (numerical instability) |

### ì§„í–‰ ì¤‘ ëª¨ë¸ (In Progress)
| ëª¨ë¸ | ìƒíƒœ | ì§„í–‰ë¥  |
|------|------|--------|
| **Llama-3.2-Korean-3B** | ğŸ”„ Training | 33% (Step 127/390) |

### ëŒ€ê¸° ì¤‘ ëª¨ë¸ (Pending)
| ëª¨ë¸ | ìš°ì„ ìˆœìœ„ | ë¹„ê³  |
|------|----------|------|
| Qwen3-4B-Instruct | High | 4th place zero-shot (45.02) |
| Qwen2.5-7B-Instruct | High | 3rd place zero-shot (46.84) |
| Llama-3-Korean-8B | Medium | 2nd place zero-shot (48.61) |

### ì„±ëŠ¥ ë¹„êµ ê¸°ì¤€
- **Baseline**: Random reference selection ~ 70 points
- **Target**: 90+ points (KoBART ìˆ˜ì¤€ ì´ìƒ)
- **í‰ê°€ ì§€í‘œ**: ROUGE-1-F1 + ROUGE-2-F1 + ROUGE-L-F1 í‰ê· 

## í•™ìŠµ ì•ˆì •ì„± ì²´í¬ë¦¬ìŠ¤íŠ¸

íŒŒì¸íŠœë‹ ì‹œì‘ ì „ ë°˜ë“œì‹œ í™•ì¸í•  ì‚¬í•­:

1. **ë””ìŠ¤í¬ ìš©ëŸ‰ í™•ì¸**
   ```bash
   du -sh / 2>/dev/null
   # 110GB ì´í•˜ ìœ ì§€ (150GB í•œê³„)
   ```

2. **Chat Template í† í° ì¶”ê°€** (LLM ëª¨ë¸ë§Œ í•´ë‹¹)
   - Llama: `<|start_header_id|>`, `<|end_header_id|>`, `<|eot_id|>`
   - Qwen: `<|im_start|>`, `<|im_end|>`

3. **QLoRA compute_dtype í™•ì¸**
   - Llama: `torch.bfloat16` + `bf16=True`
   - Qwen: `torch.float16` + `fp16=True`

4. **metric_for_best_model ì„¤ì •**
   - Encoder-Decoder: `"rouge_sum"` + `greater_is_better=True`
   - Causal LM: `"eval_loss"` + `greater_is_better=False`

5. **íŠ¹ìˆ˜ í† í° ì¶”ê°€**
   ```python
   special_tokens = ['#Person1#', '#Person2#', ..., '#PhoneNumber#', '#Address#', ...]
   tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})
   ```

6. **ëª¨ë¸ ì„ë² ë”© í¬ê¸° ì¡°ì •**
   ```python
   model.resize_token_embeddings(len(tokenizer))
   ```

7. **Max Length ì„¤ì • ê²€ì¦** (LLM ëª¨ë¸ í•„ìˆ˜!)
   - ì‹¤ì œ í† í° ê¸¸ì´ í†µê³„ ìˆ˜ì§‘ í›„ ì„¤ì •
   - encoder_max_len: 1024 ì´ìƒ ê¶Œì¥ (512ëŠ” 6% ì˜ë¦¼!)
   - decoder_max_len: 200 ì´ìƒ ê¶Œì¥
   - Left truncation ì¶”ë¡  ì‹œ ì ìš©: `tokenizer.truncation_side = "left"`