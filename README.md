# ğŸ—¨ï¸ Korean Dialogue Summarization Competition

> í•œêµ­ì–´ ëŒ€í™” ìš”ì•½ ê²½ì§„ëŒ€íšŒ í”„ë¡œì íŠ¸

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

í•œêµ­ì–´ ëŒ€í™” ë°ì´í„°ë¥¼ ì…ë ¥ë°›ì•„ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” **Seq2Seq ëª¨ë¸**ì„ ê°œë°œí•˜ëŠ” ê²½ì§„ëŒ€íšŒì…ë‹ˆë‹¤.

- **í‰ê°€ ì§€í‘œ**: ROUGE-1, ROUGE-2, ROUGE-L F1 í‰ê·  (3ê°œì˜ ì°¸ì¡° ìš”ì•½ ëŒ€ë¹„)
- **ëª¨ë¸**: BART ê¸°ë°˜ í•œêµ­ì–´ ìš”ì•½ ëª¨ë¸
- **ë°ì´í„°**: DialogSum í•œêµ­ì–´ ë²ˆì—­ ë²„ì „ (Train 12,457 / Dev 499 / Test 250)

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ìƒˆë¡œìš´ ëª¨ë“ˆí™” êµ¬ì¡° (ê¶Œì¥)

```bash
cd /Competition/NLP/dialogue-summarization

# í•™ìŠµ
python scripts/train_baseline.py --config configs/train_config.yaml

# ì¶”ë¡ 
python scripts/generate_predictions.py \
    --config configs/base_config.yaml \
    --checkpoint checkpoints/baseline_run/checkpoint-1750
```

### ì›ë³¸ Baseline (ì°¸ê³ ìš©)

```bash
cd /Competition/NLP/naturallanguageprocessingcompetition-nlp-1/code

# Jupyter Notebook
jupyter notebook baseline.ipynb
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
/Competition/NLP/
â”œâ”€â”€ dialogue-summarization/          # â­ ìƒˆë¡œìš´ ëª¨ë“ˆí™” í”„ë¡œì íŠ¸
â”‚   â”œâ”€â”€ src/                         # ëª¨ë“ˆ ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”‚   â”œâ”€â”€ data/                    # ë°ì´í„° ì „ì²˜ë¦¬
â”‚   â”‚   â”œâ”€â”€ models/                  # ëª¨ë¸ ë¡œë”
â”‚   â”‚   â”œâ”€â”€ evaluation/              # ROUGE ë©”íŠ¸ë¦­ (konlpy ì œê±°!)
â”‚   â”‚   â””â”€â”€ utils/                   # ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ scripts/                     # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”‚   â”œâ”€â”€ train_baseline.py
â”‚   â”‚   â””â”€â”€ generate_predictions.py
â”‚   â”œâ”€â”€ configs/                     # YAML ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ train_demo.ipynb            # í•™ìŠµ ì¬í˜„ ë…¸íŠ¸ë¶
â”‚   â”œâ”€â”€ inference_demo.ipynb        # ì¶”ë¡  ì¬í˜„ ë…¸íŠ¸ë¶
â”‚   â”œâ”€â”€ EXPERIMENT_LOG.md           # ì‹¤í—˜ ê¸°ë¡
â”‚   â””â”€â”€ README.md                    # ìƒì„¸ ê°€ì´ë“œ
â”‚
â”œâ”€â”€ naturallanguageprocessingcompetition-nlp-1/  # Git ì €ì¥ì†Œ
â”‚   â”œâ”€â”€ data/                        # ì›ë³¸ ë°ì´í„°ì…‹
â”‚   â”‚   â”œâ”€â”€ train.csv
â”‚   â”‚   â”œâ”€â”€ dev.csv
â”‚   â”‚   â””â”€â”€ test.csv
â”‚   â”œâ”€â”€ code/
â”‚   â”‚   â””â”€â”€ baseline.ipynb           # ê²€ì¦ëœ baseline (ROUGE Sum ~47)
â”‚   â””â”€â”€ .git/                        # âš ï¸ Git ì‘ì—…ì€ ì—¬ê¸°ì„œë§Œ!
â”‚
â”œâ”€â”€ docs/                            # ê²½ì§„ëŒ€íšŒ ë¬¸ì„œ
â”‚   â”œâ”€â”€ Competition_Overview/
â”‚   â””â”€â”€ Competition_Advanced/
â”‚
â”œâ”€â”€ .claude/                         # Epic/Task ê´€ë¦¬
â””â”€â”€ CLAUDE.md                        # Claude Code ê°€ì´ë“œ
```

## âš¡ ì£¼ìš” íŠ¹ì§•

### dialogue-summarization/ (ìƒˆ êµ¬ì¡°)

- âœ… **Java ì˜ì¡´ì„± ì œê±°** - konlpy ëŒ€ì‹  rouge ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
- âœ… **ëª¨ë“ˆí™”** - baseline.ipynbë¥¼ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“ˆë¡œ ë¶„í•´
- âœ… **íƒ€ì… ì•ˆì „ì„±** - ëª¨ë“  í•¨ìˆ˜ì— Type Hints ì ìš©
- âœ… **YAML ì„¤ì •** - í•˜ì´í¼íŒŒë¼ë¯¸í„° ê´€ë¦¬ ìš©ì´
- âœ… **ì¬í˜„ ë…¸íŠ¸ë¶** - í•™ìŠµ/ì¶”ë¡ ì„ ì‰½ê²Œ ì¬í˜„ ê°€ëŠ¥
- âœ… **ë¬¸ì„œí™”** - Docstring ë° ê°€ì´ë“œ ì™„ë¹„

### baseline.ipynb (ì›ë³¸)

- âœ… **ê²€ì¦ëœ ì„±ëŠ¥** - ROUGE Sum ~47 ë‹¬ì„±
- âœ… **ë‹¨ìˆœ êµ¬ì¡°** - ë‹¨ì¼ ë…¸íŠ¸ë¶ì—ì„œ ì™„ê²°
- âš ï¸ **Java ì˜ì¡´ì„±** - konlpy í•„ìš” (JVM ì„¤ì¹˜ í•„ìš”)
- âš ï¸ **ì¬ì‚¬ìš©ì„± ë‚®ìŒ** - ëª¨ë“ˆí™”ë˜ì§€ ì•ŠìŒ

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

### Dev Set (Validation)
| ëª¨ë¸ | ROUGE-1 | ROUGE-2 | ROUGE-L | ROUGE Sum |
|------|---------|---------|---------|-----------|
| baseline.ipynb | ~16 | ~9 | ~14 | **~47** |
| Modular Structure | 32.28 | 13.46 | 30.03 | **75.77** |

### Test Set (ê²½ì§„ëŒ€íšŒ ì œì¶œ)
| ì‹¤í—˜ | ROUGE-1 | ROUGE-2 | ROUGE-L | Final Score |
|------|---------|---------|---------|-------------|
| **Experiment #1** (Baseline) | **0.5660** | **0.3675** | **0.4719** | **46.8487** |

**ğŸ“ ìƒì„¸ ê¸°ë¡**: [dialogue-summarization/EXPERIMENT_LOG.md](./dialogue-summarization/EXPERIMENT_LOG.md)

## âš ï¸ Git ì‘ì—… ì£¼ì˜ì‚¬í•­

**ì ˆëŒ€ í—·ê°ˆë¦¬ì§€ ë§ ê²ƒ!**

```bash
# âœ… ì˜¬ë°”ë¥¸ Git ì‘ì—… ë””ë ‰í† ë¦¬
cd /Competition/NLP/naturallanguageprocessingcompetition-nlp-1
git status
git add .
git commit -m "message"
git push origin main

# âŒ ì˜ëª»ëœ ë””ë ‰í† ë¦¬ - ë£¨íŠ¸ì—ì„œ Git ëª…ë ¹ ê¸ˆì§€!
cd /Competition/NLP
git add .  # ì‘ë™ ì•ˆ í•¨!
```

**Git ì €ì¥ì†Œ**: `https://github.com/Bkankim/naturallanguageprocessingcompetition-nlp-1.git`

## ğŸ“š ë¬¸ì„œ

- **í”„ë¡œì íŠ¸ ê°€ì´ë“œ**: `dialogue-summarization/README.md`
- **ì‹¤í—˜ ê¸°ë¡**: `dialogue-summarization/EXPERIMENT_LOG.md`
- **ê²½ì§„ëŒ€íšŒ ì†Œê°œ**: `docs/Competition_Overview/introduction.md`
- **ë°ì´í„° ì„¤ëª…**: `docs/Competition_Overview/data_overview.md`
- **í‰ê°€ ë°©ë²•**: `docs/Competition_Overview/evaluation_method.md`
- **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**: `docs/Competition_Advanced/hyperparameter_tuning_tip.md`
- **Claude ê°€ì´ë“œ**: `CLAUDE.md`

## ğŸ”§ ê°œë°œ í™˜ê²½

### í•„ìˆ˜ íŒ¨í‚¤ì§€

```bash
# Python 3.10+
pip install torch transformers datasets
pip install rouge pandas pyyaml tqdm

# âš ï¸ ìƒˆ êµ¬ì¡°ëŠ” konlpy ë¶ˆí•„ìš”!
# pip install konlpy  # baseline.ipynbë§Œ í•„ìš”
```

### PM CLI ì‚¬ìš©

```bash
/pm:install     # ì „ì²´ ì˜ì¡´ì„± ì„¤ì¹˜
/pm:add rouge   # ìƒˆ íŒ¨í‚¤ì§€ ì¶”ê°€
/pm:run test    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```

## ğŸ¯ ì‹¤í—˜ ì§„í–‰ ìƒí™©

### âœ… ì™„ë£Œëœ ì‹¤í—˜

#### Experiment #1: koBART Baseline
- **ëª¨ë¸**: gogamza/kobart-base-v2
- **ìµœì¢… ì„±ëŠ¥**: ROUGE Sum **94.51** (Dev Set)
  - ROUGE-1: 51.98
  - ROUGE-2: 26.61
  - ROUGE-L: 47.03
- **í•™ìŠµ ì„¤ì •**: 20 epochs, lr=5e-5, batch=8
- **ìƒíƒœ**: âœ… ì™„ë£Œ

#### Technical Fixes Applied
1. **Metric Configuration Fix**
   - ë¬¸ì œ: `metric_for_best_model`ì´ configì— í•˜ë“œì½”ë”©ë˜ì–´ ìˆì–´ ë‹¤ë¥¸ ë©”íŠ¸ë¦­ ì‚¬ìš© ë¶ˆê°€
   - í•´ê²°: TrainingArgumentsì—ì„œ ë™ì ìœ¼ë¡œ ì„¤ì •í•˜ë„ë¡ ìˆ˜ì •

2. **Chat Template Tokens**
   - ë¬¸ì œ: LLM ëª¨ë¸ì˜ chat template í† í°ë“¤ì´ tokenizerì— ì¶”ê°€ë˜ì§€ ì•ŠìŒ
   - í•´ê²°: ëª¨ë“  chat template í† í° ìë™ ì¶”ì¶œ ë° ì¶”ê°€

3. **QLoRA Compute Dtype Alignment**
   - ë¬¸ì œ: ëª¨ë¸ë³„ë¡œ ë‹¤ë¥¸ dtype ì‚¬ìš© (Llama=bf16, Qwen=fp16)
   - í•´ê²°: ëª¨ë¸ë³„ compute_dtypeì„ ëª¨ë¸ì˜ ê¸°ë³¸ dtypeê³¼ ì¼ì¹˜ì‹œí‚´

### â³ ì§„í–‰ ì¤‘ì¸ ì‹¤í—˜

#### Experiment #2: LLM Fine-tuning with QLoRA 4bit
- **ì‹¤í—˜ ì‹œì‘**: 2025-10-04 14:22 KST
- **W&B ì¶”ì **: [dialogue-summarization-finetuning](https://wandb.ai/bkan-ai/dialogue-summarization-finetuning)

**ëª¨ë¸ í•™ìŠµ ìˆœì„œ** (Sequential Training):
1. ğŸ”„ **Llama-3.2-Korean-3B** - In Progress (33% ì™„ë£Œ)
   - Bllossom/llama-3.2-Korean-Bllossom-3B
   - QLoRA 4bit, bf16 compute dtype

2. â¸ï¸ **Qwen3-4B-Instruct** - Pending
   - Qwen/Qwen3-4B-Instruct-2507
   - QLoRA 4bit, fp16 compute dtype

3. â¸ï¸ **Qwen2.5-7B-Instruct** - Pending
   - Qwen/Qwen2.5-7B-Instruct
   - QLoRA 4bit, fp16 compute dtype

4. â¸ï¸ **Llama-3-Korean-8B** - Pending
   - MLP-KTLim/llama-3-Korean-Bllossom-8B
   - QLoRA 4bit, bf16 compute dtype

**í•™ìŠµ ì„¤ì •**:
- LoRA: r=16, alpha=32, dropout=0.1
- Target modules: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj
- Batch size: 4 (gradient_accumulation_steps=2)
- Learning rate: 2e-4
- Max epochs: 3
- Early stopping: patience=2

### ğŸ“Š ìŠ¤í† ë¦¬ì§€ í˜„í™©
- **í˜„ì¬ ì‚¬ìš©ëŸ‰**: ~110GB / 150GB
- **ì•ˆì „ ë§ˆì§„**: 40GB ì—¬ìœ 
- **ìƒíƒœ**: âœ… ì •ìƒ

### ğŸ“ ë‹¤ìŒ ë‹¨ê³„

1. âœ… ê¸°ì¡´ ì½”ë“œ ë°±ì—… ì™„ë£Œ
2. âœ… ìƒˆë¡œìš´ ëª¨ë“ˆí™” êµ¬ì¡° êµ¬ì¶• ì™„ë£Œ
3. âœ… Java ì˜ì¡´ì„± ì œê±° ì™„ë£Œ
4. âœ… ì‹¤ì œ í•™ìŠµ ì‹¤í–‰ ë° ì„±ëŠ¥ ê²€ì¦ ì™„ë£Œ
5. âœ… ì‹¤í—˜ ê¸°ë¡ ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ
6. âœ… Git ì €ì¥ì†Œ ë™ê¸°í™”
7. âœ… koBART Fine-tuning ì™„ë£Œ (ROUGE Sum: 94.51)
8. âœ… Critical Issues ìˆ˜ì • ì™„ë£Œ
9. ğŸ”„ LLM Fine-tuning ì§„í–‰ ì¤‘ (4ê°œ ëª¨ë¸ ìˆœì°¨ í•™ìŠµ)
10. â³ ìµœì¢… ëª¨ë¸ ì„ íƒ ë° ì•™ìƒë¸” (ì˜ˆì •)

## ğŸ“ ë¼ì´ì„ ìŠ¤

ê²½ì§„ëŒ€íšŒ ì œì¶œìš© í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

---

**Built with** â¤ï¸ **by Claude Code**
