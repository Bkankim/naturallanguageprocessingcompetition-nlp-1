"""
ëª¨ë¸ ìŠ¤í¬ë¦¬ë‹ ìŠ¤í¬ë¦½íŠ¸ (QLoRA 4bit + W&B)

ì—¬ëŸ¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ QLoRA 4bit ì–‘ìí™”ë¡œ ë¡œë“œí•˜ì—¬
Dev setì—ì„œ zero-shot ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- QLoRA 4bit ë¡œë”© (BitsAndBytes)
- ìˆœì°¨ ì‹¤í–‰ + ìë™ ìºì‹œ ì‚­ì œ
- ë””ìŠ¤í¬ 80GB ê´€ë¦¬
- W&B ì‹¤ì‹œê°„ ë¡œê¹…
- ROUGE ë©”íŠ¸ë¦­ í‰ê°€

ì‚¬ìš©ë²•:
    python scripts/model_screening.py --config configs/screening_config.yaml
"""

import os
import gc
import sys
import yaml
import shutil
import argparse
import subprocess
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional
from datetime import datetime

import torch
import pandas as pd
from tqdm import tqdm
from torch.utils.data import DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    AutoModelForCausalLM,
    BitsAndBytesConfig
)

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
sys.path.append(str(Path(__file__).parent.parent))

from src.data.preprocessor import Preprocess
from src.data.dataset import DatasetForInference
from src.evaluation.metrics import calculate_rouge_scores

# RTX 3090 ìµœì í™”: TF32 í™œì„±í™” (Ampere ì•„í‚¤í…ì²˜)
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
print("âœ… TF32 í™œì„±í™” (RTX 3090 ìµœì í™”)")

# W&B import (optional)
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("âš ï¸  wandb not installed. Install with: pip install wandb")


def check_disk_usage() -> float:
    """
    í˜„ì¬ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ì„ ì²´í¬í•©ë‹ˆë‹¤ (GB ë‹¨ìœ„).

    Returns:
        float: í˜„ì¬ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ (GB)
    """
    # /Competition, /opt, /data, /root ë””ë ‰í† ë¦¬ í¬ê¸° í•©ì‚°
    total_gb = 0.0

    for directory in ['/Competition', '/opt', '/data', '/root']:
        try:
            result = subprocess.run(
                ['du', '-sb', directory],
                capture_output=True,
                text=True,
                timeout=30
            )
            if result.returncode == 0:
                size_bytes = int(result.stdout.split()[0])
                total_gb += size_bytes / (1024**3)
        except Exception as e:
            print(f"âš ï¸  {directory} í¬ê¸° ì¸¡ì • ì‹¤íŒ¨: {e}")

    return total_gb


def cleanup_hf_cache(model_name: Optional[str] = None):
    """
    HuggingFace ìºì‹œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.

    Args:
        model_name: íŠ¹ì • ëª¨ë¸ ìºì‹œë§Œ ì‚­ì œ (Noneì´ë©´ ì „ì²´ ì‚­ì œí•˜ì§€ ì•ŠìŒ)
    """
    cache_dir = Path.home() / ".cache" / "huggingface" / "hub"

    if not cache_dir.exists():
        return

    if model_name:
        # íŠ¹ì • ëª¨ë¸ ìºì‹œë§Œ ì‚­ì œ
        model_cache_pattern = model_name.replace("/", "--")
        for cache_path in cache_dir.glob(f"models--{model_cache_pattern}*"):
            try:
                shutil.rmtree(cache_path)
                print(f"âœ… ìºì‹œ ì‚­ì œ ì™„ë£Œ: {cache_path.name}")
            except Exception as e:
                print(f"âš ï¸  ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {e}")

    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

    gc.collect()


def load_config(config_path: str) -> Dict[str, Any]:
    """YAML ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    with open(config_path, "r", encoding="utf-8") as file:
        config = yaml.safe_load(file)
    return config


def setup_bnb_config(config: Dict[str, Any]) -> BitsAndBytesConfig:
    """
    BitsAndBytes 4bit ì–‘ìí™” ì„¤ì •ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬

    Returns:
        BitsAndBytesConfig: 4bit ì–‘ìí™” ì„¤ì •
    """
    qlora_config = config['qlora']

    return BitsAndBytesConfig(
        load_in_4bit=qlora_config['load_in_4bit'],
        bnb_4bit_compute_dtype=getattr(torch, qlora_config['bnb_4bit_compute_dtype']),
        bnb_4bit_quant_type=qlora_config['bnb_4bit_quant_type'],
        bnb_4bit_use_double_quant=qlora_config['bnb_4bit_use_double_quant'],
    )


def load_model_and_tokenizer(
    model_name: str,
    bnb_config: Optional[BitsAndBytesConfig],
    special_tokens: List[str],
    device: torch.device
) -> Tuple[Any, Any]:
    """
    ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        model_name: HuggingFace ëª¨ë¸ëª…
        bnb_config: BitsAndBytes ì„¤ì • (Noneì´ë©´ ì¼ë°˜ ë¡œë”©)
        special_tokens: ì¶”ê°€í•  special tokens
        device: ë””ë°”ì´ìŠ¤

    Returns:
        Tuple[model, tokenizer, model_type]
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
    print(f"{'='*80}")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )

    # ëª¨ë¸ ë¡œë”© ì„¤ì • (TF32 ìµœì í™”)
    load_kwargs = {
        "trust_remote_code": True
    }

    if bnb_config:
        load_kwargs["quantization_config"] = bnb_config
        load_kwargs["device_map"] = "auto"
        quant_info = "4bit ì–‘ìí™” + TF32"
    else:
        load_kwargs["torch_dtype"] = torch.float16
        quant_info = "FP16 + TF32"

    # ëª¨ë¸ íƒ€ì… ê°ì§€ (Seq2Seq vs CausalLM)
    try:
        # Seq2Seq ëª¨ë¸ ì‹œë„ (BART, T5 ë“±)
        model = AutoModelForSeq2SeqLM.from_pretrained(
            model_name,
            **load_kwargs
        )
        model_type = "seq2seq"
        # Seq2SeqëŠ” right padding
        tokenizer.padding_side = 'right'
    except:
        # CausalLM ëª¨ë¸ (GPT, Llama, Qwen ë“±)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            **load_kwargs
        )
        model_type = "causal"
        # CausalLMì€ left padding
        tokenizer.padding_side = 'left'

    # pad_token ì„¤ì • (ì—†ëŠ” ê²½ìš° eos_token ì‚¬ìš©)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Special tokens ì¶”ê°€
    if special_tokens:
        tokenizer.add_special_tokens({
            'additional_special_tokens': special_tokens
        })

    # GPUë¡œ ì´ë™ (4bitì´ ì•„ë‹Œ ê²½ìš°)
    if not bnb_config:
        model = model.to(device)

    # Tokenizer resize
    model.resize_token_embeddings(len(tokenizer))

    print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ (íƒ€ì…: {model_type}, {quant_info})")
    print(f"   íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters() / 1e9:.2f}B")
    print(f"   Padding side: {tokenizer.padding_side}")

    return model, tokenizer, model_type


def apply_chat_template(dialogue: str, model_name: str, tokenizer: Any) -> str:
    """
    ëª¨ë¸ë³„ Chat Templateì„ ì ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        dialogue: ì›ë³¸ ëŒ€í™” í…ìŠ¤íŠ¸
        model_name: ëª¨ë¸ëª… (chat template ì„ íƒìš©)
        tokenizer: í† í¬ë‚˜ì´ì €

    Returns:
        str: Chat templateì´ ì ìš©ëœ í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
    """
    # ìš”ì•½ instruction (êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸)
    system_message = (
        "ë‹¹ì‹ ì€ ëŒ€í™” ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
        "- ë°˜ë“œì‹œ í•œêµ­ì–´ë§Œ ì‚¬ìš©í•˜ì„¸ìš” (ì˜ë¬¸/ì¼ë¬¸/ë² íŠ¸ë‚¨ì–´/ì´ëª¨ì§€/URL ê¸ˆì§€).\n"
        "- 2~3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.\n"
        "- ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´, ì°½ì‘ì€ í•˜ì§€ ë§ˆì„¸ìš”."
    )
    user_message = f"ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ì„¸ìš”:\n---\n{dialogue}"

    # Chat template ì ìš© (ëª¨ë¸ì´ ì§€ì›í•˜ëŠ” ê²½ìš°)
    if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template:
        try:
            messages = [
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_message}
            ]
            prompt = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            return prompt
        except Exception as e:
            print(f"âš ï¸  Chat template ì ìš© ì‹¤íŒ¨, ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {e}")

    # Chat templateì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
    return f"{system_message}\n\n{user_message}"


def prepare_dev_dataset(
    config: Dict[str, Any],
    tokenizer: Any,
    model_name: str,
    model_type: str
) -> Tuple[pd.DataFrame, DataLoader]:
    """
    Dev ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

    Returns:
        Tuple[dev_data, dataloader]
    """
    # Dev ë°ì´í„° ë¡œë“œ
    dev_file_path = os.path.join(config['general']['data_path'], 'dev.csv')
    dev_data = pd.read_csv(dev_file_path)

    # CausalLM ëª¨ë¸ì¸ ê²½ìš° chat template ì ìš©
    if model_type == "causal":
        encoder_input_dev = [
            apply_chat_template(dialogue, model_name, tokenizer)
            for dialogue in dev_data['dialogue'].tolist()
        ]
    else:
        # Seq2Seq ëª¨ë¸ì€ ì›ë³¸ ëŒ€í™” ì‚¬ìš© (ì´ë¯¸ ìš”ì•½ í•™ìŠµë¨)
        encoder_input_dev = dev_data['dialogue'].tolist()

    # í† í¬ë‚˜ì´ì§•
    tokenized_encoder_inputs = tokenizer(
        encoder_input_dev,
        return_tensors="pt",
        padding=True,
        add_special_tokens=True,
        truncation=True,
        max_length=config['tokenizer']['encoder_max_len'],
        return_token_type_ids=False,
    )

    # ì¶”ë¡ ìš© ë°ì´í„°ì…‹ ìƒì„±
    dev_dataset = DatasetForInference(
        tokenized_encoder_inputs,
        dev_data['fname'].tolist(),
        len(encoder_input_dev)
    )

    # ë°ì´í„°ë¡œë” ìƒì„±
    dataloader = DataLoader(
        dev_dataset,
        batch_size=config['inference']['batch_size'],
        shuffle=False
    )

    print(f"âœ… Dev ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: {len(dev_data)} samples")
    if model_type == "causal":
        print(f"   Chat template ì ìš©: {len(encoder_input_dev)}ê°œ í”„ë¡¬í”„íŠ¸ ìƒì„±")
        print(f"   ìƒ˜í”Œ í”„ë¡¬í”„íŠ¸ (ì²˜ìŒ 200ì):\n{encoder_input_dev[0][:200]}...")

    return dev_data, dataloader


def generate_bad_words_ids(tokenizer) -> List[List[int]]:
    """
    ì™¸êµ­ì–´ ë¬¸ìë¥¼ í¬í•¨í•œ í† í°ì˜ bad_words_idsë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ë¼í‹´/íˆë¼ê°€ë‚˜/ê°€íƒ€ì¹´ë‚˜/CJK í•œìë¥¼ í¬í•¨í•˜ëŠ” í† í°ì„ ì°¨ë‹¨í•©ë‹ˆë‹¤.

    Args:
        tokenizer: HuggingFace í† í¬ë‚˜ì´ì €

    Returns:
        List[List[int]]: bad_words_ids ë¦¬ìŠ¤íŠ¸
    """
    bad_ids = []
    vocab_size = len(tokenizer)

    for token_id in range(vocab_size):
        try:
            # í† í°ì„ ë””ì½”ë”©
            token_str = tokenizer.decode([token_id], skip_special_tokens=True)
            if not token_str:  # ë¹ˆ ë¬¸ìì—´ì€ ìŠ¤í‚µ
                continue

            # ê° ë¬¸ìë¥¼ ì²´í¬
            is_bad = False
            for ch in token_str:
                code = ord(ch)
                # ë¼í‹´ ê¸°ë³¸ A-Z / a-z
                if 0x41 <= code <= 0x5A or 0x61 <= code <= 0x7A:
                    is_bad = True
                    break
                # ë¼í‹´ í™•ì¥ (ë² íŠ¸ë‚¨ì–´ ë“± ë””ì•¡ë¦¬í‹± í¬í•¨)
                if 0x00C0 <= code <= 0x024F:
                    is_bad = True
                    break
                # íˆë¼ê°€ë‚˜
                if 0x3040 <= code <= 0x309F:
                    is_bad = True
                    break
                # ê°€íƒ€ì¹´ë‚˜
                if 0x30A0 <= code <= 0x30FF:
                    is_bad = True
                    break
                # CJK í†µí•© í•œì (Unified Ideographs)
                if 0x4E00 <= code <= 0x9FFF:
                    is_bad = True
                    break

            if is_bad:
                bad_ids.append([token_id])

        except Exception:
            continue

    return bad_ids


def generate_summaries(
    model: Any,
    model_type: str,
    tokenizer: Any,
    dataloader: DataLoader,
    config: Dict[str, Any],
    device: torch.device
) -> List[str]:
    """
    ëª¨ë¸ë¡œ ìš”ì•½ë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Returns:
        List[str]: ìƒì„±ëœ ìš”ì•½ë¬¸ ë¦¬ìŠ¤íŠ¸
    """
    model.eval()
    summaries = []

    # bad_words_ids ë°©ì‹ìœ¼ë¡œ ì™¸êµ­ì–´ ì°¨ë‹¨ (ë¼í‹´/ì¼ë³¸ì–´/CJK í•œì)
    print("\nğŸš« ì™¸êµ­ì–´ í† í° ì°¨ë‹¨ ëª©ë¡ ìƒì„± ì¤‘...")
    bad_words_ids = generate_bad_words_ids(tokenizer)
    print(f"   ì°¨ë‹¨ ëŒ€ìƒ í† í° ìˆ˜: {len(bad_words_ids)}ê°œ")

    print("\nğŸ”® ìš”ì•½ ìƒì„± ì¤‘...")

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="ìƒì„±"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)

            if model_type == "seq2seq":
                # Seq2Seq ëª¨ë¸ (BART, T5)
                generated_ids = model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_length=config['inference']['generate_max_length'],
                    num_beams=config['inference']['num_beams'],
                    no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                    early_stopping=config['inference']['early_stopping'],
                    length_penalty=1.0,
                    repetition_penalty=1.1,
                    bad_words_ids=bad_words_ids,
                )
            else:
                # CausalLM ëª¨ë¸ (Llama, Qwen, SOLAR)
                generated_ids = model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_new_tokens=config['inference']['generate_max_length'],
                    num_beams=config['inference']['num_beams'],
                    no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                    early_stopping=config['inference']['early_stopping'],
                    length_penalty=1.0,
                    repetition_penalty=1.1,
                    bad_words_ids=bad_words_ids,
                )
                # Input ë¶€ë¶„ ì œê±° (CausalLMì€ inputì„ í¬í•¨í•˜ì—¬ ìƒì„±)
                generated_ids = generated_ids[:, input_ids.shape[1]:]

            # ë””ì½”ë”© (special tokens ì œê±°)
            for ids in generated_ids:
                result = tokenizer.decode(ids, skip_special_tokens=True)
                summaries.append(result)

    print(f"âœ… ìš”ì•½ ìƒì„± ì™„ë£Œ: {len(summaries)}ê°œ")
    print(f"   ìƒ˜í”Œ ìš”ì•½ (ì²˜ìŒ 3ê°œ):")
    for i in range(min(3, len(summaries))):
        print(f"   [{i+1}] {summaries[i][:100]}...")

    return summaries


def evaluate_model(
    model_name: str,
    nickname: str,
    config: Dict[str, Any],
    wandb_run: Optional[Any] = None
) -> Dict[str, Any]:
    """
    ë‹¨ì¼ ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤.

    Returns:
        Dict: í‰ê°€ ê²°ê³¼ {model_name, rouge-1, rouge-2, rouge-l, rouge_sum}
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # ë””ìŠ¤í¬ ì²´í¬
    if config['disk_management']['check_before_download']:
        disk_usage_before = check_disk_usage()
        print(f"\nğŸ’¾ í˜„ì¬ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰: {disk_usage_before:.2f} GB")

        if disk_usage_before > config['general']['max_disk_usage_gb']:
            raise RuntimeError(
                f"ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ì´ˆê³¼! "
                f"{disk_usage_before:.2f}GB > {config['general']['max_disk_usage_gb']}GB"
            )

    try:
        # BitsAndBytes ì„¤ì • (4bitì´ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ)
        bnb_config = None
        if config['qlora']['load_in_4bit']:
            bnb_config = setup_bnb_config(config)

        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        model, tokenizer, model_type = load_model_and_tokenizer(
            model_name=model_name,
            bnb_config=bnb_config,
            special_tokens=config['tokenizer']['special_tokens'],
            device=device
        )

        # Dev ë°ì´í„°ì…‹ ì¤€ë¹„
        dev_data, dataloader = prepare_dev_dataset(
            config=config,
            tokenizer=tokenizer,
            model_name=model_name,
            model_type=model_type
        )

        # ìš”ì•½ ìƒì„±
        summaries = generate_summaries(
            model=model,
            model_type=model_type,
            tokenizer=tokenizer,
            dataloader=dataloader,
            config=config,
            device=device
        )

        # í›„ì²˜ë¦¬: ë¶ˆí•„ìš”í•œ í† í° ì œê±°
        from src.evaluation.metrics import clean_text
        remove_tokens = config['inference']['remove_tokens']
        cleaned_summaries = clean_text(summaries, remove_tokens)

        # ROUGE í‰ê°€ (ë¬¸ì ë‹¨ìœ„ í† í°í™”)
        print("\nğŸ“Š ROUGE í‰ê°€ ì¤‘ (Mecab í˜•íƒœì†Œ í† í°í™”)...")
        references = dev_data['summary'].tolist()

        rouge_scores = calculate_rouge_scores(
            predictions=cleaned_summaries,
            references=references,
            remove_tokens=remove_tokens,
            tokenization_mode='mecab'  # ëŒ€íšŒ ê³µì‹ í‰ê°€ ë°©ì‹
        )

        # ì ìˆ˜ ì¶”ì¶œ
        rouge_1 = rouge_scores['rouge-1']['f'] * 100
        rouge_2 = rouge_scores['rouge-2']['f'] * 100
        rouge_l = rouge_scores['rouge-l']['f'] * 100
        rouge_sum = rouge_1 + rouge_2 + rouge_l

        print(f"\n{'='*80}")
        print(f"âœ… {nickname} í‰ê°€ ì™„ë£Œ")
        print(f"{'='*80}")
        print(f"ROUGE-1: {rouge_1:.2f}")
        print(f"ROUGE-2: {rouge_2:.2f}")
        print(f"ROUGE-L: {rouge_l:.2f}")
        print(f"ROUGE Sum: {rouge_sum:.2f}")
        print(f"{'='*80}\n")

        # W&B ë¡œê¹…
        if wandb_run:
            wandb_run.log({
                f"{nickname}/rouge-1": rouge_1,
                f"{nickname}/rouge-2": rouge_2,
                f"{nickname}/rouge-l": rouge_l,
                f"{nickname}/rouge_sum": rouge_sum,
            })

        result = {
            'model_name': model_name,
            'nickname': nickname,
            'rouge-1': rouge_1,
            'rouge-2': rouge_2,
            'rouge-l': rouge_l,
            'rouge_sum': rouge_sum,
            'status': 'success'
        }

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model, tokenizer
        torch.cuda.empty_cache()
        gc.collect()

        return result

    except Exception as e:
        print(f"\nâŒ {nickname} í‰ê°€ ì‹¤íŒ¨: {e}")

        result = {
            'model_name': model_name,
            'nickname': nickname,
            'rouge-1': 0.0,
            'rouge-2': 0.0,
            'rouge-l': 0.0,
            'rouge_sum': 0.0,
            'status': f'failed: {str(e)}'
        }

        return result

    finally:
        # ìºì‹œ ì •ë¦¬
        if config['disk_management']['auto_cleanup_cache']:
            cleanup_hf_cache(model_name)

        # ë””ìŠ¤í¬ ì²´í¬
        if config['disk_management']['check_after_inference']:
            disk_usage_after = check_disk_usage()
            print(f"ğŸ’¾ ì •ë¦¬ í›„ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰: {disk_usage_after:.2f} GB\n")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ëª¨ë¸ ìŠ¤í¬ë¦¬ë‹ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument(
        "--config",
        type=str,
        required=True,
        help="ì„¤ì • YAML íŒŒì¼ ê²½ë¡œ"
    )
    args = parser.parse_args()

    # ì„¤ì • ë¡œë“œ
    config = load_config(args.config)
    print(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ: {args.config}")

    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    result_path = Path(config['general']['result_path'])
    result_path.mkdir(parents=True, exist_ok=True)

    # W&B ì´ˆê¸°í™”
    wandb_run = None
    if config['wandb']['enabled'] and WANDB_AVAILABLE:
        run_name = f"{config['wandb']['name_prefix']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        wandb_run = wandb.init(
            project=config['wandb']['project'],
            entity=config['wandb']['entity'],
            name=run_name,
            tags=config['wandb']['tags'],
            config=config
        )
        print("âœ… W&B ì´ˆê¸°í™” ì™„ë£Œ")

    # ëª¨ë¸ ìŠ¤í¬ë¦¬ë‹
    results = []
    models = config['models']

    print(f"\n{'='*80}")
    print(f"ğŸš€ ëª¨ë¸ ìŠ¤í¬ë¦¬ë‹ ì‹œì‘: {len(models)}ê°œ ëª¨ë¸")
    print(f"{'='*80}\n")

    for i, model_info in enumerate(models, 1):
        print(f"\n{'#'*80}")
        print(f"# [{i}/{len(models)}] {model_info['nickname']}")
        print(f"# Model: {model_info['model_name']}")
        print(f"# Description: {model_info['description']}")
        print(f"{'#'*80}\n")

        result = evaluate_model(
            model_name=model_info['model_name'],
            nickname=model_info['nickname'],
            config=config,
            wandb_run=wandb_run
        )

        results.append(result)

    # ê²°ê³¼ ì €ì¥
    results_df = pd.DataFrame(results)
    csv_path = result_path / f"screening_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    results_df.to_csv(csv_path, index=False)

    print(f"\n{'='*80}")
    print(f"ğŸ‰ ëª¨ë¸ ìŠ¤í¬ë¦¬ë‹ ì™„ë£Œ!")
    print(f"{'='*80}\n")
    print(results_df.to_string(index=False))
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {csv_path}")

    # W&Bì— í…Œì´ë¸” ë¡œê¹…
    if wandb_run:
        wandb_run.log({"screening_results": wandb.Table(dataframe=results_df)})
        wandb_run.finish()

    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì¶œë ¥
    best_model = results_df.loc[results_df['rouge_sum'].idxmax()]
    print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['nickname']}")
    print(f"   ROUGE Sum: {best_model['rouge_sum']:.2f}")
    print(f"   (R1: {best_model['rouge-1']:.2f}, R2: {best_model['rouge-2']:.2f}, RL: {best_model['rouge-l']:.2f})")


if __name__ == "__main__":
    main()
