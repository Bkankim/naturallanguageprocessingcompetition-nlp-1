"""
ëª¨ë¸ ìŠ¤í¬ë¦¬ë‹ ìŠ¤í¬ë¦½íŠ¸ (QLoRA 4bit + W&B)

ì—¬ëŸ¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ QLoRA 4bit ì–‘ìí™”ë¡œ ë¡œë“œí•˜ì—¬
Dev setì—ì„œ zero-shot ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- QLoRA 4bit ë¡œë”© (BitsAndBytes)
- ìˆœì°¨ ì‹¤í–‰ + ìë™ ìºì‹œ ì‚­ì œ
- ë””ìŠ¤í¬ 80GB ê´€ë¦¬
- W&B ì‹¤ì‹œê°„ ë¡œê¹…
- ROUGE ë©”íŠ¸ë¦­ í‰ê°€

ì‚¬ìš©ë²•:
    python scripts/model_screening.py --config configs/screening_config.yaml
"""

import os
import gc
import sys
import yaml
import shutil
import argparse
import subprocess
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional
from datetime import datetime

import torch
import pandas as pd
from tqdm import tqdm
from torch.utils.data import DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    AutoModelForCausalLM,
    BitsAndBytesConfig
)

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
sys.path.append(str(Path(__file__).parent.parent))

from src.data.preprocessor import Preprocess
from src.data.dataset import DatasetForInference
from src.evaluation.metrics import calculate_rouge_scores

# W&B import (optional)
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("âš ï¸  wandb not installed. Install with: pip install wandb")


def check_disk_usage() -> float:
    """
    í˜„ì¬ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ì„ ì²´í¬í•©ë‹ˆë‹¤ (GB ë‹¨ìœ„).

    Returns:
        float: í˜„ì¬ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ (GB)
    """
    # /Competition, /opt, /data, /root ë””ë ‰í† ë¦¬ í¬ê¸° í•©ì‚°
    total_gb = 0.0

    for directory in ['/Competition', '/opt', '/data', '/root']:
        try:
            result = subprocess.run(
                ['du', '-sb', directory],
                capture_output=True,
                text=True,
                timeout=30
            )
            if result.returncode == 0:
                size_bytes = int(result.stdout.split()[0])
                total_gb += size_bytes / (1024**3)
        except Exception as e:
            print(f"âš ï¸  {directory} í¬ê¸° ì¸¡ì • ì‹¤íŒ¨: {e}")

    return total_gb


def cleanup_hf_cache(model_name: Optional[str] = None):
    """
    HuggingFace ìºì‹œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.

    Args:
        model_name: íŠ¹ì • ëª¨ë¸ ìºì‹œë§Œ ì‚­ì œ (Noneì´ë©´ ì „ì²´ ì‚­ì œí•˜ì§€ ì•ŠìŒ)
    """
    cache_dir = Path.home() / ".cache" / "huggingface" / "hub"

    if not cache_dir.exists():
        return

    if model_name:
        # íŠ¹ì • ëª¨ë¸ ìºì‹œë§Œ ì‚­ì œ
        model_cache_pattern = model_name.replace("/", "--")
        for cache_path in cache_dir.glob(f"models--{model_cache_pattern}*"):
            try:
                shutil.rmtree(cache_path)
                print(f"âœ… ìºì‹œ ì‚­ì œ ì™„ë£Œ: {cache_path.name}")
            except Exception as e:
                print(f"âš ï¸  ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {e}")

    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

    gc.collect()


def load_config(config_path: str) -> Dict[str, Any]:
    """YAML ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    with open(config_path, "r", encoding="utf-8") as file:
        config = yaml.safe_load(file)
    return config


def setup_bnb_config(config: Dict[str, Any]) -> BitsAndBytesConfig:
    """
    BitsAndBytes 4bit ì–‘ìí™” ì„¤ì •ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬

    Returns:
        BitsAndBytesConfig: 4bit ì–‘ìí™” ì„¤ì •
    """
    qlora_config = config['qlora']

    return BitsAndBytesConfig(
        load_in_4bit=qlora_config['load_in_4bit'],
        bnb_4bit_compute_dtype=getattr(torch, qlora_config['bnb_4bit_compute_dtype']),
        bnb_4bit_quant_type=qlora_config['bnb_4bit_quant_type'],
        bnb_4bit_use_double_quant=qlora_config['bnb_4bit_use_double_quant'],
    )


def load_model_and_tokenizer(
    model_name: str,
    bnb_config: Optional[BitsAndBytesConfig],
    special_tokens: List[str],
    device: torch.device
) -> Tuple[Any, Any]:
    """
    ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        model_name: HuggingFace ëª¨ë¸ëª…
        bnb_config: BitsAndBytes ì„¤ì • (Noneì´ë©´ ì¼ë°˜ ë¡œë”©)
        special_tokens: ì¶”ê°€í•  special tokens
        device: ë””ë°”ì´ìŠ¤

    Returns:
        Tuple[model, tokenizer, model_type]
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
    print(f"{'='*80}")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )

    # Special tokens ì¶”ê°€
    if special_tokens:
        tokenizer.add_special_tokens({
            'additional_special_tokens': special_tokens
        })

    # ëª¨ë¸ ë¡œë”© ì„¤ì •
    load_kwargs = {
        "trust_remote_code": True
    }

    if bnb_config:
        load_kwargs["quantization_config"] = bnb_config
        load_kwargs["device_map"] = "auto"
        quant_info = "4bit ì–‘ìí™”"
    else:
        load_kwargs["torch_dtype"] = torch.float16
        quant_info = "FP16"

    # ëª¨ë¸ íƒ€ì… ê°ì§€ (Seq2Seq vs CausalLM)
    try:
        # Seq2Seq ëª¨ë¸ ì‹œë„ (BART, T5 ë“±)
        model = AutoModelForSeq2SeqLM.from_pretrained(
            model_name,
            **load_kwargs
        )
        model_type = "seq2seq"
    except:
        # CausalLM ëª¨ë¸ (GPT, Llama, Qwen ë“±)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            **load_kwargs
        )
        model_type = "causal"

    # GPUë¡œ ì´ë™ (4bitì´ ì•„ë‹Œ ê²½ìš°)
    if not bnb_config:
        model = model.to(device)

    # Tokenizer resize
    model.resize_token_embeddings(len(tokenizer))

    print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ (íƒ€ì…: {model_type}, {quant_info})")
    print(f"   íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters() / 1e9:.2f}B")

    return model, tokenizer, model_type


def prepare_dev_dataset(
    config: Dict[str, Any],
    tokenizer: Any
) -> Tuple[pd.DataFrame, DataLoader]:
    """
    Dev ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

    Returns:
        Tuple[dev_data, dataloader]
    """
    # ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    preprocessor = Preprocess(
        bos_token=config['tokenizer']['bos_token'],
        eos_token=config['tokenizer']['eos_token']
    )

    # Dev ë°ì´í„° ë¡œë“œ (validationì´ë¯€ë¡œ is_train=Trueë¡œ summary í¬í•¨)
    dev_file_path = os.path.join(config['general']['data_path'], 'dev.csv')
    dev_data = preprocessor.make_set_as_df(dev_file_path, is_train=True)

    # ì¸ì½”ë” ì…ë ¥ ìƒì„±
    encoder_input_dev, _ = preprocessor.make_input(dev_data, is_test=True)

    # í† í¬ë‚˜ì´ì§•
    tokenized_encoder_inputs = tokenizer(
        encoder_input_dev,
        return_tensors="pt",
        padding=True,
        add_special_tokens=True,
        truncation=True,
        max_length=config['tokenizer']['encoder_max_len'],
        return_token_type_ids=False,
    )

    # ì¶”ë¡ ìš© ë°ì´í„°ì…‹ ìƒì„±
    dev_dataset = DatasetForInference(
        tokenized_encoder_inputs,
        dev_data['fname'].tolist(),
        len(encoder_input_dev)
    )

    # ë°ì´í„°ë¡œë” ìƒì„±
    dataloader = DataLoader(
        dev_dataset,
        batch_size=config['inference']['batch_size'],
        shuffle=False
    )

    print(f"âœ… Dev ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: {len(dev_data)} samples")

    return dev_data, dataloader


def generate_summaries(
    model: Any,
    model_type: str,
    tokenizer: Any,
    dataloader: DataLoader,
    config: Dict[str, Any],
    device: torch.device
) -> List[str]:
    """
    ëª¨ë¸ë¡œ ìš”ì•½ë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Returns:
        List[str]: ìƒì„±ëœ ìš”ì•½ë¬¸ ë¦¬ìŠ¤íŠ¸
    """
    model.eval()
    summaries = []

    print("\nğŸ”® ìš”ì•½ ìƒì„± ì¤‘...")

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="ìƒì„±"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)

            if model_type == "seq2seq":
                # Seq2Seq ëª¨ë¸ (BART, T5)
                generated_ids = model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                    early_stopping=config['inference']['early_stopping'],
                    max_length=config['inference']['generate_max_length'],
                    num_beams=config['inference']['num_beams'],
                )
            else:
                # CausalLM ëª¨ë¸ (Llama, Qwen, SOLAR)
                # Prompt êµ¬ì„± í•„ìš”
                generated_ids = model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_new_tokens=config['inference']['generate_max_length'],
                    num_beams=config['inference']['num_beams'],
                    no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                    early_stopping=config['inference']['early_stopping'],
                )
                # Input ë¶€ë¶„ ì œê±° (CausalLMì€ inputì„ í¬í•¨í•˜ì—¬ ìƒì„±)
                generated_ids = generated_ids[:, input_ids.shape[1]:]

            # ë””ì½”ë”©
            for ids in generated_ids:
                result = tokenizer.decode(ids, skip_special_tokens=False)
                summaries.append(result)

    print(f"âœ… ìš”ì•½ ìƒì„± ì™„ë£Œ: {len(summaries)}ê°œ")

    return summaries


def evaluate_model(
    model_name: str,
    nickname: str,
    config: Dict[str, Any],
    wandb_run: Optional[Any] = None
) -> Dict[str, Any]:
    """
    ë‹¨ì¼ ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤.

    Returns:
        Dict: í‰ê°€ ê²°ê³¼ {model_name, rouge-1, rouge-2, rouge-l, rouge_sum}
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # ë””ìŠ¤í¬ ì²´í¬
    if config['disk_management']['check_before_download']:
        disk_usage_before = check_disk_usage()
        print(f"\nğŸ’¾ í˜„ì¬ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰: {disk_usage_before:.2f} GB")

        if disk_usage_before > config['general']['max_disk_usage_gb']:
            raise RuntimeError(
                f"ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ì´ˆê³¼! "
                f"{disk_usage_before:.2f}GB > {config['general']['max_disk_usage_gb']}GB"
            )

    try:
        # BitsAndBytes ì„¤ì • (4bitì´ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ)
        bnb_config = None
        if config['qlora']['load_in_4bit']:
            bnb_config = setup_bnb_config(config)

        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        model, tokenizer, model_type = load_model_and_tokenizer(
            model_name=model_name,
            bnb_config=bnb_config,
            special_tokens=config['tokenizer']['special_tokens'],
            device=device
        )

        # Dev ë°ì´í„°ì…‹ ì¤€ë¹„
        dev_data, dataloader = prepare_dev_dataset(config, tokenizer)

        # ìš”ì•½ ìƒì„±
        summaries = generate_summaries(
            model=model,
            model_type=model_type,
            tokenizer=tokenizer,
            dataloader=dataloader,
            config=config,
            device=device
        )

        # í›„ì²˜ë¦¬: ë¶ˆí•„ìš”í•œ í† í° ì œê±°
        from src.evaluation.metrics import clean_text
        remove_tokens = config['inference']['remove_tokens']
        cleaned_summaries = clean_text(summaries, remove_tokens)

        # ROUGE í‰ê°€
        print("\nğŸ“Š ROUGE í‰ê°€ ì¤‘...")
        references = dev_data['summary'].tolist()

        rouge_scores = calculate_rouge_scores(
            predictions=cleaned_summaries,
            references=references,
            remove_tokens=remove_tokens
        )

        # ì ìˆ˜ ì¶”ì¶œ
        rouge_1 = rouge_scores['rouge-1']['f'] * 100
        rouge_2 = rouge_scores['rouge-2']['f'] * 100
        rouge_l = rouge_scores['rouge-l']['f'] * 100
        rouge_sum = rouge_1 + rouge_2 + rouge_l

        print(f"\n{'='*80}")
        print(f"âœ… {nickname} í‰ê°€ ì™„ë£Œ")
        print(f"{'='*80}")
        print(f"ROUGE-1: {rouge_1:.2f}")
        print(f"ROUGE-2: {rouge_2:.2f}")
        print(f"ROUGE-L: {rouge_l:.2f}")
        print(f"ROUGE Sum: {rouge_sum:.2f}")
        print(f"{'='*80}\n")

        # W&B ë¡œê¹…
        if wandb_run:
            wandb_run.log({
                f"{nickname}/rouge-1": rouge_1,
                f"{nickname}/rouge-2": rouge_2,
                f"{nickname}/rouge-l": rouge_l,
                f"{nickname}/rouge_sum": rouge_sum,
            })

        result = {
            'model_name': model_name,
            'nickname': nickname,
            'rouge-1': rouge_1,
            'rouge-2': rouge_2,
            'rouge-l': rouge_l,
            'rouge_sum': rouge_sum,
            'status': 'success'
        }

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model, tokenizer
        torch.cuda.empty_cache()
        gc.collect()

        return result

    except Exception as e:
        print(f"\nâŒ {nickname} í‰ê°€ ì‹¤íŒ¨: {e}")

        result = {
            'model_name': model_name,
            'nickname': nickname,
            'rouge-1': 0.0,
            'rouge-2': 0.0,
            'rouge-l': 0.0,
            'rouge_sum': 0.0,
            'status': f'failed: {str(e)}'
        }

        return result

    finally:
        # ìºì‹œ ì •ë¦¬
        if config['disk_management']['auto_cleanup_cache']:
            cleanup_hf_cache(model_name)

        # ë””ìŠ¤í¬ ì²´í¬
        if config['disk_management']['check_after_inference']:
            disk_usage_after = check_disk_usage()
            print(f"ğŸ’¾ ì •ë¦¬ í›„ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰: {disk_usage_after:.2f} GB\n")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ëª¨ë¸ ìŠ¤í¬ë¦¬ë‹ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument(
        "--config",
        type=str,
        required=True,
        help="ì„¤ì • YAML íŒŒì¼ ê²½ë¡œ"
    )
    args = parser.parse_args()

    # ì„¤ì • ë¡œë“œ
    config = load_config(args.config)
    print(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ: {args.config}")

    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    result_path = Path(config['general']['result_path'])
    result_path.mkdir(parents=True, exist_ok=True)

    # W&B ì´ˆê¸°í™”
    wandb_run = None
    if config['wandb']['enabled'] and WANDB_AVAILABLE:
        run_name = f"{config['wandb']['name_prefix']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        wandb_run = wandb.init(
            project=config['wandb']['project'],
            entity=config['wandb']['entity'],
            name=run_name,
            tags=config['wandb']['tags'],
            config=config
        )
        print("âœ… W&B ì´ˆê¸°í™” ì™„ë£Œ")

    # ëª¨ë¸ ìŠ¤í¬ë¦¬ë‹
    results = []
    models = config['models']

    print(f"\n{'='*80}")
    print(f"ğŸš€ ëª¨ë¸ ìŠ¤í¬ë¦¬ë‹ ì‹œì‘: {len(models)}ê°œ ëª¨ë¸")
    print(f"{'='*80}\n")

    for i, model_info in enumerate(models, 1):
        print(f"\n{'#'*80}")
        print(f"# [{i}/{len(models)}] {model_info['nickname']}")
        print(f"# Model: {model_info['model_name']}")
        print(f"# Description: {model_info['description']}")
        print(f"{'#'*80}\n")

        result = evaluate_model(
            model_name=model_info['model_name'],
            nickname=model_info['nickname'],
            config=config,
            wandb_run=wandb_run
        )

        results.append(result)

    # ê²°ê³¼ ì €ì¥
    results_df = pd.DataFrame(results)
    csv_path = result_path / f"screening_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    results_df.to_csv(csv_path, index=False)

    print(f"\n{'='*80}")
    print(f"ğŸ‰ ëª¨ë¸ ìŠ¤í¬ë¦¬ë‹ ì™„ë£Œ!")
    print(f"{'='*80}\n")
    print(results_df.to_string(index=False))
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {csv_path}")

    # W&Bì— í…Œì´ë¸” ë¡œê¹…
    if wandb_run:
        wandb_run.log({"screening_results": wandb.Table(dataframe=results_df)})
        wandb_run.finish()

    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì¶œë ¥
    best_model = results_df.loc[results_df['rouge_sum'].idxmax()]
    print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['nickname']}")
    print(f"   ROUGE Sum: {best_model['rouge_sum']:.2f}")
    print(f"   (R1: {best_model['rouge-1']:.2f}, R2: {best_model['rouge-2']:.2f}, RL: {best_model['rouge-l']:.2f})")


if __name__ == "__main__":
    main()
