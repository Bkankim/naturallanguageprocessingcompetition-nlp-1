"""
LLM íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸ (QLoRA + LoRA + W&B)

6ê°œ ëª¨ë¸ (koBART, koT5, Llama-3.2-3B, Qwen3-4B, Qwen2.5-7B, Llama-3-8B)ì„
ëŒ€í™” ìš”ì•½ íƒœìŠ¤í¬ë¡œ íŒŒì¸íŠœë‹í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- Encoder-Decoder (koBART, koT5) ì „ì²´ íŒŒì¸íŠœë‹
- Decoder-only LLM (Llama, Qwen) QLoRA 4bit + LoRA íŒŒì¸íŠœë‹
- ëª¨ë¸ë³„ ìµœì í™” ì„¤ì • (LR, dropout, float type)
- W&B ì‹¤ì‹œê°„ ë¡œê¹…
- Mecab ROUGE í‰ê°€
- ë””ìŠ¤í¬ ê´€ë¦¬ (100GB)

ì‚¬ìš©ë²•:
    python scripts/llm_finetuning.py --config configs/finetune_config.yaml
"""

import os
import gc
import sys
import yaml
import shutil
import random
import argparse
import subprocess
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional
from datetime import datetime

import torch
import pandas as pd
from tqdm import tqdm
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
    TrainerCallback
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType
)
from datasets import Dataset

# TRL import (SFTTrainer)
try:
    from trl import SFTTrainer, SFTConfig
    TRL_AVAILABLE = True
    print("âœ… TRL ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    TRL_AVAILABLE = False
    print("âš ï¸  TRL not installed - SFTTrainer ì‚¬ìš© ë¶ˆê°€")

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
sys.path.append(str(Path(__file__).parent.parent))

from src.evaluation.metrics import calculate_rouge_scores
from src.utils.wandb_logger import WandBLogger

# RTX 3090 ìµœì í™”: TF32 í™œì„±í™”
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
print("âœ… TF32 í™œì„±í™” (RTX 3090 ìµœì í™”)")

# W&B import
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("âš ï¸  wandb not installed")


def set_seed(seed: int = 42):
    """ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    print(f"âœ… Seed ì„¤ì •: {seed}")


def check_disk_usage(critical_limit_gb: float = 150.0) -> float:
    """
    ì „ì²´ ë£¨íŠ¸ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ì²´í¬ (GB)

    âš ï¸  CRITICAL: 150GB ì´ˆê³¼ ì‹œ ì„œë²„ ì´ˆê¸°í™”ë¨!

    Args:
        critical_limit_gb: ì„ê³„ê°’ (ê¸°ë³¸ 150GB)

    Returns:
        í˜„ì¬ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ (GB)

    Raises:
        RuntimeError: 150GB ì´ˆê³¼ ì‹œ
    """
    try:
        # du -sh / 2>/dev/null ëª…ë ¹ ì‹¤í–‰
        result = subprocess.run(
            ['du', '-sh', '/'],
            stdout=subprocess.PIPE,
            stderr=subprocess.DEVNULL,  # 2>/dev/null
            text=True,
            timeout=60
        )

        if result.returncode == 0:
            # "111G\t/" í˜•ì‹ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
            size_str = result.stdout.split()[0]
            # G, M, K ë“±ì˜ ë‹¨ìœ„ íŒŒì‹±
            if size_str.endswith('G'):
                total_gb = float(size_str[:-1])
            elif size_str.endswith('M'):
                total_gb = float(size_str[:-1]) / 1024
            elif size_str.endswith('K'):
                total_gb = float(size_str[:-1]) / (1024**2)
            elif size_str.endswith('T'):
                total_gb = float(size_str[:-1]) * 1024
            else:
                # ë‹¨ìœ„ ì—†ìœ¼ë©´ bytesë¡œ ê°„ì£¼
                total_gb = float(size_str) / (1024**3)

            # âš ï¸ CRITICAL: 150GB ì²´í¬
            if total_gb >= critical_limit_gb:
                raise RuntimeError(
                    f"ğŸš¨ CRITICAL: ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ {total_gb:.1f}GB >= {critical_limit_gb}GB! "
                    f"ì„œë²„ ì´ˆê¸°í™” ìœ„í—˜! ì¦‰ì‹œ ì¤‘ë‹¨í•©ë‹ˆë‹¤."
                )

            # ê²½ê³ : 140GB ì´ìƒ
            if total_gb >= 140.0:
                print(f"âš ï¸  WARNING: ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ {total_gb:.1f}GB (í•œê³„ê¹Œì§€ {critical_limit_gb - total_gb:.1f}GB)")

            return total_gb
        else:
            print(f"âš ï¸  ë””ìŠ¤í¬ ì²´í¬ ì‹¤íŒ¨ (returncode={result.returncode})")
            print(f"âš ï¸  ì‹¤ì œ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸ ë¶ˆê°€ - ìˆ˜ë™ í™•ì¸ ê¶Œì¥: du -sh /")
            return 0.0

    except subprocess.TimeoutExpired:
        print(f"âš ï¸  ë””ìŠ¤í¬ ì²´í¬ ì‹œê°„ ì´ˆê³¼ (60ì´ˆ)")
        print(f"âš ï¸  ì‹¤ì œ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸ ë¶ˆê°€ - ìˆ˜ë™ í™•ì¸ ê¶Œì¥: du -sh /")
        return 0.0
    except Exception as e:
        print(f"âš ï¸  ë””ìŠ¤í¬ ì²´í¬ ì˜¤ë¥˜: {e}")
        print(f"âš ï¸  ì‹¤ì œ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸ ë¶ˆê°€ - ìˆ˜ë™ í™•ì¸ ê¶Œì¥: du -sh /")
        return 0.0


def cleanup_hf_cache(model_name: Optional[str] = None):
    """HuggingFace ìºì‹œ ì •ë¦¬"""
    cache_dir = Path.home() / ".cache" / "huggingface" / "hub"
    if not cache_dir.exists():
        return

    if model_name:
        # ëª¨ë¸ëª…ì„ ìºì‹œ ë””ë ‰í† ë¦¬ëª…ìœ¼ë¡œ ë³€í™˜
        cache_model_name = model_name.replace("/", "--")
        cache_model_dir = cache_dir / f"models--{cache_model_name}"

        if cache_model_dir.exists():
            shutil.rmtree(cache_model_dir)
            print(f"âœ… ìºì‹œ ì‚­ì œ ì™„ë£Œ: {cache_model_name}")


def load_data(data_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¡œë“œ"""
    train_df = pd.read_csv(Path(data_path) / "train.csv")
    dev_df = pd.read_csv(Path(data_path) / "dev.csv")
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: Train {len(train_df)}, Dev {len(dev_df)}")
    return train_df, dev_df


def apply_chat_template(
    dialogue: str,
    summary: str,
    template_type: str,
    system_prompt: str,
    tokenizer: Any,
    for_training: bool = True
) -> Dict[str, str]:
    """
    Chat template ì ìš© (HuggingFace ê³µì‹ API ì‚¬ìš©)

    Args:
        for_training: Trueë©´ add_generation_prompt=False (í›ˆë ¨ìš©),
                     Falseë©´ add_generation_prompt=True (ì¶”ë¡ ìš©)

    Returns:
        dict: {"input": full_text} (í›ˆë ¨) ë˜ëŠ” {"input": prompt} (ì¶”ë¡ )
    """
    if template_type not in ("llama", "qwen"):
        # Encoder-DecoderëŠ” chat template ë¶ˆí•„ìš”
        return {"input": dialogue, "target": summary}

    if for_training:
        # í›ˆë ¨: add_generation_prompt=False (ì •ë‹µ í¬í•¨)
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ì„¸ìš”:\n---\n{dialogue}\n---"},
            {"role": "assistant", "content": summary}  # ì •ë‹µ í¬í•¨
        ]
        full_text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False
        )
        # full_textëŠ” ì´ë¯¸ system + user + assistant(summary) ì „ì²´ë¥¼ í¬í•¨
        return {"input": full_text, "target": ""}  # targetì€ ë¹ˆ ë¬¸ìì—´
    else:
        # ì¶”ë¡ : add_generation_prompt=True (assistant í„´ ì‹œì‘ë§Œ)
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ì„¸ìš”:\n---\n{dialogue}\n---"}
            # assistant ì œì™¸
        ]
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        return {"input": prompt, "target": ""}


def prepare_dataset(
    df: pd.DataFrame,
    tokenizer: Any,
    model_config: Dict[str, Any],
    config: Dict[str, Any],
    max_input_length: int = 512,
    max_target_length: int = 100
) -> Dataset:
    """ë°ì´í„°ì…‹ ì¤€ë¹„ (HuggingFace Dataset í˜•ì‹)"""

    template_type = model_config.get("chat_template_type", None)
    system_prompt = config["data"]["system_prompt"]

    data_dicts = []
    for _, row in df.iterrows():
        dialogue = row["dialogue"]
        summary = row["summary"]

        # Chat template ì ìš© (í›ˆë ¨ìš©)
        templated = apply_chat_template(
            dialogue, summary, template_type, system_prompt, tokenizer,
            for_training=True
        )

        if template_type:  # Decoder-only (Llama, Qwen)
            # âœ… HuggingFace ê³µì‹ API ì‚¬ìš© (add_generation_prompt=False)
            # full_textëŠ” ì´ë¯¸ system + user + assistant(summary) ì „ì²´ë¥¼ í¬í•¨
            full_text = templated["input"]

            # ì „ì²´ í† í¬ë‚˜ì´ì¦ˆ
            full_ids = tokenizer(
                full_text,
                max_length=max_input_length + max_target_length,
                truncation=True,
                padding=False,
                add_special_tokens=True
            )["input_ids"]

            # Assistant í—¤ë” ìœ„ì¹˜ ì°¾ê¸° (ëª¨ë¸ë³„ í—¤ë” ë‹¤ë¦„)
            # Llama: "<|start_header_id|>assistant<|end_header_id|>\n\n"
            # Qwen: "<|im_start|>assistant\n"
            if template_type == "llama":
                assistant_header = "<|start_header_id|>assistant<|end_header_id|>\n\n"
            elif template_type == "qwen":
                assistant_header = "<|im_start|>assistant\n"
            else:
                assistant_header = ""

            # Assistant í—¤ë” í† í° ID ë¦¬ìŠ¤íŠ¸ ìƒì„±
            assistant_header_ids = tokenizer.encode(
                assistant_header,
                add_special_tokens=False
            )

            # full_idsì—ì„œ assistant í—¤ë” ìœ„ì¹˜ ê²€ìƒ‰
            prompt_length = 0
            for i in range(len(full_ids) - len(assistant_header_ids) + 1):
                if full_ids[i:i+len(assistant_header_ids)] == assistant_header_ids:
                    # Assistant í—¤ë”ë¥¼ ì°¾ìŒ â†’ í—¤ë” ëë¶€í„° í•™ìŠµ ëŒ€ìƒ
                    prompt_length = i + len(assistant_header_ids)
                    break

            # Labels ê³„ì‚°
            if prompt_length == 0 or prompt_length >= len(full_ids):
                # Assistant í—¤ë”ë¥¼ ëª» ì°¾ì•˜ê±°ë‚˜, truncationìœ¼ë¡œ ì˜ë¦° ê²½ìš°
                # â†’ ì „ì²´ë¥¼ -100ìœ¼ë¡œ ì²˜ë¦¬ (í•™ìŠµ ì œì™¸)
                input_ids = full_ids
                labels = [-100] * len(full_ids)
            else:
                # ì •ìƒì ìœ¼ë¡œ ì°¾ì€ ê²½ìš°
                # â†’ Prompt ë¶€ë¶„ì€ -100, Response ë¶€ë¶„ë§Œ í•™ìŠµ
                input_ids = full_ids
                labels = [-100] * prompt_length + full_ids[prompt_length:]

            data_dicts.append({
                "input_ids": input_ids,
                "labels": labels
            })
        else:  # Encoder-Decoder (koBART, koT5)
            inputs = tokenizer(
                templated["input"],
                max_length=max_input_length,
                truncation=True,
                padding=False
            )

            labels = tokenizer(
                templated["target"],
                max_length=max_target_length,
                truncation=True,
                padding=False
            )

            data_dicts.append({
                "input_ids": inputs["input_ids"],
                "attention_mask": inputs["attention_mask"],
                "labels": labels["input_ids"]
            })

    return Dataset.from_list(data_dicts)


def load_model_and_tokenizer(
    model_config: Dict[str, Any],
    config: Dict[str, Any]
) -> Tuple[Any, Any]:
    """ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (QLoRA + PEFT)"""

    model_name = model_config["model_name"]
    model_type = model_config["model_type"]
    use_qlora = model_config.get("use_qlora", False)

    print(f"\n{'='*80}")
    print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
    print(f"   íƒ€ì…: {model_type}, QLoRA: {use_qlora}")
    print(f"{'='*80}\n")

    # Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Causal LMì€ left-padding í•„ìˆ˜ (generation ì‹œ ì˜¬ë°”ë¥¸ ê²°ê³¼ ë³´ì¥)
    if model_type == "causal_lm":
        tokenizer.padding_side = "left"
        # EOS/PAD ë™ê¸°í™”
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id
    else:
        # Encoder-DecoderëŠ” right-padding
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

    # Special tokens ì¶”ê°€
    special_tokens = config["tokenizer"]["special_tokens"]

    # Chat template í† í° ì¶”ê°€ (ëª¨ë¸ë³„)
    chat_template_type = model_config.get("chat_template_type")
    if chat_template_type and "chat_template_tokens" in config["tokenizer"]:
        chat_tokens = config["tokenizer"]["chat_template_tokens"].get(chat_template_type, [])
        special_tokens = special_tokens + chat_tokens

    tokenizer.add_special_tokens({"additional_special_tokens": special_tokens})

    # Model loading
    if use_qlora:
        # QLoRA 4bit config - compute dtypeì€ ëª¨ë¸ dtypeê³¼ ì¼ì¹˜ì‹œí‚´
        qlora_config = config["qlora"]
        # ëª¨ë¸ë³„ dtypeì— ë§ì¶° compute dtype ì„¤ì • (Llama=bf16, Qwen=fp16)
        compute_dtype = torch.bfloat16 if model_config.get("use_bf16", False) else torch.float16

        bnb_config = BitsAndBytesConfig(
            load_in_4bit=qlora_config["load_in_4bit"],
            bnb_4bit_compute_dtype=compute_dtype,
            bnb_4bit_quant_type=qlora_config["bnb_4bit_quant_type"],
            bnb_4bit_use_double_quant=qlora_config["bnb_4bit_use_double_quant"]
        )

        # Load model
        if model_type == "causal_lm":
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=bnb_config,
                device_map="auto",
                torch_dtype=torch.bfloat16 if model_config.get("use_bf16", False) else torch.float16,
                trust_remote_code=True
            )
        else:
            raise ValueError(f"QLoRAëŠ” causal_lmë§Œ ì§€ì›: {model_type}")

        # Resize token embeddings BEFORE prepare_model_for_kbit_training
        # (special tokens ì¶”ê°€í–ˆìœ¼ë¯€ë¡œ í•„ìˆ˜!)
        model.resize_token_embeddings(len(tokenizer))

        # âœ… gradient_checkpointing ì‚¬ìš© ì‹œ use_cacheëŠ” Falseì—¬ì•¼ í•¨
        # (í•™ìŠµ ì‹œ KV cache ë¶ˆí•„ìš” + gradient_checkpointingê³¼ ì¶©ëŒ)
        model.config.use_cache = False

        # Prepare for k-bit training
        model = prepare_model_for_kbit_training(model)

        # LoRA config
        lora_config_dict = config["lora"]
        lora_config = LoraConfig(
            r=lora_config_dict["r"],
            lora_alpha=lora_config_dict["lora_alpha"],
            lora_dropout=model_config.get("lora_dropout", 0.1),
            target_modules=lora_config_dict["target_modules"],
            bias=lora_config_dict["bias"],
            task_type=TaskType.CAUSAL_LM
        )

        # Apply LoRA
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()

    else:
        # Full fine-tuning (Encoder-Decoder)
        if model_type == "encoder_decoder":
            # FP32ë¡œ ë¡œë“œí•˜ê³  Trainerê°€ mixed precision ê´€ë¦¬í•˜ë„ë¡ í•¨
            model = AutoModelForSeq2SeqLM.from_pretrained(
                model_name
            )
        else:
            raise ValueError(f"Full tuningì€ encoder_decoderë§Œ ì§€ì›: {model_type}")

        # Resize token embeddings
        model.resize_token_embeddings(len(tokenizer))
        model = model.cuda()

    print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    return model, tokenizer


def compute_metrics(eval_preds, tokenizer):
    """ROUGE ë©”íŠ¸ë¦­ ê³„ì‚° (Mecab)"""
    predictions, labels = eval_preds

    # Decode
    if isinstance(predictions, tuple):
        predictions = predictions[0]

    # Replace -100 with pad_token_id
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # ROUGE
    rouge_scores = calculate_rouge_scores(decoded_labels, decoded_preds)

    return {
        "rouge-1": rouge_scores["rouge-1"]["f"],
        "rouge-2": rouge_scores["rouge-2"]["f"],
        "rouge-l": rouge_scores["rouge-l"]["f"],
        "rouge_sum": (
            rouge_scores["rouge-1"]["f"] +
            rouge_scores["rouge-2"]["f"] +
            rouge_scores["rouge-l"]["f"]
        )
    }


def clean_predictions(predictions: List[str], tokenizer: Any) -> List[str]:
    """ìƒì„± ê²°ê³¼ì—ì„œ ë¶ˆí•„ìš”í•œ ëª¨ë¸ í† í° ì œê±° (Baseline ë°©ì‹)"""
    remove_tokens = [
        '<usr>',
        tokenizer.bos_token,
        tokenizer.eos_token,
        tokenizer.pad_token,
        # Chat template í† í° (Llama)
        '<|start_header_id|>',
        '<|end_header_id|>',
        '<|eot_id|>',
        # Chat template í† í° (Qwen)
        '<|im_start|>',
        '<|im_end|>',
        # ì—­í•  í† í°
        'system',
        'user',
        'assistant',
    ]

    cleaned = predictions.copy()
    for token in remove_tokens:
        if token:
            cleaned = [s.replace(token, " ") for s in cleaned]

    # ì—°ì†ëœ ê³µë°± ì œê±°
    cleaned = [" ".join(s.split()) for s in cleaned]
    return cleaned


def run_inference_on_dev(
    model: Any,
    tokenizer: Any,
    dev_df: pd.DataFrame,
    device: str,
    model_config: Dict[str, Any],
    config: Dict[str, Any],
    batch_size: int = 4,
    max_new_tokens: int = 100,
    num_beams: int = 4,
    is_test: bool = False
) -> Tuple[List[str], List[str]]:
    """
    Dev/Test set ì¶”ë¡  ì‹¤í–‰ (Chat Template ì ìš©)

    Args:
        is_test: Trueë©´ Test mode (references ì—†ìŒ), Falseë©´ Dev mode

    Returns:
        (predictions, references) íŠœí”Œ (Test modeì—ì„œëŠ” referencesëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸)
    """
    model.eval()

    dialogues = dev_df['dialogue'].tolist()
    references = dev_df['summary'].tolist() if 'summary' in dev_df.columns else []
    predictions = []

    # Chat template íƒ€ì… ê°€ì ¸ì˜¤ê¸°
    template_type = model_config.get("chat_template_type", None)
    system_prompt = config["data"]["system_prompt"]

    dataset_name = "Test" if is_test else "Dev"
    print(f"\nğŸ”„ {dataset_name} set ì¶”ë¡  ì‹œì‘ (samples={len(dialogues)}, batch_size={batch_size})")
    if template_type:
        print(f"   âœ… Chat template: {template_type}")
        # Causal LMì€ left-padding + left-truncation í•„ìˆ˜
        # - left-padding: ë°°ì¹˜ ìƒì„± ì‹œ ì˜¬ë°”ë¥¸ ê²°ê³¼ ë³´ì¥
        # - left-truncation: assistant í—¤ë” ë³´ì¡´ (ê¸´ ëŒ€í™” ì‹œ ì¤‘ìš”!)
        tokenizer.padding_side = "left"
        tokenizer.truncation_side = "left"
        print(f"   âœ… Padding/Truncation side: left (assistant í—¤ë” ë³´ì¡´)")

    for i in tqdm(range(0, len(dialogues), batch_size), desc="Inference"):
        batch_dialogues = dialogues[i:i+batch_size]

        # Chat template ì ìš© (Causal LMë§Œ)
        if template_type:
            batch_prompts = []
            for dialogue in batch_dialogues:
                # ì¶”ë¡ ìš©: add_generation_prompt=True
                templated = apply_chat_template(
                    dialogue,
                    "",  # summaryëŠ” ë¹ˆ ë¬¸ìì—´ (ìƒì„±í•  ê²ƒì´ë¯€ë¡œ)
                    template_type,
                    system_prompt,
                    tokenizer,
                    for_training=False  # ì¶”ë¡  ëª¨ë“œ
                )
                batch_prompts.append(templated["input"])
        else:
            # Encoder-DecoderëŠ” raw dialogue
            batch_prompts = batch_dialogues

        # Tokenize
        inputs = tokenizer(
            batch_prompts,
            max_length=1024,  # 512 â†’ 1024 (prompt truncation 6.81% â†’ 0% í•´ê²°)
            truncation=True,
            padding=True,
            return_tensors="pt"
        )

        # Remove token_type_ids if present (BART doesn't use it)
        inputs = {k: v.to(device) for k, v in inputs.items() if k != 'token_type_ids'}

        # Generate
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                num_beams=num_beams,
                early_stopping=True,
                no_repeat_ngram_size=3,
                length_penalty=0.9,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        # Decode - Decoder-onlyëŠ” input ì œê±°, Encoder-DecoderëŠ” ê·¸ëŒ€ë¡œ
        if template_type:
            # Decoder-only: outputsì— input_ids í¬í•¨ë¨ â†’ ìƒì„± ë¶€ë¶„ë§Œ ì¶”ì¶œ
            # outputs shape: [batch_size, total_length]
            # inputs['input_ids'] shape: [batch_size, prompt_length]
            input_length = inputs['input_ids'].shape[1]
            generated_ids = outputs[:, input_length:]  # ìƒì„±ëœ ë¶€ë¶„ë§Œ
            batch_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)
        else:
            # Encoder-Decoder: outputsëŠ” ìƒì„±ëœ ë¶€ë¶„ë§Œ í¬í•¨
            batch_preds = tokenizer.batch_decode(outputs, skip_special_tokens=False)

        predictions.extend(batch_preds)

    # Clean predictions (remove only model tokens, keep data tokens)
    predictions = clean_predictions(predictions, tokenizer)

    print(f"âœ… ì¶”ë¡  ì™„ë£Œ: {len(predictions)}ê°œ ìƒì„±")
    return predictions, references


class DataCollatorForSupervisedDataset(object):
    """
    Korean_DCS_2024 ë² ì´ìŠ¤ë¼ì¸ DataCollator

    SFTTrainerìš© Custom DataCollator
    - input_idsì™€ labelsë§Œ ì‚¬ìš© (ê°„ê²°í•¨)
    - labelsëŠ” -100ìœ¼ë¡œ íŒ¨ë”© (loss ê³„ì‚° ì œì™¸)
    - attention_mask ìë™ ìƒì„±
    """

    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def __call__(self, instances):
        input_ids, labels = tuple([instance[key] for instance in instances] for key in ("input_ids", "labels"))
        input_ids = torch.nn.utils.rnn.pad_sequence(
            [torch.tensor(ids) for ids in input_ids],
            batch_first=True,
            padding_value=self.tokenizer.pad_token_id
        )
        labels = torch.nn.utils.rnn.pad_sequence(
            [torch.tensor(lbls) for lbls in labels],
            batch_first=True,
            padding_value=-100
        )
        return dict(
            input_ids=input_ids,
            labels=labels,
            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),
        )


class GradientClippingCallback(TrainerCallback):
    """
    Gradient Clipping ë¹„ìœ¨ì„ ì¶”ì í•˜ëŠ” Callback

    ë§¤ ë¡œê¹… ìŠ¤í…ë§ˆë‹¤:
    - Clipping ë°œìƒ ì—¬ë¶€ ì¶”ì 
    - Clipping ë¹„ìœ¨ ê³„ì‚° (clipped steps / total steps)
    - W&Bì— ì‹¤ì‹œê°„ ë¡œê¹…
    """

    def __init__(self, max_grad_norm: float):
        self.max_grad_norm = max_grad_norm
        self.total_steps = 0
        self.clipped_steps = 0
        self.grad_norms = []

    def on_log(self, args, state, control, logs=None, **kwargs):
        """ë¡œê¹… ì‹œì ì— grad_norm ìº¡ì²˜"""
        if logs and "grad_norm" in logs:
            grad_norm = logs["grad_norm"]
            self.grad_norms.append(grad_norm)
            self.total_steps += 1

            # Clipping ë°œìƒ ì—¬ë¶€
            if grad_norm > self.max_grad_norm:
                self.clipped_steps += 1

            # Clipping ë¹„ìœ¨ ê³„ì‚°
            clip_ratio = self.clipped_steps / self.total_steps if self.total_steps > 0 else 0.0

            # ì¶”ê°€ ë©”íŠ¸ë¦­ ë¡œê¹…
            logs["grad_clip_ratio"] = clip_ratio
            logs["grad_clip_count"] = self.clipped_steps
            logs["grad_norm_mean"] = sum(self.grad_norms) / len(self.grad_norms) if self.grad_norms else 0.0


def train_model(
    model: Any,
    tokenizer: Any,
    train_dataset: Dataset,
    eval_dataset: Dataset,
    model_config: Dict[str, Any],
    config: Dict[str, Any],
    wandb_logger: WandBLogger
) -> Any:
    """
    ëª¨ë¸ í•™ìŠµ (ëª¨ë¸ íƒ€ì…ë³„ Trainer ìë™ ì„ íƒ)

    - Encoder-Decoder: Seq2SeqTrainer (generation ì§€ì›)
    - Causal LM: Trainer (language modeling)
    """

    nickname = model_config["nickname"]
    model_type = model_config["model_type"]
    output_dir = Path(config["general"]["output_base_dir"]) / nickname
    output_dir.mkdir(parents=True, exist_ok=True)

    # Training arguments
    training_config = config["training"]

    # ëª¨ë¸ë³„ ì„¤ì • override
    lr = model_config.get("learning_rate", training_config["learning_rate"])
    batch_size = model_config.get("batch_size", training_config["per_device_train_batch_size"])
    use_bf16 = model_config.get("use_bf16", training_config.get("bf16", False))
    use_fp16 = model_config.get("use_fp16", training_config.get("fp16", True))

    # W&B run name ìƒì„±
    run_name = wandb_logger._create_run_name(model_config) if wandb_logger.enabled else f"{nickname}"

    # Gradient Clipping Callback ìƒì„± (configì—ì„œ max_grad_norm ì½ê¸°)
    max_grad_norm = training_config.get("max_grad_norm", 1.0)
    grad_clip_callback = GradientClippingCallback(max_grad_norm)

    # Encoder-Decoder vs Causal LMì— ë”°ë¼ ë‹¤ë¥¸ Trainer ì‚¬ìš©
    if model_type == "encoder_decoder":
        # Seq2SeqTrainer (generation ì§€ì›)
        args = Seq2SeqTrainingArguments(
            output_dir=str(output_dir),
            num_train_epochs=training_config["num_train_epochs"],
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=training_config["per_device_eval_batch_size"],
            gradient_accumulation_steps=training_config["gradient_accumulation_steps"],
            learning_rate=lr,
            warmup_ratio=training_config["warmup_ratio"],
            lr_scheduler_type=training_config["lr_scheduler_type"],
            optim=training_config["optim"],
            adam_beta1=training_config.get("adam_beta1", 0.9),
            adam_beta2=training_config.get("adam_beta2", 0.999),
            max_grad_norm=training_config.get("max_grad_norm", 0.3),
            bf16=use_bf16,
            fp16=use_fp16,
            gradient_checkpointing=training_config.get("gradient_checkpointing", True),
            predict_with_generate=True,
            generation_max_length=training_config.get("generation_max_length", 100),
            generation_num_beams=training_config.get("generation_num_beams", 4),
            save_strategy=training_config.get("save_strategy", "epoch"),
            eval_strategy=training_config.get("evaluation_strategy", "epoch"),
            save_total_limit=training_config.get("save_total_limit", 2),
            load_best_model_at_end=training_config.get("load_best_model_at_end", True),
            metric_for_best_model=training_config.get("metric_for_best_model", "rouge_sum"),
            greater_is_better=training_config.get("greater_is_better", True),
            logging_steps=training_config.get("logging_steps", 10),
            logging_first_step=training_config.get("logging_first_step", True),
            report_to="wandb" if wandb_logger.enabled else "none",
            run_name=run_name
        )

        data_collator = DataCollatorForSeq2Seq(
            tokenizer=tokenizer,
            model=model,
            padding=True
        )

        trainer = Seq2SeqTrainer(
            model=model,
            args=args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=tokenizer,
            data_collator=data_collator,
            compute_metrics=lambda eval_preds: compute_metrics(eval_preds, tokenizer),
            callbacks=[grad_clip_callback]
        )

    elif model_type == "causal_lm":
        # SFTTrainer (Korean_DCS_2024 ë² ì´ìŠ¤ë¼ì¸)
        if not TRL_AVAILABLE:
            raise ImportError("TRL not installed. Run: pip install trl")

        # gradient_checkpointing_kwargs ê°€ì ¸ì˜¤ê¸°
        gradient_checkpointing_kwargs = training_config.get("gradient_checkpointing_kwargs", {"use_reentrant": False})

        args = SFTConfig(
            # ê¸°ë³¸ ì„¤ì • (Korean_DCS_2024 ë² ì´ìŠ¤ë¼ì¸)
            output_dir=str(output_dir),
            overwrite_output_dir=True,  # ì¶œë ¥ ë””ë ‰í† ë¦¬ ë®ì–´ì“°ê¸°
            do_train=True,
            do_eval=True,
            # í•™ìŠµ ì„¤ì •
            num_train_epochs=training_config["num_train_epochs"],
            max_steps=-1,  # Epoch ê¸°ë°˜ í•™ìŠµ (-1 = ë¬´ì œí•œ)
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=training_config["per_device_eval_batch_size"],
            gradient_accumulation_steps=training_config["gradient_accumulation_steps"],
            # ì˜µí‹°ë§ˆì´ì €
            learning_rate=lr,
            warmup_ratio=training_config["warmup_ratio"],
            lr_scheduler_type=training_config["lr_scheduler_type"],
            weight_decay=training_config.get("weight_decay", 0.1),
            optim=training_config["optim"],
            adam_beta1=training_config.get("adam_beta1", 0.9),
            adam_beta2=training_config.get("adam_beta2", 0.999),
            max_grad_norm=training_config.get("max_grad_norm", 1.2),
            # Float precision
            bf16=use_bf16,
            fp16=use_fp16,
            # Gradient checkpointing
            gradient_checkpointing=training_config.get("gradient_checkpointing", True),
            gradient_checkpointing_kwargs=gradient_checkpointing_kwargs,
            # ì €ì¥ & í‰ê°€
            save_strategy=training_config.get("save_strategy", "epoch"),
            eval_strategy=training_config.get("evaluation_strategy", "epoch"),
            save_total_limit=training_config.get("save_total_limit", 2),
            load_best_model_at_end=training_config.get("load_best_model_at_end", True),
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            # ë¡œê¹…
            log_level="info",
            logging_steps=training_config.get("logging_steps", 10),
            logging_first_step=training_config.get("logging_first_step", True),
            report_to="wandb" if wandb_logger.enabled else "none",
            run_name=run_name,
            # SFT íŠ¹í™” íŒŒë¼ë¯¸í„° (TRL 0.23.1 í˜¸í™˜)
            max_length=config["tokenizer"].get("encoder_max_len", 1024),  # TRL 0.23.1: max_length (0.9.4: max_seq_length)
            packing=True,  # íš¨ìœ¨ì„± í–¥ìƒ (2-3x speedup)
            seed=42
        )

        # Korean_DCS_2024 DataCollator
        data_collator = DataCollatorForSupervisedDataset(tokenizer)

        trainer = SFTTrainer(
            model=model,
            processing_class=tokenizer,  # TRL 0.23.1: processing_class (0.9.4: tokenizer)
            args=args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator,
            callbacks=[grad_clip_callback]
        )

    else:
        raise ValueError(f"Unknown model_type: {model_type}")

    print(f"\nğŸš€ í•™ìŠµ ì‹œì‘: {nickname}")
    print(f"   Model Type: {model_type}")
    print(f"   Trainer: {'Seq2SeqTrainer' if model_type == 'encoder_decoder' else 'SFTTrainer (packing=True)'}")
    print(f"   LR: {lr}, Batch: {batch_size}, Float: {'bf16' if use_bf16 else 'fp16'}")
    print(f"   Output: {output_dir}\n")

    # Train
    trainer.train()

    # Save
    final_dir = output_dir / "final_model"
    trainer.save_model(str(final_dir))
    tokenizer.save_pretrained(str(final_dir))

    print(f"âœ… í•™ìŠµ ì™„ë£Œ: {final_dir}")

    return trainer


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, required=True, help="Config file path")
    args = parser.parse_args()

    # Load config
    with open(args.config, "r") as f:
        config = yaml.safe_load(f)

    print(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ: {args.config}")

    # Seed
    set_seed(42)

    # W&B Logger ì´ˆê¸°í™”
    wandb_logger = WandBLogger(config)
    print(f"âœ… W&B Logger ì´ˆê¸°í™” (enabled={wandb_logger.enabled})")

    # Load data
    data_path = config["general"]["data_path"]
    train_df, dev_df = load_data(data_path)

    # Train each model
    models = config["models"]
    print(f"\n{'='*80}")
    print(f"ğŸš€ ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹œì‘: {len(models)}ê°œ ëª¨ë¸")
    print(f"{'='*80}\n")

    for idx, model_config in enumerate(models, 1):
        nickname = model_config["nickname"]

        print(f"\n{'#'*80}")
        print(f"# [{idx}/{len(models)}] {nickname}")
        print(f"# Model: {model_config['model_name']}")
        print(f"# Description: {model_config['description']}")
        print(f"{'#'*80}\n")

        # Disk check
        disk_usage = check_disk_usage()
        print(f"ğŸ’¾ í˜„ì¬ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰: {disk_usage:.2f} GB")

        if disk_usage > config["general"]["max_disk_usage_gb"]:
            print(f"âš ï¸  ë””ìŠ¤í¬ ìš©ëŸ‰ ì´ˆê³¼! ìºì‹œ ì •ë¦¬ ì¤‘...")
            cleanup_hf_cache()

        try:
            # W&B Run ì´ˆê¸°í™” (ëª¨ë¸ë³„)
            wandb_logger.init_run(model_config)

            # Load model
            model, tokenizer = load_model_and_tokenizer(model_config, config)

            # Prepare datasets
            print("ğŸ“Š ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")
            train_dataset = prepare_dataset(
                train_df, tokenizer, model_config, config,
                max_input_length=config["tokenizer"]["encoder_max_len"],
                max_target_length=config["tokenizer"]["decoder_max_len"]
            )
            eval_dataset = prepare_dataset(
                dev_df, tokenizer, model_config, config,
                max_input_length=config["tokenizer"]["encoder_max_len"],
                max_target_length=config["tokenizer"]["decoder_max_len"]
            )
            print(f"âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: Train {len(train_dataset)}, Eval {len(eval_dataset)}")

            # 1. Train
            trainer = train_model(
                model, tokenizer, train_dataset, eval_dataset,
                model_config, config, wandb_logger
            )

            # 2. Inference on Test set
            print(f"\n{'='*80}")
            print(f"ğŸ“Š Test Set ì¶”ë¡  ì‹œì‘: {nickname}")
            print(f"{'='*80}\n")

            device = "cuda" if torch.cuda.is_available() else "cpu"

            # Test ë°ì´í„° ë¡œë“œ
            test_file = Path(config["general"]["data_path"]) / "test.csv"
            if not test_file.exists():
                print(f"âš ï¸  Test íŒŒì¼ ì—†ìŒ: {test_file}")
                print(f"   Test ì¶”ë¡ ì„ ê±´ë„ˆë›°ê³  ë‹¤ìŒ ëª¨ë¸ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\n")
                wandb_logger.finish()
                continue

            test_df = pd.read_csv(test_file)
            print(f"âœ… Test ë°ì´í„° ë¡œë“œ: {len(test_df)}ê°œ ìƒ˜í”Œ\n")

            # Test set ì¶”ë¡  (references ì—†ìŒ)
            predictions, _ = run_inference_on_dev(
                model, tokenizer, test_df, device,
                model_config, config,
                batch_size=4,
                max_new_tokens=150,
                num_beams=4,
                is_test=True  # Test mode
            )

            # 3. Save Submission CSV
            submission_dir = Path(config["general"]["output_base_dir"]) / "submissions"
            submission_dir.mkdir(parents=True, exist_ok=True)
            submission_file = submission_dir / f"{nickname}_submission.csv"

            submission_df = pd.DataFrame({
                "fname": test_df['fname'].tolist(),
                "summary": predictions
            })
            submission_df.to_csv(submission_file, index=False)

            print(f"\n{'='*80}")
            print(f"ğŸ’¾ Submission íŒŒì¼ ì €ì¥ ì™„ë£Œ")
            print(f"{'='*80}")
            print(f"ê²½ë¡œ: {submission_file}")
            print(f"ìƒ˜í”Œ ìˆ˜: {len(submission_df)}")
            print(f"\nìƒ˜í”Œ (ì²˜ìŒ 3ê°œ):")
            print(submission_df.head(3))
            print(f"{'='*80}\n")

            # W&B Run ì¢…ë£Œ
            wandb_logger.finish()

            # 5. Cleanup - ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ (Config ì„¤ì •ì— ë”°ë¼)
            if config["disk_management"]["cleanup_old_checkpoints"]:
                print(f"\nğŸ—‘ï¸  ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ ì¤‘...")
                checkpoint_dir = Path(config["general"]["output_base_dir"]) / nickname
                if checkpoint_dir.exists():
                    size_before = sum(f.stat().st_size for f in checkpoint_dir.rglob('*') if f.is_file()) / (1024**3)
                    shutil.rmtree(checkpoint_dir)
                    print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ ì™„ë£Œ: {nickname} ({size_before:.2f}GB í™•ë³´)")
            else:
                print(f"\nğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ë³´ì¡´ ëª¨ë“œ: {nickname} ì‚­ì œ ê±´ë„ˆë›°ê¸°")

            # Cleanup model & memory
            del model
            del tokenizer
            del trainer
            gc.collect()
            torch.cuda.empty_cache()
            print("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

            # Cleanup model cache
            print(f"\nğŸ—‘ï¸  ëª¨ë¸ ìºì‹œ ì‚­ì œ ì¤‘...")
            cleanup_hf_cache(model_config["model_name"])

            # Final disk check
            disk_usage_after = check_disk_usage()
            print(f"ğŸ’¾ ìµœì¢… ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰: {disk_usage_after:.2f} GB\n")

        except Exception as e:
            print(f"âŒ {nickname} í•™ìŠµ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()

            # ì‹¤íŒ¨ ì‹œì—ë„ W&B Run ì¢…ë£Œ
            wandb_logger.finish()
            continue

    print(f"\n{'='*80}")
    print("ğŸ‰ ëª¨ë“  ëª¨ë¸ íŒŒì¸íŠœë‹ ì™„ë£Œ!")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    main()