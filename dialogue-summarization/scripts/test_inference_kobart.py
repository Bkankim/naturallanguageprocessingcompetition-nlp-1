"""
KoBART Checkpoint ì¶”ë¡  í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ëª©ì : koBART fine-tuned ëª¨ë¸ë¡œ ì¶”ë¡  ë° ROUGE í‰ê°€
"""
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from tqdm import tqdm
from src.evaluation.metrics import calculate_rouge_scores

def load_model_and_tokenizer(checkpoint_path: str):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”©: {checkpoint_path}")

    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)
    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)

    # GPU ì‚¬ìš©
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device)
    model.eval()

    print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ (device: {device})")
    return model, tokenizer, device


def generate_summaries(
    model,
    tokenizer,
    dialogues: list,
    device: str,
    batch_size: int = 4,
    max_length: int = 512,
    max_new_tokens: int = 100,
    num_beams: int = 4
):
    """ë°°ì¹˜ ì¶”ë¡ ìœ¼ë¡œ ìš”ì•½ ìƒì„±"""
    summaries = []

    print(f"\nğŸ”„ ì¶”ë¡  ì‹œì‘ (batch_size={batch_size}, num_beams={num_beams})")

    for i in tqdm(range(0, len(dialogues), batch_size), desc="Generating"):
        batch = dialogues[i:i+batch_size]

        # Tokenize
        inputs = tokenizer(
            batch,
            max_length=max_length,
            truncation=True,
            padding=True,
            return_tensors="pt"
        )

        # BARTëŠ” token_type_idsë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°
        inputs = {k: v.to(device) for k, v in inputs.items() if k != 'token_type_ids'}

        # Generate
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                num_beams=num_beams,
                early_stopping=True,
                no_repeat_ngram_size=2
            )

        # Decode (skip_special_tokens=Falseë¡œ ëª¨ë“  í† í° í¬í•¨)
        batch_summaries = tokenizer.batch_decode(
            outputs,
            skip_special_tokens=False  # Baselineê³¼ ë™ì¼í•˜ê²Œ False
        )
        summaries.extend(batch_summaries)

    return summaries


def clean_predictions(predictions: list, tokenizer) -> list:
    """ìƒì„± ê²°ê³¼ì—ì„œ ë¶ˆí•„ìš”í•œ ëª¨ë¸ í† í° ì œê±° (Baseline ë°©ì‹)"""
    # ì œê±°í•  í† í°: ëª¨ë¸ ê´€ë ¨ í† í°ë§Œ (ë°ì´í„° í† í°ì€ ìœ ì§€)
    remove_tokens = [
        '<usr>',  # KoBART specific
        tokenizer.bos_token,  # <s>
        tokenizer.eos_token,  # </s>
        tokenizer.pad_token,  # <pad>
    ]

    cleaned = predictions.copy()
    for token in remove_tokens:
        if token:  # None ì²´í¬
            cleaned = [s.replace(token, " ") for s in cleaned]

    return cleaned


def evaluate_rouge(predictions: list, references: list):
    """ROUGE ì ìˆ˜ ê³„ì‚° (Mecab)"""
    print("\nğŸ“Š ROUGE í‰ê°€ ì¤‘ (Mecab tokenization)...")

    # Mecab í† í¬ë‚˜ì´ì œì´ì…˜ìœ¼ë¡œ ROUGE ê³„ì‚°
    scores = calculate_rouge_scores(
        predictions=predictions,
        references=references,
        tokenization_mode='mecab'
    )

    return scores


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("="*80)
    print("KoBART ì¶”ë¡  í…ŒìŠ¤íŠ¸")
    print("="*80)

    # ê²½ë¡œ ì„¤ì •
    checkpoint_path = project_root / "checkpoints/llm_finetuning/koBART-summarization/final_model"
    data_path = project_root.parent / "data/dev.csv"

    # ì²´í¬í¬ì¸íŠ¸ ì¡´ì¬ í™•ì¸
    if not checkpoint_path.exists():
        print(f"âŒ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
        return

    print(f"\nğŸ“‚ ì²´í¬í¬ì¸íŠ¸: {checkpoint_path}")
    print(f"ğŸ“‚ ë°ì´í„°: {data_path}")

    # 1. ëª¨ë¸ ë¡œë“œ
    model, tokenizer, device = load_model_and_tokenizer(str(checkpoint_path))

    # 2. ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“¥ ë°ì´í„° ë¡œë”©: {data_path}")
    df = pd.read_csv(data_path)
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ")

    # ì‘ì€ ìƒ˜í”Œë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸ (50ê°œ)
    n_samples = min(50, len(df))
    df_sample = df.head(n_samples)

    print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {n_samples}ê°œ")

    dialogues = df_sample['dialogue'].tolist()
    references = df_sample['summary'].tolist()

    # 3. ì¶”ë¡ 
    predictions = generate_summaries(
        model=model,
        tokenizer=tokenizer,
        dialogues=dialogues,
        device=device,
        batch_size=4,
        max_new_tokens=100,
        num_beams=4
    )

    # 3-1. í† í° ì •ì œ (Baseline ë°©ì‹)
    print("\nğŸ§¹ ëª¨ë¸ í† í° ì œê±° ì¤‘...")
    predictions_cleaned = clean_predictions(predictions, tokenizer)

    # 4. ROUGE í‰ê°€
    scores = evaluate_rouge(predictions_cleaned, references)

    # 5. ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*80)
    print("ğŸ“ˆ ROUGE ì ìˆ˜ (Mecab tokenization)")
    print("="*80)
    print(f"ROUGE-1 F1: {scores['rouge-1']['f']:.4f}")
    print(f"ROUGE-2 F1: {scores['rouge-2']['f']:.4f}")
    print(f"ROUGE-L F1: {scores['rouge-l']['f']:.4f}")
    print(f"ROUGE SUM:  {scores['rouge-1']['f'] + scores['rouge-2']['f'] + scores['rouge-l']['f']:.4f}")
    print("="*80)

    # 6. ìƒ˜í”Œ ì¶œë ¥
    print("\n" + "="*80)
    print("ğŸ“ ìƒ˜í”Œ ì˜ˆì¸¡ (ì²˜ìŒ 3ê°œ)")
    print("="*80)
    for i in range(min(3, len(predictions_cleaned))):
        print(f"\n[ìƒ˜í”Œ {i+1}]")
        print(f"ëŒ€í™”: {dialogues[i][:100]}...")
        print(f"ì •ë‹µ: {references[i]}")
        print(f"ì˜ˆì¸¡ (ì›ë³¸): {predictions[i]}")
        print(f"ì˜ˆì¸¡ (ì •ì œ): {predictions_cleaned[i]}")

    print("\nâœ… ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del model
    del tokenizer
    torch.cuda.empty_cache()


if __name__ == "__main__":
    main()