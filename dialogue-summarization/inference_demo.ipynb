{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ—¨ï¸ Dialogue Summarization - Inference Demo\n",
    "\n",
    "> í•™ìŠµëœ ëª¨ë¸ë¡œ test ì…‹ ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ test ë°ì´í„°ì— ëŒ€í•œ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€\n",
    "project_root = '/Competition/NLP/dialogue-summarization'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "os.chdir(project_root)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ëª¨ë“ˆ Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë“ˆë“¤\n",
    "from src.data.preprocessor import Preprocess\n",
    "from src.data.dataset import DatasetForInference\n",
    "from src.models.model_loader import load_tokenizer_and_model\n",
    "from src.evaluation.metrics import clean_text\n",
    "\n",
    "print(\"âœ… All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²½ë¡œ ì„¤ì •\n",
    "DATA_PATH = \"/Competition/NLP/naturallanguageprocessingcompetition-nlp-1/data\"\n",
    "CHECKPOINT_PATH = \"/Competition/NLP/dialogue-summarization/checkpoints/baseline_run/checkpoint-1750\"  # Best model\n",
    "OUTPUT_DIR = \"/Competition/NLP/dialogue-summarization/submissions\"\n",
    "\n",
    "# ìƒì„± íŒŒë¼ë¯¸í„°\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_OUTPUT_LENGTH = 100\n",
    "NUM_BEAMS = 4\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Special tokens\n",
    "SPECIAL_TOKENS = ['#Person1#', '#Person2#', '#Person3#', '#PhoneNumber#', '#Address#', '#PassportNumber#']\n",
    "\n",
    "print(\"ğŸ“‹ Configuration:\")\n",
    "print(f\"  Data path: {DATA_PATH}\")\n",
    "print(f\"  Checkpoint: {CHECKPOINT_PATH}\")\n",
    "print(f\"  Output dir: {OUTPUT_DIR}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Num beams: {NUM_BEAMS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ë””ë°”ì´ìŠ¤ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"ğŸ–¥ï¸  Device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“¥ Loading model and tokenizer...\")\n",
    "\n",
    "model, tokenizer = load_tokenizer_and_model(\n",
    "    model_name=\"digit82/kobart-summarization\",  # Base model name for config\n",
    "    special_tokens=SPECIAL_TOKENS,\n",
    "    device=device,\n",
    "    checkpoint_path=CHECKPOINT_PATH\n",
    ")\n",
    "\n",
    "model.eval()  # Evaluation mode\n",
    "\n",
    "print(f\"âœ… Model loaded from: {CHECKPOINT_PATH}\")\n",
    "print(f\"   Vocab size: {len(tokenizer)}\")\n",
    "print(f\"   Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessor ì´ˆê¸°í™”\n",
    "preprocessor = Preprocess(\n",
    "    bos_token=tokenizer.bos_token,\n",
    "    eos_token=tokenizer.eos_token\n",
    ")\n",
    "\n",
    "# Test ë°ì´í„° ë¡œë“œ\n",
    "test_data = preprocessor.make_set_as_df(f\"{DATA_PATH}/test.csv\", is_train=False)\n",
    "\n",
    "print(f\"ğŸ“Š Test data loaded: {len(test_data)} samples\")\n",
    "print(f\"\\nğŸ” Sample:\")\n",
    "print(f\"   ID: {test_data.iloc[0]['fname']}\")\n",
    "print(f\"   Dialogue: {test_data.iloc[0]['dialogue'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ì „ì²˜ë¦¬\nencoder_input_test, _ = preprocessor.make_input(test_data, is_test=True)\n\n# í† í¬ë‚˜ì´ì§•\ntokenized_encoder_inputs = tokenizer(\n    encoder_input_test,\n    return_tensors=\"pt\",\n    padding=True,\n    truncation=True,\n    max_length=MAX_INPUT_LENGTH,\n    return_token_type_ids=False\n)\n\nprint(f\"âœ… Tokenization completed\")\nprint(f\"   Input shape: {tokenized_encoder_inputs['input_ids'].shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dataset & DataLoader ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "test_dataset = DatasetForInference(\n    tokenized_encoder_inputs,\n    test_data['fname'].tolist(),  # test_idë¡œ fname ë¦¬ìŠ¤íŠ¸ ì „ë‹¬\n    len(test_data)\n)\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n\nprint(f\"âœ… DataLoader created\")\nprint(f\"   Total batches: {len(test_dataloader)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ì¶”ë¡  ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸš€ Starting inference...\")\n",
    "\n",
    "summaries = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Generating summaries\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        # Generate\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=MAX_OUTPUT_LENGTH,\n",
    "            num_beams=NUM_BEAMS,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "        \n",
    "        # Decode\n",
    "        decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)\n",
    "        summaries.extend(decoded)\n",
    "\n",
    "print(f\"\\nâœ… Inference completed!\")\n",
    "print(f\"   Generated {len(summaries)} summaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. í›„ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# í† í° ì •ë¦¬\nremove_tokens = ['<usr>', tokenizer.bos_token, tokenizer.eos_token, tokenizer.pad_token]\ncleaned_summaries = clean_text(summaries, remove_tokens)  # ë¦¬ìŠ¤íŠ¸ ì „ì²´ë¥¼ í•œ ë²ˆì— ì „ë‹¬\n\nprint(\"âœ… Post-processing completed\")\nprint(f\"\\nğŸ” Sample outputs:\")\nfor i in range(min(3, len(cleaned_summaries))):\n    print(f\"\\n[Sample {i+1}]\")\n    print(f\"Original: {summaries[i][:80]}...\")\n    print(f\"Cleaned:  {cleaned_summaries[i][:80]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ì œì¶œ íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# DataFrame ìƒì„±\n",
    "submission_df = pd.DataFrame({\n",
    "    'fname': test_data['fname'],\n",
    "    'summary': cleaned_summaries\n",
    "})\n",
    "\n",
    "# íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_filename = f\"submission_{timestamp}.csv\"\n",
    "output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "\n",
    "# CSV ì €ì¥\n",
    "submission_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"âœ… Submission file saved!\")\n",
    "print(f\"   Path: {output_path}\")\n",
    "print(f\"   Rows: {len(submission_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. ê²°ê³¼ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì œì¶œ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°\n",
    "print(\"ğŸ“„ Submission file preview:\")\n",
    "print(submission_df.head(10))\n",
    "\n",
    "# í†µê³„\n",
    "print(f\"\\nğŸ“Š Statistics:\")\n",
    "print(f\"   Total samples: {len(submission_df)}\")\n",
    "print(f\"   Average summary length: {submission_df['summary'].str.len().mean():.1f} chars\")\n",
    "print(f\"   Min summary length: {submission_df['summary'].str.len().min()} chars\")\n",
    "print(f\"   Max summary length: {submission_df['summary'].str.len().max()} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. ìƒ˜í”Œ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª‡ ê°œ ìƒ˜í”Œ ìì„¸íˆ í™•ì¸\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "sample_indices = random.sample(range(len(test_data)), min(3, len(test_data)))\n",
    "\n",
    "for idx in sample_indices:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Sample {idx + 1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n[Dialogue]\")\n",
    "    print(test_data.iloc[idx]['dialogue'][:300] + \"...\")\n",
    "    print(f\"\\n[Generated Summary]\")\n",
    "    print(submission_df.iloc[idx]['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì™„ë£Œ!\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„:\n",
    "1. **ì œì¶œ íŒŒì¼ í™•ì¸**: `submissions/` í´ë”ì˜ CSV íŒŒì¼ í™•ì¸\n",
    "2. **ê²½ì§„ëŒ€íšŒ ì œì¶œ**: í”Œë«í¼ì— CSV íŒŒì¼ ì—…ë¡œë“œ\n",
    "3. **ê²°ê³¼ í™•ì¸**: ë¦¬ë”ë³´ë“œì—ì„œ ì ìˆ˜ í™•ì¸"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}