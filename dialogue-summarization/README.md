# ğŸ—¨ï¸ Korean Dialogue Summarization

> baseline.ipynb ê¸°ë°˜ ê¹”ë”í•œ ëª¨ë“ˆí™” í”„ë¡œì íŠ¸

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

í•œêµ­ì–´ ëŒ€í™” ìš”ì•½ ê²½ì§„ëŒ€íšŒë¥¼ ìœ„í•œ **BART ê¸°ë°˜ Seq2Seq ëª¨ë¸**ì…ë‹ˆë‹¤.
baseline.ipynbì˜ ê²€ì¦ëœ ì½”ë“œë¥¼ ëª¨ë“ˆí™”í•˜ì—¬ ì¬ì‚¬ìš©ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.

## ğŸ“Š ìµœì‹  ì‹¤í—˜ ê²°ê³¼

### Zero-shot LLM Screening (2025-10-04)

5ê°œ í•œêµ­ì–´ LLMì„ ëŒ€í™” ìš”ì•½ íƒœìŠ¤í¬ë¡œ ì œë¡œìƒ· í‰ê°€í•œ ê²°ê³¼:

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | ROUGE Sum | ìƒíƒœ |
|------|----------|-----------|------|
| Llama-3.2-Korean-3B | 3.2B | **49.52** | âœ… 1ìœ„ |
| Llama-3-Korean-8B | 8.0B | 48.61 | ğŸ¥ˆ 2ìœ„ |
| Qwen2.5-7B | 7.6B | 46.84 | ğŸ¥‰ 3ìœ„ |
| Qwen3-4B-Instruct | 4.0B | 45.02 | 4ìœ„ |
| Llama-3.2-AICA-5B | 4.3B | 41.99 | 5ìœ„ |

**ë‹¤ìŒ ë‹¨ê³„**: Llama-3.2-Korean-3B íŒŒì¸íŠœë‹ (ì˜ˆìƒ ì†Œìš” ì‹œê°„: ~1ì‹œê°„)

ìì„¸í•œ ê²°ê³¼ëŠ” [EXPERIMENT_LOG.md](./EXPERIMENT_LOG.md) ì°¸ì¡°.

### ì£¼ìš” íŠ¹ì§•

- âœ… **LLM Screening ì™„ë£Œ** - 5ê°œ ëª¨ë¸ Zero-shot í‰ê°€ (Llama-3.2-3B ì„ ì •)
- âœ… **konlpy/Java ì˜ì¡´ì„± ì œê±°** - rouge ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ì‚¬ìš©
- âœ… **ëª¨ë“ˆí™”ëœ êµ¬ì¡°** - ê° ê¸°ëŠ¥ì„ ë…ë¦½ì ì¸ ëª¨ë“ˆë¡œ ë¶„ë¦¬
- âœ… **íƒ€ì… ì•ˆì „ì„±** - ëª¨ë“  í•¨ìˆ˜ì— Type hints ì ìš©
- âœ… **ìƒì„¸í•œ ë¬¸ì„œí™”** - Docstring ë° ê°€ì´ë“œ ì™„ë¹„
- âœ… **baseline.ipynb ê²€ì¦** - ì›ë³¸ ì½”ë“œì˜ ì•ˆì •ì„± ë³´ì¥

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í•™ìŠµ ì‹¤í–‰

```bash
cd /Competition/NLP/dialogue-summarization

python scripts/train_baseline.py --config configs/train_config.yaml
```

### 2. ì¶”ë¡  ì‹¤í–‰

```bash
python scripts/generate_predictions.py \
    --config configs/base_config.yaml \
    --checkpoint checkpoints/baseline_run/final_model
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
dialogue-summarization/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/              # ë°ì´í„° ì „ì²˜ë¦¬
â”‚   â”‚   â”œâ”€â”€ preprocessor.py
â”‚   â”‚   â””â”€â”€ dataset.py
â”‚   â”œâ”€â”€ models/            # ëª¨ë¸ ë¡œë”©
â”‚   â”‚   â””â”€â”€ model_loader.py
â”‚   â”œâ”€â”€ training/          # í•™ìŠµ ê´€ë ¨ (í–¥í›„ í™•ì¥)
â”‚   â”œâ”€â”€ evaluation/        # í‰ê°€ ë©”íŠ¸ë¦­
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â”œâ”€â”€ inference/         # ì¶”ë¡  ê´€ë ¨ (í–¥í›„ í™•ì¥)
â”‚   â””â”€â”€ utils/             # ìœ í‹¸ë¦¬í‹°
â”‚       â””â”€â”€ seed.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_baseline.py           # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ generate_predictions.py     # ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ run_inference.sh            # ì¶”ë¡  ì‰˜ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base_config.yaml            # ê¸°ë³¸ ì„¤ì •
â”‚   â””â”€â”€ train_config.yaml           # í•™ìŠµ ì„¤ì •
â”œâ”€â”€ checkpoints/                    # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
â”œâ”€â”€ logs/                           # ë¡œê·¸ íŒŒì¼
â””â”€â”€ submissions/                    # ì œì¶œ íŒŒì¼
```

## ğŸ”§ ì„¤ì • íŒŒì¼

### train_config.yaml

```yaml
general:
  data_path: "/Competition/NLP/naturallanguageprocessingcompetition-nlp-1/data/"
  model_name: "digit82/kobart-summarization"  # ìš”ì•½ íŠ¹í™” ëª¨ë¸
  output_dir: "./checkpoints/baseline_run"

training:
  num_train_epochs: 20
  per_device_train_batch_size: 50
  per_device_eval_batch_size: 32
  learning_rate: 1.0e-5
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
  # ... ê¸°íƒ€ ì„¤ì •
```

## ğŸ“Š ì„±ëŠ¥

### Dev Set (Validation)
| ëª¨ë¸ | ROUGE-1 | ROUGE-2 | ROUGE-L | ROUGE Sum |
|------|---------|---------|---------|-----------|
| baseline.ipynb | ~16 | ~9 | ~14 | **~47** |
| Modular Structure | 32.28 | 13.46 | 30.03 | **75.77** |

### Test Set (ê²½ì§„ëŒ€íšŒ ì œì¶œ)
| ì‹¤í—˜ | ROUGE-1 | ROUGE-2 | ROUGE-L | Final Score |
|------|---------|---------|---------|-------------|
| **Experiment #1** (Baseline) | **0.5660** | **0.3675** | **0.4719** | **46.8487** |

**ğŸ“ ìƒì„¸ ê¸°ë¡**: [EXPERIMENT_LOG.md](./EXPERIMENT_LOG.md)

## ğŸ› ï¸ ê°œë°œ ê°€ì´ë“œ

### ëª¨ë“ˆ ì‚¬ìš© ì˜ˆì‹œ

```python
# ë°ì´í„° ì „ì²˜ë¦¬
from src.data.preprocessor import Preprocess
from src.data.dataset import DatasetForTrain

preprocessor = Preprocess(bos_token="<s>", eos_token="</s>")
dataset = DatasetForTrain(encoder_input, decoder_input, labels, len=100)

# ëª¨ë¸ ë¡œë”©
from src.models.model_loader import load_tokenizer_and_model

model, tokenizer = load_tokenizer_and_model(
    model_name="digit82/kobart-summarization",
    special_tokens=['#Person1#', '#Person2#']
)

# í‰ê°€ ë©”íŠ¸ë¦­
from src.evaluation.metrics import compute_metrics_for_trainer

compute_metrics = compute_metrics_for_trainer(tokenizer)
```

### ì˜ì¡´ì„±

- Python 3.10+
- PyTorch 2.0+
- Transformers 4.35+
- rouge 1.0.1
- pandas, PyYAML, tqdm

**ì¤‘ìš”**: konlpyë‚˜ JavaëŠ” í•„ìš” ì—†ìŠµë‹ˆë‹¤!

## ğŸ“ ì£¼ìš” ë³€ê²½ì‚¬í•­

### baseline.ipynb ëŒ€ë¹„ ê°œì„ ì 

1. **Java ì˜ì¡´ì„± ì œê±°**
   - konlpy â†’ rouge ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ëŒ€ì²´
   - JVM ì´ˆê¸°í™” ì˜¤ë¥˜ ì™„ì „ í•´ê²°

2. **ëª¨ë“ˆí™”**
   - ë‹¨ì¼ ë…¸íŠ¸ë¶ â†’ ë…ë¦½ì ì¸ ëª¨ë“ˆë“¤
   - ì¬ì‚¬ìš©ì„± ë° í…ŒìŠ¤íŠ¸ ìš©ì´ì„± í–¥ìƒ

3. **íƒ€ì… ì•ˆì „ì„±**
   - ëª¨ë“  í•¨ìˆ˜ì— Type hints ì¶”ê°€
   - IDE ìë™ì™„ì„± ë° íƒ€ì… ì²´í‚¹ ì§€ì›

4. **ë¬¸ì„œí™”**
   - ìƒì„¸í•œ Docstring (Google ìŠ¤íƒ€ì¼)
   - README ë° ì‚¬ìš© ê°€ì´ë“œ ì™„ë¹„

## ğŸ’¡ ê°œë°œ ì¸ì‚¬ì´íŠ¸ & ë°°ìš´ ì 

### 1. ëª¨ë¸ í¬ê¸° â‰  ì„±ëŠ¥

**ë¬¸ì œ**: ì²˜ìŒì—ëŠ” "í° ëª¨ë¸ì´ ë¬´ì¡°ê±´ ì¢‹ì„ ê²ƒ"ì´ë¼ê³  ìƒê°í–ˆìŠµë‹ˆë‹¤.

**ë°œê²¬**: Llama-3.2-Korean-3B (3.21B)ê°€ AICA-5B (4.31B)ì™€ Llama-3-8B (8.03B)ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | ROUGE Sum | ë¹„ê³  |
|------|----------|-----------|------|
| Llama-3.2-Korean-3B | 3.21B | **49.52** | ğŸ† |
| Llama-3-Korean-8B | 8.03B | 48.61 | 2.6ë°° í¬ì§€ë§Œ 0.91ì ë§Œ ì°¨ì´ |
| Llama-3.2-AICA-5B | 4.31B | 41.99 | 1.3ë°° í¬ì§€ë§Œ 7.53ì  ë‚®ìŒ |

**êµí›ˆ**:
- **Task Alignmentì´ ëª¨ë¸ í¬ê¸°ë³´ë‹¤ ì¤‘ìš”í•©ë‹ˆë‹¤**
- Instruction-tuned ëª¨ë¸ > Conversation-specialized ëª¨ë¸ (ìš”ì•½ íƒœìŠ¤í¬ì˜ ê²½ìš°)
- AICAëŠ” "ëŒ€í™” ìƒì„±"ì— íŠ¹í™”ë˜ì–´ Zero-shot ìš”ì•½ ì„±ëŠ¥ì´ ë‚®ì•˜ìŠµë‹ˆë‹¤
- íŒŒì¸íŠœë‹ìœ¼ë¡œ ì—­ì „ ê°€ëŠ¥ì„±ì€ ìˆì§€ë§Œ, ì¢‹ì€ ì¶œë°œì ì„ ì„ íƒí•˜ëŠ” ê²Œ íš¨ìœ¨ì ì…ë‹ˆë‹¤

### 2. SOLAR ëª¨ë¸ì˜ ê·¹ì‹¬í•œ ì†ë„ ì €í•˜

**ë¬¸ì œ**: SOLAR-10.7Bê°€ ë‹¤ë¥¸ ëª¨ë¸ ëŒ€ë¹„ **40ë°° ëŠë ¸ìŠµë‹ˆë‹¤** (22ë¶„/ë°°ì¹˜ vs 27-33ì´ˆ/ë°°ì¹˜).

**ê°€ì„¤ ê²€ì¦**:
- âœ… ëª¨ë¸ í¬ê¸° (10.7B vs 7-8B): 1.5ë°° ì°¨ì´ëŠ” 40ë°° ì†ë„ ì €í•˜ë¥¼ ì„¤ëª…í•˜ì§€ ëª»í•¨
- âœ… ì–‘ìí™” ë¬¸ì œ: 4bit NF4 ì •ìƒ ì‘ë™ (15GB GPU, 2GB RAM)
- âœ… FlashAttention: SDPA í™œì„±í™”ë¨
- âœ… ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´: ëª¨ë‘ ë™ì¼
- ğŸš¨ **Root Cause**: **Depth Upscaling ì•„í‚¤í…ì²˜**

**ë°œê²¬**: SOLARì€ ë‘ ê°œì˜ Llama2-7B ëª¨ë¸ì„ ìˆ˜ì§ìœ¼ë¡œ ë³‘í•©í•˜ì—¬ ~48 layersë¥¼ ê°€ì§‘ë‹ˆë‹¤.
- ì¼ë°˜ 10B ëª¨ë¸: ~32 layers (~20 TFLOPs/token)
- SOLAR: ~48 layers (~30 TFLOPs/token)
- Beam search (4 beams) Ã— ê¸´ forward pass = ê·¹ì‹¬í•œ ì†ë„ ì €í•˜

**êµí›ˆ**:
- ëª¨ë¸ ì•„í‚¤í…ì²˜ê°€ ì¶”ë¡  ì†ë„ì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤
- íŒŒë¼ë¯¸í„° ìˆ˜ë§Œìœ¼ë¡œ ì†ë„ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤
- Zero-shot ìŠ¤í¬ë¦¬ë‹ì€ ì‹¤ì œ ì¶”ë¡  í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤

**í•´ê²°**: SOLARì„ Qwen3-4B-Instruct-2507ë¡œ êµì²´ (ì •ìƒ ì†ë„ ë³µêµ¬)

### 3. ì´ˆê¸° ìŠ¤í¬ë¦¬ë‹ì˜ 10ê°€ì§€ ë¬¸ì œì 

ì²« ìŠ¤í¬ë¦¬ë‹ì—ì„œ ROUGE Sumì´ 1-2ì ëŒ€ë¡œ ë¹„ì •ìƒì ìœ¼ë¡œ ë‚®ê²Œ ë‚˜ì™”ìŠµë‹ˆë‹¤.

**ë°œê²¬í•œ ë¬¸ì œë“¤** (ìƒì„¸: [SCREENING_ISSUES_ANALYSIS.md](./screening_results/SCREENING_ISSUES_ANALYSIS.md)):

**Critical Issues**:
1. âŒ Chat Template ë¯¸ì ìš© â†’ ëª¨ë¸ì´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë¬´ì‹œ
2. âŒ í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ ë¶€ì¡± â†’ ì˜ì–´/ì¼ë³¸ì–´ í˜¼í•© ì¶œë ¥
3. âŒ Character-level í† í°í™” â†’ í•œêµ­ì–´ ROUGE ë¶€ì •í™•
4. âŒ ì™¸êµ­ì–´ í† í° ì°¨ë‹¨ ë¯¸ì ìš© â†’ Latin/Japanese ì¶œë ¥
5. âŒ QLoRA ë¯¸ì ìš© â†’ OOM ë˜ëŠ” ëŠë¦° ì†ë„
6. âŒ TF32 ë¯¸í™œì„±í™” â†’ RTX 3090 ì„±ëŠ¥ ë¯¸í™œìš©

**ìˆ˜ì • í›„ ì„±ëŠ¥ ê°œì„ **:
- Llama-3.2-Korean-3B: 1.84 â†’ **49.52** (26.9ë°° í–¥ìƒ)
- Llama-3-Korean-8B: 1.18 â†’ **48.61** (41.2ë°° í–¥ìƒ)

**êµí›ˆ**:
- LLMì€ **Chat Templateì´ í•„ìˆ˜**ì…ë‹ˆë‹¤ (íŠ¹íˆ Instruction-tuned ëª¨ë¸)
- í•œêµ­ì–´ í‰ê°€ëŠ” **Mecab í˜•íƒœì†Œ í† í°í™”**ê°€ ì •í™•í•©ë‹ˆë‹¤
- `bad_words_ids`ë¡œ 121,413ê°œ ì™¸êµ­ì–´ í† í° ì°¨ë‹¨ â†’ ìˆœìˆ˜ í•œêµ­ì–´ ì¶œë ¥ ë³´ì¥
- RTX 3090ì—ì„œëŠ” TF32 í™œì„±í™”ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤

### 4. ë””ìŠ¤í¬ ê´€ë¦¬ ì „ëµ

**ë¬¸ì œ**: Qwen3-4B ìŠ¤í¬ë¦¬ë‹ ì‹œì‘ ì‹œ ë””ìŠ¤í¬ ìš©ëŸ‰ ì´ˆê³¼ (91.53GB > 80GB í•œë„)

**ë°œê²¬**:
```bash
HuggingFace ëª¨ë¸ ìºì‹œ: 20GB (SOLAR, Qwen2.5 ë“± ë¶ˆí•„ìš”í•œ ìºì‹œ)
HuggingFace xet ìºì‹œ: 9.3GB
UV archive: 7.8GB
W&B artifacts: 473MB
```

**í•´ê²°**: ë¶ˆí•„ìš”í•œ ìºì‹œ ì •ë¦¬ â†’ 68GBë¡œ ì¶•ì†Œ

**ìºì‹œ ì‚­ì œ ì „ëµ**:
```bash
# ì‚¬ìš© ì™„ë£Œëœ ëª¨ë¸ ìºì‹œ ì‚­ì œ
rm -rf ~/.cache/huggingface/hub/models--<model_name>

# ì „ì²´ xet ìºì‹œ ì‚­ì œ (LFS ê´€ë ¨)
rm -rf ~/.cache/huggingface/xet

# UV íŒ¨í‚¤ì§€ ê´€ë¦¬ì ì•„ì¹´ì´ë¸Œ ì‚­ì œ
rm -rf ~/.cache/uv/archive-v0

# W&B ì˜¤ë˜ëœ artifacts ì‚­ì œ
rm -rf ~/.cache/wandb/artifacts
```

**êµí›ˆ**:
- ì‹¤í—˜ ì¤‘ì—ëŠ” ì •ê¸°ì ìœ¼ë¡œ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ì„ ì²´í¬í•˜ì„¸ìš”
- ê° ëª¨ë¸ ìŠ¤í¬ë¦¬ë‹ í›„ ì¦‰ì‹œ ìºì‹œë¥¼ ì‚­ì œí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤
- `delete_cache=True` ì˜µì…˜ì„ ì½”ë“œì— í†µí•©í•˜ë©´ ìë™í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

### 5. Zero-shot í‰ê°€ì˜ ì¤‘ìš”ì„±

**êµí›ˆ**: íŒŒì¸íŠœë‹ ì „ì— Zero-shot í‰ê°€ë¡œ ëª¨ë¸ì„ ìŠ¤í¬ë¦¬ë‹í•˜ë©´:
- ì‹œê°„ ì ˆì•½: 5ê°œ ëª¨ë¸ Ã— 1ì‹œê°„ í•™ìŠµ = 5ì‹œê°„ vs 5ê°œ Ã— 15ë¶„ ì¶”ë¡  = 75ë¶„
- ì´ˆê¸° ì„±ëŠ¥ íŒŒì•…: ì¢‹ì€ ì¶œë°œì  ì„ íƒ â†’ íŒŒì¸íŠœë‹ íš¨ìœ¨ í–¥ìƒ
- Task Alignment ê²€ì¦: ëª¨ë¸ì´ íƒœìŠ¤í¬ë¥¼ ì´í•´í•˜ëŠ”ì§€ ë¯¸ë¦¬ í™•ì¸

**ë‹¤ìŒ ë‹¨ê³„**: Llama-3.2-Korean-3Bë¥¼ ì„ íƒí•˜ì—¬ íŒŒì¸íŠœë‹ ì§„í–‰ (ì˜ˆìƒ: 1ì‹œê°„)

### 6. ì‹¤íŒ¨í•œ ì‹œë„ë“¤

**ì‹œë„ 1**: AICA-5Bê°€ 5B ëª¨ë¸ì´ë‹ˆ ì¢‹ì„ ê²ƒì´ë¼ ì˜ˆìƒ
- **ê²°ê³¼**: 5ìœ„ (41.99)
- **ì´ìœ **: ëŒ€í™” ìƒì„±ì— íŠ¹í™”ë˜ì–´ ìš”ì•½ ì„±ëŠ¥ ë‚®ìŒ

**ì‹œë„ 2**: SOLAR-10.7Bê°€ 10B ëª¨ë¸ì´ë‹ˆ ê°€ì¥ ì¢‹ì„ ê²ƒì´ë¼ ì˜ˆìƒ
- **ê²°ê³¼**: ì¤‘ë‹¨ (ê·¹ì‹¬í•œ ì†ë„ ì €í•˜)
- **ì´ìœ **: Depth Upscaling ì•„í‚¤í…ì²˜ë¡œ ì¸í•œ ë†’ì€ FLOPs

**ì‹œë„ 3**: Character-level ROUGEë¡œ í•œêµ­ì–´ í‰ê°€
- **ê²°ê³¼**: ë¶€ì •í™•í•œ ì ìˆ˜ (í˜•íƒœì†Œ ë‹¨ìœ„ê°€ ì •í™•í•¨)
- **ìˆ˜ì •**: Mecab í˜•íƒœì†Œ í† í°í™” ì ìš©

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### Q: Java/konlpy ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤
**A**: ì´ í”„ë¡œì íŠ¸ëŠ” Javaë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. `src/evaluation/metrics.py`ê°€ rouge ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

### Q: ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤
**A**:
```bash
# ëª¨ë¸ëª… í™•ì¸
model_name: "digit82/kobart-summarization"

# ë˜ëŠ” ì›ë³¸ ëª¨ë¸
model_name: "gogamza/kobart-base-v2"
```

### Q: ROUGE ì ìˆ˜ê°€ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤
**A**:
- configì—ì„œ `num_train_epochs`ë¥¼ 20ìœ¼ë¡œ ì„¤ì •í–ˆëŠ”ì§€ í™•ì¸
- í•™ìŠµë¥ (`learning_rate: 1e-5`)ì´ ì ì ˆí•œì§€ í™•ì¸
- Early stopping patience(3)ê°€ ë„ˆë¬´ ì‘ì§€ ì•Šì€ì§€ í™•ì¸

## ğŸ“š ì°¸ê³  ìë£Œ

- ì›ë³¸ baseline: `/Competition/NLP/naturallanguageprocessingcompetition-nlp-1/code/baseline.ipynb`
- ëŒ€íšŒ ë¬¸ì„œ: `/Competition/NLP/docs/`
- Claude ê°€ì´ë“œ: `/Competition/NLP/CLAUDE.md`

## ğŸ”„ ë‹¤ìŒ ë‹¨ê³„

- [ ] WandB í†µí•© (ì„ íƒ)
- [ ] Hyperparameter tuning (Optuna)
- [ ] Advanced models (mBART, KoGPT ë“±)
- [ ] Ensemble ì „ëµ

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” ê²½ì§„ëŒ€íšŒ ì œì¶œìš©ì…ë‹ˆë‹¤.

---

**Built with** â¤ï¸ **by Claude Code**
