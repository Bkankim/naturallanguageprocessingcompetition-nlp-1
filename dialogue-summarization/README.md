# ğŸ—¨ï¸ Korean Dialogue Summarization

> baseline.ipynb ê¸°ë°˜ ê¹”ë”í•œ ëª¨ë“ˆí™” í”„ë¡œì íŠ¸

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

í•œêµ­ì–´ ëŒ€í™” ìš”ì•½ ê²½ì§„ëŒ€íšŒë¥¼ ìœ„í•œ **BART ê¸°ë°˜ Seq2Seq ëª¨ë¸**ì…ë‹ˆë‹¤.
baseline.ipynbì˜ ê²€ì¦ëœ ì½”ë“œë¥¼ ëª¨ë“ˆí™”í•˜ì—¬ ì¬ì‚¬ìš©ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.

## ğŸ“Š ìµœì‹  ì‹¤í—˜ ê²°ê³¼

### Zero-shot LLM Screening (2025-10-04)

5ê°œ í•œêµ­ì–´ LLMì„ ëŒ€í™” ìš”ì•½ íƒœìŠ¤í¬ë¡œ ì œë¡œìƒ· í‰ê°€í•œ ê²°ê³¼:

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | ROUGE Sum | ìƒíƒœ |
|------|----------|-----------|------|
| Llama-3.2-Korean-3B | 3.2B | **49.52** | âœ… 1ìœ„ |
| Llama-3-Korean-8B | 8.0B | 48.61 | ğŸ¥ˆ 2ìœ„ |
| Qwen2.5-7B | 7.6B | 46.84 | ğŸ¥‰ 3ìœ„ |
| Qwen3-4B-Instruct | 4.0B | 45.02 | 4ìœ„ |
| Llama-3.2-AICA-5B | 4.3B | 41.99 | 5ìœ„ |

**ë‹¤ìŒ ë‹¨ê³„**: Llama-3.2-Korean-3B íŒŒì¸íŠœë‹ (ì˜ˆìƒ ì†Œìš” ì‹œê°„: ~1ì‹œê°„)

ìì„¸í•œ ê²°ê³¼ëŠ” [EXPERIMENT_LOG.md](./EXPERIMENT_LOG.md) ì°¸ì¡°.

### ì£¼ìš” íŠ¹ì§•

- âœ… **LLM Screening ì™„ë£Œ** - 5ê°œ ëª¨ë¸ Zero-shot í‰ê°€ (Llama-3.2-3B ì„ ì •)
- âœ… **konlpy/Java ì˜ì¡´ì„± ì œê±°** - rouge ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ì‚¬ìš©
- âœ… **ëª¨ë“ˆí™”ëœ êµ¬ì¡°** - ê° ê¸°ëŠ¥ì„ ë…ë¦½ì ì¸ ëª¨ë“ˆë¡œ ë¶„ë¦¬
- âœ… **íƒ€ì… ì•ˆì „ì„±** - ëª¨ë“  í•¨ìˆ˜ì— Type hints ì ìš©
- âœ… **ìƒì„¸í•œ ë¬¸ì„œí™”** - Docstring ë° ê°€ì´ë“œ ì™„ë¹„
- âœ… **baseline.ipynb ê²€ì¦** - ì›ë³¸ ì½”ë“œì˜ ì•ˆì •ì„± ë³´ì¥

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í•™ìŠµ ì‹¤í–‰

```bash
cd /Competition/NLP/dialogue-summarization

python scripts/train_baseline.py --config configs/train_config.yaml
```

### 2. ì¶”ë¡  ì‹¤í–‰

```bash
python scripts/generate_predictions.py \
    --config configs/base_config.yaml \
    --checkpoint checkpoints/baseline_run/final_model
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
dialogue-summarization/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/              # ë°ì´í„° ì „ì²˜ë¦¬
â”‚   â”‚   â”œâ”€â”€ preprocessor.py
â”‚   â”‚   â””â”€â”€ dataset.py
â”‚   â”œâ”€â”€ models/            # ëª¨ë¸ ë¡œë”©
â”‚   â”‚   â””â”€â”€ model_loader.py
â”‚   â”œâ”€â”€ training/          # í•™ìŠµ ê´€ë ¨ (í–¥í›„ í™•ì¥)
â”‚   â”œâ”€â”€ evaluation/        # í‰ê°€ ë©”íŠ¸ë¦­
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â”œâ”€â”€ inference/         # ì¶”ë¡  ê´€ë ¨ (í–¥í›„ í™•ì¥)
â”‚   â””â”€â”€ utils/             # ìœ í‹¸ë¦¬í‹°
â”‚       â””â”€â”€ seed.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_baseline.py           # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ generate_predictions.py     # ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ run_inference.sh            # ì¶”ë¡  ì‰˜ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base_config.yaml            # ê¸°ë³¸ ì„¤ì •
â”‚   â””â”€â”€ train_config.yaml           # í•™ìŠµ ì„¤ì •
â”œâ”€â”€ checkpoints/                    # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
â”œâ”€â”€ logs/                           # ë¡œê·¸ íŒŒì¼
â””â”€â”€ submissions/                    # ì œì¶œ íŒŒì¼
```

## ğŸ”§ ì„¤ì • íŒŒì¼

### train_config.yaml

```yaml
general:
  data_path: "/Competition/NLP/naturallanguageprocessingcompetition-nlp-1/data/"
  model_name: "digit82/kobart-summarization"  # ìš”ì•½ íŠ¹í™” ëª¨ë¸
  output_dir: "./checkpoints/baseline_run"

training:
  num_train_epochs: 20
  per_device_train_batch_size: 50
  per_device_eval_batch_size: 32
  learning_rate: 1.0e-5
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
  # ... ê¸°íƒ€ ì„¤ì •
```

## ğŸ“Š ì„±ëŠ¥

### Dev Set (Validation)
| ëª¨ë¸ | ROUGE-1 | ROUGE-2 | ROUGE-L | ROUGE Sum |
|------|---------|---------|---------|-----------|
| baseline.ipynb | ~16 | ~9 | ~14 | **~47** |
| Modular Structure | 32.28 | 13.46 | 30.03 | **75.77** |

### Test Set (ê²½ì§„ëŒ€íšŒ ì œì¶œ)
| ì‹¤í—˜ | ROUGE-1 | ROUGE-2 | ROUGE-L | Final Score |
|------|---------|---------|---------|-------------|
| **Experiment #1** (Baseline) | **0.5660** | **0.3675** | **0.4719** | **46.8487** |

**ğŸ“ ìƒì„¸ ê¸°ë¡**: [EXPERIMENT_LOG.md](./EXPERIMENT_LOG.md)

## ğŸ› ï¸ ê°œë°œ ê°€ì´ë“œ

### ëª¨ë“ˆ ì‚¬ìš© ì˜ˆì‹œ

```python
# ë°ì´í„° ì „ì²˜ë¦¬
from src.data.preprocessor import Preprocess
from src.data.dataset import DatasetForTrain

preprocessor = Preprocess(bos_token="<s>", eos_token="</s>")
dataset = DatasetForTrain(encoder_input, decoder_input, labels, len=100)

# ëª¨ë¸ ë¡œë”©
from src.models.model_loader import load_tokenizer_and_model

model, tokenizer = load_tokenizer_and_model(
    model_name="digit82/kobart-summarization",
    special_tokens=['#Person1#', '#Person2#']
)

# í‰ê°€ ë©”íŠ¸ë¦­
from src.evaluation.metrics import compute_metrics_for_trainer

compute_metrics = compute_metrics_for_trainer(tokenizer)
```

### ì˜ì¡´ì„±

- Python 3.10+
- PyTorch 2.0+
- Transformers 4.35+
- rouge 1.0.1
- pandas, PyYAML, tqdm

**ì¤‘ìš”**: konlpyë‚˜ JavaëŠ” í•„ìš” ì—†ìŠµë‹ˆë‹¤!

## ğŸ“ ì£¼ìš” ë³€ê²½ì‚¬í•­

### baseline.ipynb ëŒ€ë¹„ ê°œì„ ì 

1. **Java ì˜ì¡´ì„± ì œê±°**
   - konlpy â†’ rouge ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ëŒ€ì²´
   - JVM ì´ˆê¸°í™” ì˜¤ë¥˜ ì™„ì „ í•´ê²°

2. **ëª¨ë“ˆí™”**
   - ë‹¨ì¼ ë…¸íŠ¸ë¶ â†’ ë…ë¦½ì ì¸ ëª¨ë“ˆë“¤
   - ì¬ì‚¬ìš©ì„± ë° í…ŒìŠ¤íŠ¸ ìš©ì´ì„± í–¥ìƒ

3. **íƒ€ì… ì•ˆì „ì„±**
   - ëª¨ë“  í•¨ìˆ˜ì— Type hints ì¶”ê°€
   - IDE ìë™ì™„ì„± ë° íƒ€ì… ì²´í‚¹ ì§€ì›

4. **ë¬¸ì„œí™”**
   - ìƒì„¸í•œ Docstring (Google ìŠ¤íƒ€ì¼)
   - README ë° ì‚¬ìš© ê°€ì´ë“œ ì™„ë¹„

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### Q: Java/konlpy ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤
**A**: ì´ í”„ë¡œì íŠ¸ëŠ” Javaë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. `src/evaluation/metrics.py`ê°€ rouge ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

### Q: ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤
**A**:
```bash
# ëª¨ë¸ëª… í™•ì¸
model_name: "digit82/kobart-summarization"

# ë˜ëŠ” ì›ë³¸ ëª¨ë¸
model_name: "gogamza/kobart-base-v2"
```

### Q: ROUGE ì ìˆ˜ê°€ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤
**A**:
- configì—ì„œ `num_train_epochs`ë¥¼ 20ìœ¼ë¡œ ì„¤ì •í–ˆëŠ”ì§€ í™•ì¸
- í•™ìŠµë¥ (`learning_rate: 1e-5`)ì´ ì ì ˆí•œì§€ í™•ì¸
- Early stopping patience(3)ê°€ ë„ˆë¬´ ì‘ì§€ ì•Šì€ì§€ í™•ì¸

## ğŸ“š ì°¸ê³  ìë£Œ

- ì›ë³¸ baseline: `/Competition/NLP/naturallanguageprocessingcompetition-nlp-1/code/baseline.ipynb`
- ëŒ€íšŒ ë¬¸ì„œ: `/Competition/NLP/docs/`
- Claude ê°€ì´ë“œ: `/Competition/NLP/CLAUDE.md`

## ğŸ”„ ë‹¤ìŒ ë‹¨ê³„

- [ ] WandB í†µí•© (ì„ íƒ)
- [ ] Hyperparameter tuning (Optuna)
- [ ] Advanced models (mBART, KoGPT ë“±)
- [ ] Ensemble ì „ëµ

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” ê²½ì§„ëŒ€íšŒ ì œì¶œìš©ì…ë‹ˆë‹¤.

---

**Built with** â¤ï¸ **by Claude Code**
