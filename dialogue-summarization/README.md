# ğŸ—¨ï¸ Korean Dialogue Summarization

> baseline.ipynb ê¸°ë°˜ ê¹”ë”í•œ ëª¨ë“ˆí™” í”„ë¡œì íŠ¸

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

í•œêµ­ì–´ ëŒ€í™” ìš”ì•½ ê²½ì§„ëŒ€íšŒë¥¼ ìœ„í•œ **BART ê¸°ë°˜ Seq2Seq ëª¨ë¸**ì…ë‹ˆë‹¤.
baseline.ipynbì˜ ê²€ì¦ëœ ì½”ë“œë¥¼ ëª¨ë“ˆí™”í•˜ì—¬ ì¬ì‚¬ìš©ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.

## ğŸ“Š ìµœì‹  ì‹¤í—˜ ê²°ê³¼

### Zero-shot LLM Screening (2025-10-04)

5ê°œ í•œêµ­ì–´ LLMì„ ëŒ€í™” ìš”ì•½ íƒœìŠ¤í¬ë¡œ ì œë¡œìƒ· í‰ê°€í•œ ê²°ê³¼:

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | ROUGE Sum | ìƒíƒœ |
|------|----------|-----------|------|
| Llama-3.2-Korean-3B | 3.2B | **49.52** | âœ… 1ìœ„ |
| Llama-3-Korean-8B | 8.0B | 48.61 | ğŸ¥ˆ 2ìœ„ |
| Qwen2.5-7B | 7.6B | 46.84 | ğŸ¥‰ 3ìœ„ |
| Qwen3-4B-Instruct | 4.0B | 45.02 | 4ìœ„ |
| Llama-3.2-AICA-5B | 4.3B | 41.99 | 5ìœ„ |

**ë‹¤ìŒ ë‹¨ê³„**: Llama-3.2-Korean-3B íŒŒì¸íŠœë‹ (ì˜ˆìƒ ì†Œìš” ì‹œê°„: ~1ì‹œê°„)

ìì„¸í•œ ê²°ê³¼ëŠ” [EXPERIMENT_LOG.md](./EXPERIMENT_LOG.md) ì°¸ì¡°.

### ì£¼ìš” íŠ¹ì§•

- âœ… **LLM Screening ì™„ë£Œ** - 5ê°œ ëª¨ë¸ Zero-shot í‰ê°€ (Llama-3.2-3B ì„ ì •)
- âœ… **konlpy/Java ì˜ì¡´ì„± ì œê±°** - rouge ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ì‚¬ìš©
- âœ… **ëª¨ë“ˆí™”ëœ êµ¬ì¡°** - ê° ê¸°ëŠ¥ì„ ë…ë¦½ì ì¸ ëª¨ë“ˆë¡œ ë¶„ë¦¬
- âœ… **íƒ€ì… ì•ˆì „ì„±** - ëª¨ë“  í•¨ìˆ˜ì— Type hints ì ìš©
- âœ… **ìƒì„¸í•œ ë¬¸ì„œí™”** - Docstring ë° ê°€ì´ë“œ ì™„ë¹„
- âœ… **baseline.ipynb ê²€ì¦** - ì›ë³¸ ì½”ë“œì˜ ì•ˆì •ì„± ë³´ì¥

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 0. ğŸ““ ë°ëª¨ ë…¸íŠ¸ë¶ (ì¶”ì²œ!)

**LLM Fine-tuning ì™„ë²½ ê°€ì´ë“œ ë…¸íŠ¸ë¶**:
```bash
jupyter notebook llm_finetuning_demo.ipynb
```

ì´ ë…¸íŠ¸ë¶ì—ì„œ ë°°ìš¸ ìˆ˜ ìˆëŠ” ë‚´ìš©:
- âœ… QLoRA ê°œë… ë° êµ¬í˜„ ë°©ë²•
- âœ… Fine-tuned ëª¨ë¸ ë¡œë”© ë° ì¶”ë¡ 
- âœ… Chat Template ì´ìŠˆ ë° í•´ê²°
- âœ… ROUGE ì ìˆ˜ ê³„ì‚°
- âœ… ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ

### 1. í•™ìŠµ ì‹¤í–‰

```bash
cd /Competition/NLP/dialogue-summarization

python scripts/train_baseline.py --config configs/train_config.yaml
```

### 2. ì¶”ë¡  ì‹¤í–‰

```bash
python scripts/generate_predictions.py \
    --config configs/base_config.yaml \
    --checkpoint checkpoints/baseline_run/final_model
```

### 3. LLM íŒŒì¸íŠœë‹ (QLoRA)

6ê°œ ëª¨ë¸ (koBART, koT5, Llama-3.2-3B, Qwen3-4B, Qwen2.5-7B, Llama-3-8B) íŒŒì¸íŠœë‹:

```bash
cd /Competition/NLP/dialogue-summarization

# W&B ë¡œê·¸ì¸ (ì²˜ìŒ í•œ ë²ˆë§Œ)
wandb login

# íŒŒì¸íŠœë‹ ì‹¤í–‰
python scripts/llm_finetuning.py --config configs/finetune_config.yaml
```

**ì£¼ìš” ê¸°ëŠ¥**:
- **QLoRA 4bit ì–‘ìí™”** - RTX 3090 24GBì—ì„œ 8B ëª¨ë¸ í•™ìŠµ ê°€ëŠ¥
- **LoRA r=16** - 7ê°œ linear layers (attention + MLP)
- **W&B ì‹¤ì‹œê°„ ë¡œê¹…** - êµ¬ì¡°í™”ëœ ë©”íŠ¸ë¦­ ì¶”ì 
- **ëª¨ë¸ë³„ ìµœì í™”** - LR, batch size, float type ê°œë³„ ì„¤ì •
- **ìë™ Trainer ì„ íƒ** - Encoder-DecoderëŠ” Seq2SeqTrainer, Causal LMì€ Trainer ì‚¬ìš©

**W&B Run Naming Convention**:
```
{nickname}_ep{epochs}_bs{effective_bs}_lr{lr}_{timestamp}
ì˜ˆ: Llama-3.2-Korean-3B_ep3_bs16_lr2e-4_20250103-143025
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„** (RTX 3090):
- Llama-3.2-Korean-3B: ~1ì‹œê°„
- Llama-3-Korean-8B: ~3ì‹œê°„

ìì„¸í•œ ì„¤ì •ì€ `configs/finetune_config.yaml` ì°¸ì¡°.

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
dialogue-summarization/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/              # ë°ì´í„° ì „ì²˜ë¦¬
â”‚   â”‚   â”œâ”€â”€ preprocessor.py
â”‚   â”‚   â””â”€â”€ dataset.py
â”‚   â”œâ”€â”€ models/            # ëª¨ë¸ ë¡œë”©
â”‚   â”‚   â””â”€â”€ model_loader.py
â”‚   â”œâ”€â”€ training/          # í•™ìŠµ ê´€ë ¨ (í–¥í›„ í™•ì¥)
â”‚   â”œâ”€â”€ evaluation/        # í‰ê°€ ë©”íŠ¸ë¦­
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â”œâ”€â”€ inference/         # ì¶”ë¡  ê´€ë ¨ (í–¥í›„ í™•ì¥)
â”‚   â””â”€â”€ utils/             # ìœ í‹¸ë¦¬í‹°
â”‚       â”œâ”€â”€ seed.py
â”‚       â””â”€â”€ wandb_logger.py         # W&B ë¡œê¹… ì „ìš© ëª¨ë“ˆ
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_baseline.py           # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ generate_predictions.py     # ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ llm_finetuning.py           # LLM íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ model_screening.py          # LLM Zero-shot ìŠ¤í¬ë¦¬ë‹
â”‚   â””â”€â”€ run_inference.sh            # ì¶”ë¡  ì‰˜ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base_config.yaml            # ê¸°ë³¸ ì„¤ì •
â”‚   â”œâ”€â”€ train_config.yaml           # í•™ìŠµ ì„¤ì •
â”‚   â”œâ”€â”€ finetune_config.yaml        # LLM íŒŒì¸íŠœë‹ ì„¤ì • (QLoRA)
â”‚   â””â”€â”€ screening_config.yaml       # LLM ìŠ¤í¬ë¦¬ë‹ ì„¤ì •
â”œâ”€â”€ notebooks/                      # Jupyter Notebooks
â”‚   â”œâ”€â”€ llm_finetuning_demo.ipynb   # ğŸ¯ LLM íŒŒì¸íŠœë‹ ë°ëª¨ (ì¶”ì²œ!)
â”‚   â”œâ”€â”€ train_demo.ipynb            # í•™ìŠµ ë°ëª¨
â”‚   â””â”€â”€ inference_demo.ipynb        # ì¶”ë¡  ë°ëª¨
â”œâ”€â”€ checkpoints/                    # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
â”œâ”€â”€ logs/                           # ë¡œê·¸ íŒŒì¼
â””â”€â”€ submissions/                    # ì œì¶œ íŒŒì¼
```

## ğŸ”§ ì„¤ì • íŒŒì¼

### train_config.yaml

```yaml
general:
  data_path: "/Competition/NLP/naturallanguageprocessingcompetition-nlp-1/data/"
  model_name: "digit82/kobart-summarization"  # ìš”ì•½ íŠ¹í™” ëª¨ë¸
  output_dir: "./checkpoints/baseline_run"

training:
  num_train_epochs: 20
  per_device_train_batch_size: 50
  per_device_eval_batch_size: 32
  learning_rate: 1.0e-5
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
  # ... ê¸°íƒ€ ì„¤ì •
```

## ğŸ“Š ì„±ëŠ¥

### Dev Set (Validation)
| ëª¨ë¸ | ROUGE-1 | ROUGE-2 | ROUGE-L | ROUGE Sum |
|------|---------|---------|---------|-----------|
| baseline.ipynb | ~16 | ~9 | ~14 | **~47** |
| Modular Structure | 32.28 | 13.46 | 30.03 | **75.77** |
| **koBART Fine-tuned** | **56.20** | **24.35** | **13.96** | **94.51** |

**Fine-tuning ìƒì„¸**:
- ëª¨ë¸: `digit82/kobart-summarization` (3 epochs, full fine-tuning)
- í† í° ì²˜ë¦¬: `skip_special_tokens=False` + ìˆ˜ë™ ì •ì œ (Baseline ë°©ì‹)
- í‰ê°€: Mecab í˜•íƒœì†Œ ê¸°ë°˜ ROUGE
- Baseline ëŒ€ë¹„ **+24.7%** í–¥ìƒ (75.77 â†’ 94.51)

### Test Set (ê²½ì§„ëŒ€íšŒ ì œì¶œ)
| ì‹¤í—˜ | ROUGE-1 | ROUGE-2 | ROUGE-L | Final Score |
|------|---------|---------|---------|-------------|
| **Experiment #1** (Baseline) | **0.5660** | **0.3675** | **0.4719** | **46.8487** |

**ğŸ“ ìƒì„¸ ê¸°ë¡**: [EXPERIMENT_LOG.md](./EXPERIMENT_LOG.md)

## ğŸ› ï¸ ê°œë°œ ê°€ì´ë“œ

### ëª¨ë“ˆ ì‚¬ìš© ì˜ˆì‹œ

```python
# ë°ì´í„° ì „ì²˜ë¦¬
from src.data.preprocessor import Preprocess
from src.data.dataset import DatasetForTrain

preprocessor = Preprocess(bos_token="<s>", eos_token="</s>")
dataset = DatasetForTrain(encoder_input, decoder_input, labels, len=100)

# ëª¨ë¸ ë¡œë”©
from src.models.model_loader import load_tokenizer_and_model

model, tokenizer = load_tokenizer_and_model(
    model_name="digit82/kobart-summarization",
    special_tokens=['#Person1#', '#Person2#']
)

# í‰ê°€ ë©”íŠ¸ë¦­
from src.evaluation.metrics import compute_metrics_for_trainer

compute_metrics = compute_metrics_for_trainer(tokenizer)
```

### ì˜ì¡´ì„±

- Python 3.10+
- PyTorch 2.0+
- Transformers 4.35+
- rouge 1.0.1
- pandas, PyYAML, tqdm

**ì¤‘ìš”**: konlpyë‚˜ JavaëŠ” í•„ìš” ì—†ìŠµë‹ˆë‹¤!

## ğŸ“ ì£¼ìš” ë³€ê²½ì‚¬í•­

### baseline.ipynb ëŒ€ë¹„ ê°œì„ ì 

1. **Java ì˜ì¡´ì„± ì œê±°**
   - konlpy â†’ rouge ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ëŒ€ì²´
   - JVM ì´ˆê¸°í™” ì˜¤ë¥˜ ì™„ì „ í•´ê²°

2. **ëª¨ë“ˆí™”**
   - ë‹¨ì¼ ë…¸íŠ¸ë¶ â†’ ë…ë¦½ì ì¸ ëª¨ë“ˆë“¤
   - ì¬ì‚¬ìš©ì„± ë° í…ŒìŠ¤íŠ¸ ìš©ì´ì„± í–¥ìƒ

3. **íƒ€ì… ì•ˆì „ì„±**
   - ëª¨ë“  í•¨ìˆ˜ì— Type hints ì¶”ê°€
   - IDE ìë™ì™„ì„± ë° íƒ€ì… ì²´í‚¹ ì§€ì›

4. **ë¬¸ì„œí™”**
   - ìƒì„¸í•œ Docstring (Google ìŠ¤íƒ€ì¼)
   - README ë° ì‚¬ìš© ê°€ì´ë“œ ì™„ë¹„

## ğŸ’¡ ê°œë°œ ì¸ì‚¬ì´íŠ¸ & ë°°ìš´ ì 

### 1. ëª¨ë¸ í¬ê¸° â‰  ì„±ëŠ¥

**ë¬¸ì œ**: ì²˜ìŒì—ëŠ” "í° ëª¨ë¸ì´ ë¬´ì¡°ê±´ ì¢‹ì„ ê²ƒ"ì´ë¼ê³  ìƒê°í–ˆìŠµë‹ˆë‹¤.

**ë°œê²¬**: Llama-3.2-Korean-3B (3.21B)ê°€ AICA-5B (4.31B)ì™€ Llama-3-8B (8.03B)ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | ROUGE Sum | ë¹„ê³  |
|------|----------|-----------|------|
| Llama-3.2-Korean-3B | 3.21B | **49.52** | ğŸ† |
| Llama-3-Korean-8B | 8.03B | 48.61 | 2.6ë°° í¬ì§€ë§Œ 0.91ì ë§Œ ì°¨ì´ |
| Llama-3.2-AICA-5B | 4.31B | 41.99 | 1.3ë°° í¬ì§€ë§Œ 7.53ì  ë‚®ìŒ |

**êµí›ˆ**:
- **Task Alignmentì´ ëª¨ë¸ í¬ê¸°ë³´ë‹¤ ì¤‘ìš”í•©ë‹ˆë‹¤**
- Instruction-tuned ëª¨ë¸ > Conversation-specialized ëª¨ë¸ (ìš”ì•½ íƒœìŠ¤í¬ì˜ ê²½ìš°)
- AICAëŠ” "ëŒ€í™” ìƒì„±"ì— íŠ¹í™”ë˜ì–´ Zero-shot ìš”ì•½ ì„±ëŠ¥ì´ ë‚®ì•˜ìŠµë‹ˆë‹¤
- íŒŒì¸íŠœë‹ìœ¼ë¡œ ì—­ì „ ê°€ëŠ¥ì„±ì€ ìˆì§€ë§Œ, ì¢‹ì€ ì¶œë°œì ì„ ì„ íƒí•˜ëŠ” ê²Œ íš¨ìœ¨ì ì…ë‹ˆë‹¤

### 2. SOLAR ëª¨ë¸ì˜ ê·¹ì‹¬í•œ ì†ë„ ì €í•˜

**ë¬¸ì œ**: SOLAR-10.7Bê°€ ë‹¤ë¥¸ ëª¨ë¸ ëŒ€ë¹„ **40ë°° ëŠë ¸ìŠµë‹ˆë‹¤** (22ë¶„/ë°°ì¹˜ vs 27-33ì´ˆ/ë°°ì¹˜).

**ê°€ì„¤ ê²€ì¦**:
- âœ… ëª¨ë¸ í¬ê¸° (10.7B vs 7-8B): 1.5ë°° ì°¨ì´ëŠ” 40ë°° ì†ë„ ì €í•˜ë¥¼ ì„¤ëª…í•˜ì§€ ëª»í•¨
- âœ… ì–‘ìí™” ë¬¸ì œ: 4bit NF4 ì •ìƒ ì‘ë™ (15GB GPU, 2GB RAM)
- âœ… FlashAttention: SDPA í™œì„±í™”ë¨
- âœ… ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´: ëª¨ë‘ ë™ì¼
- ğŸš¨ **Root Cause**: **Depth Upscaling ì•„í‚¤í…ì²˜**

**ë°œê²¬**: SOLARì€ ë‘ ê°œì˜ Llama2-7B ëª¨ë¸ì„ ìˆ˜ì§ìœ¼ë¡œ ë³‘í•©í•˜ì—¬ ~48 layersë¥¼ ê°€ì§‘ë‹ˆë‹¤.
- ì¼ë°˜ 10B ëª¨ë¸: ~32 layers (~20 TFLOPs/token)
- SOLAR: ~48 layers (~30 TFLOPs/token)
- Beam search (4 beams) Ã— ê¸´ forward pass = ê·¹ì‹¬í•œ ì†ë„ ì €í•˜

**êµí›ˆ**:
- ëª¨ë¸ ì•„í‚¤í…ì²˜ê°€ ì¶”ë¡  ì†ë„ì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤
- íŒŒë¼ë¯¸í„° ìˆ˜ë§Œìœ¼ë¡œ ì†ë„ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤
- Zero-shot ìŠ¤í¬ë¦¬ë‹ì€ ì‹¤ì œ ì¶”ë¡  í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤

**í•´ê²°**: SOLARì„ Qwen3-4B-Instruct-2507ë¡œ êµì²´ (ì •ìƒ ì†ë„ ë³µêµ¬)

### 3. ì´ˆê¸° ìŠ¤í¬ë¦¬ë‹ì˜ 10ê°€ì§€ ë¬¸ì œì 

ì²« ìŠ¤í¬ë¦¬ë‹ì—ì„œ ROUGE Sumì´ 1-2ì ëŒ€ë¡œ ë¹„ì •ìƒì ìœ¼ë¡œ ë‚®ê²Œ ë‚˜ì™”ìŠµë‹ˆë‹¤.

**ë°œê²¬í•œ ë¬¸ì œë“¤** (ìƒì„¸: [SCREENING_ISSUES_ANALYSIS.md](./screening_results/SCREENING_ISSUES_ANALYSIS.md)):

**Critical Issues**:
1. âŒ Chat Template ë¯¸ì ìš© â†’ ëª¨ë¸ì´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë¬´ì‹œ
2. âŒ í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ ë¶€ì¡± â†’ ì˜ì–´/ì¼ë³¸ì–´ í˜¼í•© ì¶œë ¥
3. âŒ Character-level í† í°í™” â†’ í•œêµ­ì–´ ROUGE ë¶€ì •í™•
4. âŒ ì™¸êµ­ì–´ í† í° ì°¨ë‹¨ ë¯¸ì ìš© â†’ Latin/Japanese ì¶œë ¥
5. âŒ QLoRA ë¯¸ì ìš© â†’ OOM ë˜ëŠ” ëŠë¦° ì†ë„
6. âŒ TF32 ë¯¸í™œì„±í™” â†’ RTX 3090 ì„±ëŠ¥ ë¯¸í™œìš©

**ìˆ˜ì • í›„ ì„±ëŠ¥ ê°œì„ **:
- Llama-3.2-Korean-3B: 1.84 â†’ **49.52** (26.9ë°° í–¥ìƒ)
- Llama-3-Korean-8B: 1.18 â†’ **48.61** (41.2ë°° í–¥ìƒ)

**êµí›ˆ**:
- LLMì€ **Chat Templateì´ í•„ìˆ˜**ì…ë‹ˆë‹¤ (íŠ¹íˆ Instruction-tuned ëª¨ë¸)
- í•œêµ­ì–´ í‰ê°€ëŠ” **Mecab í˜•íƒœì†Œ í† í°í™”**ê°€ ì •í™•í•©ë‹ˆë‹¤
- `bad_words_ids`ë¡œ 121,413ê°œ ì™¸êµ­ì–´ í† í° ì°¨ë‹¨ â†’ ìˆœìˆ˜ í•œêµ­ì–´ ì¶œë ¥ ë³´ì¥
- RTX 3090ì—ì„œëŠ” TF32 í™œì„±í™”ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤

### 4. ë””ìŠ¤í¬ ê´€ë¦¬ ì „ëµ

**ë¬¸ì œ**: Qwen3-4B ìŠ¤í¬ë¦¬ë‹ ì‹œì‘ ì‹œ ë””ìŠ¤í¬ ìš©ëŸ‰ ì´ˆê³¼ (91.53GB > 80GB í•œë„)

**ë°œê²¬**:
```bash
HuggingFace ëª¨ë¸ ìºì‹œ: 20GB (SOLAR, Qwen2.5 ë“± ë¶ˆí•„ìš”í•œ ìºì‹œ)
HuggingFace xet ìºì‹œ: 9.3GB
UV archive: 7.8GB
W&B artifacts: 473MB
```

**í•´ê²°**: ë¶ˆí•„ìš”í•œ ìºì‹œ ì •ë¦¬ â†’ 68GBë¡œ ì¶•ì†Œ

**ìºì‹œ ì‚­ì œ ì „ëµ**:
```bash
# ì‚¬ìš© ì™„ë£Œëœ ëª¨ë¸ ìºì‹œ ì‚­ì œ
rm -rf ~/.cache/huggingface/hub/models--<model_name>

# ì „ì²´ xet ìºì‹œ ì‚­ì œ (LFS ê´€ë ¨)
rm -rf ~/.cache/huggingface/xet

# UV íŒ¨í‚¤ì§€ ê´€ë¦¬ì ì•„ì¹´ì´ë¸Œ ì‚­ì œ
rm -rf ~/.cache/uv/archive-v0

# W&B ì˜¤ë˜ëœ artifacts ì‚­ì œ
rm -rf ~/.cache/wandb/artifacts
```

**êµí›ˆ**:
- ì‹¤í—˜ ì¤‘ì—ëŠ” ì •ê¸°ì ìœ¼ë¡œ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ì„ ì²´í¬í•˜ì„¸ìš”
- ê° ëª¨ë¸ ìŠ¤í¬ë¦¬ë‹ í›„ ì¦‰ì‹œ ìºì‹œë¥¼ ì‚­ì œí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤
- `delete_cache=True` ì˜µì…˜ì„ ì½”ë“œì— í†µí•©í•˜ë©´ ìë™í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

### 5. Zero-shot í‰ê°€ì˜ ì¤‘ìš”ì„±

**êµí›ˆ**: íŒŒì¸íŠœë‹ ì „ì— Zero-shot í‰ê°€ë¡œ ëª¨ë¸ì„ ìŠ¤í¬ë¦¬ë‹í•˜ë©´:
- ì‹œê°„ ì ˆì•½: 5ê°œ ëª¨ë¸ Ã— 1ì‹œê°„ í•™ìŠµ = 5ì‹œê°„ vs 5ê°œ Ã— 15ë¶„ ì¶”ë¡  = 75ë¶„
- ì´ˆê¸° ì„±ëŠ¥ íŒŒì•…: ì¢‹ì€ ì¶œë°œì  ì„ íƒ â†’ íŒŒì¸íŠœë‹ íš¨ìœ¨ í–¥ìƒ
- Task Alignment ê²€ì¦: ëª¨ë¸ì´ íƒœìŠ¤í¬ë¥¼ ì´í•´í•˜ëŠ”ì§€ ë¯¸ë¦¬ í™•ì¸

**ë‹¤ìŒ ë‹¨ê³„**: Llama-3.2-Korean-3Bë¥¼ ì„ íƒí•˜ì—¬ íŒŒì¸íŠœë‹ ì§„í–‰ (ì˜ˆìƒ: 1ì‹œê°„)

### 6. ì‹¤íŒ¨í•œ ì‹œë„ë“¤

**ì‹œë„ 1**: AICA-5Bê°€ 5B ëª¨ë¸ì´ë‹ˆ ì¢‹ì„ ê²ƒì´ë¼ ì˜ˆìƒ
- **ê²°ê³¼**: 5ìœ„ (41.99)
- **ì´ìœ **: ëŒ€í™” ìƒì„±ì— íŠ¹í™”ë˜ì–´ ìš”ì•½ ì„±ëŠ¥ ë‚®ìŒ

**ì‹œë„ 2**: SOLAR-10.7Bê°€ 10B ëª¨ë¸ì´ë‹ˆ ê°€ì¥ ì¢‹ì„ ê²ƒì´ë¼ ì˜ˆìƒ
- **ê²°ê³¼**: ì¤‘ë‹¨ (ê·¹ì‹¬í•œ ì†ë„ ì €í•˜)
- **ì´ìœ **: Depth Upscaling ì•„í‚¤í…ì²˜ë¡œ ì¸í•œ ë†’ì€ FLOPs

**ì‹œë„ 3**: Character-level ROUGEë¡œ í•œêµ­ì–´ í‰ê°€
- **ê²°ê³¼**: ë¶€ì •í™•í•œ ì ìˆ˜ (í˜•íƒœì†Œ ë‹¨ìœ„ê°€ ì •í™•í•¨)
- **ìˆ˜ì •**: Mecab í˜•íƒœì†Œ í† í°í™” ì ìš©

### 7. QLoRA ì„¤ì • ìµœì í™” (ìµœì‹  ì—°êµ¬ ê¸°ë°˜)

**ë¬¸ì œ**: ì´ˆê¸° ì„¤ì •ì´ ë³´ìˆ˜ì ì´ê³  ìµœì‹  ì—°êµ¬ì™€ ë¶ˆì¼ì¹˜

**ê²€ì¦ ë°©ë²•**: QLoRA ë…¼ë¬¸, Lightning AI ì‹¤í—˜, ê³µì‹ ë¬¸ì„œ ì°¸ì¡°

**ì£¼ìš” ë°œê²¬ ë° ìˆ˜ì •**:

| í•­ëª© | ì´ˆê¸° ì„¤ì • | ìµœì¢… ì„¤ì • | ê·¼ê±° |
|------|----------|----------|------|
| Target Modules | Attentionë§Œ (4ê°œ) | **All linear (7ê°œ)** | QLoRA paperÂ¹: "add LoRA on all linear layers" |
| Learning Rate | 3e-4 (ì‘ì€ ëª¨ë¸) | **2e-4** | QLoRA paperÂ¹ + Lightning AIÂ²: í‘œì¤€ê°’ |
| LR Scheduler | cosine | **constant** | QLoRA paperÂ¹: benchmarked best |
| LoRA Dropout | 0.05 (ëª¨ë“  ëª¨ë¸) | **0.1 (â‰¤13B)** | QLoRA paperÂ¹: ëª¨ë¸ í¬ê¸°ë³„ ì°¨ë³„í™” |
| Adam Beta2 | ë¯¸ì„¤ì • | **0.999** | QLoRA paperÂ¹ |
| Max Grad Norm | ë¯¸ì„¤ì • | **0.3** | QLoRA paperÂ¹ |
| Float Type | fp16 (ëª¨ë“  ëª¨ë¸) | **ëª¨ë¸ë³„** | LlamaÂ³: bf16, Qwenâ´: fp16 |

**ì˜ˆìƒ íš¨ê³¼**:
- MLP ë ˆì´ì–´ ì¶”ê°€ â†’ í•™ìŠµ íŒŒë¼ë¯¸í„° 30% â†’ **70%** (2.3ë°°)
- ROUGE Sum í–¥ìƒ: **+3~5ì ** ì˜ˆìƒ
- í•™ìŠµ ì‹œê°„: 1.5ë°° ì¦ê°€ (Trade-off)

**ì°¸ê³  ë¬¸í—Œ**:
1. Dettmers et al. (2023). "QLoRA: Efficient Finetuning of Quantized LLMs". arXiv:2305.14314
2. Lightning AI (2024). "Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments"
3. Bllossom Team (2024). "Llama-3-Korean-Bllossom" - AAAI2024, NAACL2024
4. Qwen Team (2024). "Qwen2.5 Official Documentation"

**êµí›ˆ**:
- ìµœì‹  ì—°êµ¬ ë…¼ë¬¸ì˜ ì‹¤ì œ ì„¤ì •ì„ ë”°ë¥´ëŠ” ê²ƒì´ ì¤‘ìš”
- "ì¼ë°˜ì  ê¶Œì¥"ë³´ë‹¤ **ë…¼ë¬¸ì˜ ì‹¤ì œ ì‹¤í—˜ ì„¤ì •** ì°¸ê³ 
- ëª¨ë¸ë³„ ê³µì‹ ë¬¸ì„œ í™•ì¸ (Llama vs Qwenì˜ float type ì°¨ì´)
- ì„¤ì • ë³€ê²½ ì‹œ í•­ìƒ ê·¼ê±°ì™€ ì¶œì²˜ ë‚¨ê¸°ê¸°

### 8. W&B ë¡œê¹… ëª¨ë“ˆ ì„¤ê³„

**ë¬¸ì œ**: íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸ì— W&B ë¡œê¹… ì½”ë“œë¥¼ ì§ì ‘ ì‘ì„±í•˜ë©´ ì½”ë“œê°€ ë³µì¡í•˜ê³  ì¬ì‚¬ìš©ì´ ì–´ë ¤ì›€

**í•´ê²°**: ì „ìš© W&B ë¡œê¹… ëª¨ë“ˆ (`src/utils/wandb_logger.py`) ìƒì„±

**ëª¨ë“ˆ êµ¬ì¡°**:
```python
from src.utils.wandb_logger import WandBLogger

# ì´ˆê¸°í™”
logger = WandBLogger(config)

# ëª¨ë¸ë³„ Run ìƒì„±
logger.init_run(model_config)

# í•™ìŠµ (ìë™ ë¡œê¹…)
trainer.train()

# Run ì¢…ë£Œ
logger.finish()
```

**ì£¼ìš” ê¸°ëŠ¥**:

1. **êµ¬ì¡°í™”ëœ Run Naming**:
   ```
   {nickname}_ep{epochs}_bs{effective_bs}_lr{lr}_{timestamp}
   ì˜ˆ: Llama-3.2-Korean-3B_ep3_bs16_lr2e-4_20250103-143025
   ```

2. **ìë™ íƒœê·¸ ìƒì„±**:
   - `model-type:{encoder_decoder|causal_lm}`
   - `training:{full-finetune|qlora-4bit}`
   - `size:{3B|4B|7B|8B|base}`
   - `arch:{llama|qwen|bart|t5}`
   - `float:{bf16|fp16}`

3. **Group ê´€ë¦¬**:
   - Encoder-Decoder: `encoder-decoder-baseline`
   - Decoder-only: `decoder-only-qlora`

4. **Config ë¡œê¹…**:
   - ëª¨ë¸ ì •ë³´, í•™ìŠµ ì„¤ì •, LoRA íŒŒë¼ë¯¸í„° ë“± ìë™ ê¸°ë¡
   - W&B UIì—ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¹„êµ ê°€ëŠ¥

**ì¥ì **:
- **ì¬ì‚¬ìš©ì„±**: ë‹¤ë¥¸ ìŠ¤í¬ë¦½íŠ¸(ì¶”ë¡ , í‰ê°€)ì—ì„œë„ ë™ì¼í•œ ë¡œê¹… êµ¬ì¡° ì‚¬ìš©
- **ìœ ì§€ë³´ìˆ˜**: W&B ê´€ë ¨ ì½”ë“œê°€ í•œ ê³³ì— ì§‘ì¤‘
- **ì¼ê´€ì„±**: ëª¨ë“  ì‹¤í—˜ì—ì„œ ë™ì¼í•œ ë„¤ì´ë°/íƒœê¹… ê·œì¹™ ì ìš©
- **í…ŒìŠ¤íŠ¸ ìš©ì´**: ë¡œê¹… ë¡œì§ì„ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥

**ì˜ˆì œ ì‚¬ìš©**:
```python
# íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ
wandb_logger = WandBLogger(config)

for model_config in models:
    # ëª¨ë¸ë³„ Run ìƒì„±
    wandb_logger.init_run(model_config)

    # í•™ìŠµ
    trainer = train_model(model, tokenizer, ...)
    trainer.train()  # W&B ìë™ ë¡œê¹…

    # Run ì¢…ë£Œ
    wandb_logger.finish()
```

**ì°¸ê³ **: `src/utils/wandb_logger.py` ì½”ë“œ ì°¸ì¡°

### 9. Seq2SeqTrainer vs Trainer í˜¸í™˜ì„±

**ë¬¸ì œ**: ì²˜ìŒì—ëŠ” ëª¨ë“  ëª¨ë¸ì— `Seq2SeqTrainer`ë¥¼ ì‚¬ìš©í•˜ë ¤ í–ˆìŒ

**ë°œê²¬**: GitHub Issue ê²€ìƒ‰ ê²°ê³¼, `Seq2SeqTrainer`ëŠ” Causal LM (Decoder-only)ê³¼ í˜¸í™˜ë˜ì§€ ì•ŠìŒ
- Encoder-Decoder: `predict_with_generate=True` ì§€ì› â†’ Seq2SeqTrainer ì‚¬ìš© ê°€ëŠ¥
- Causal LM: Generationì„ ë³„ë„ë¡œ ì²˜ë¦¬í•´ì•¼ í•¨ â†’ Trainer ì‚¬ìš©

**í•´ê²°**: ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ìë™ìœ¼ë¡œ ì˜¬ë°”ë¥¸ Trainer ì„ íƒ
```python
if model_type == "encoder_decoder":
    # Seq2SeqTrainer (generation ì§€ì›)
    args = Seq2SeqTrainingArguments(...)
    trainer = Seq2SeqTrainer(...)

elif model_type == "causal_lm":
    # Trainer (language modeling)
    args = TrainingArguments(...)
    trainer = Trainer(...)
```

**êµí›ˆ**:
- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì œí•œì‚¬í•­ì„ ë¯¸ë¦¬ íŒŒì•…í•˜ëŠ” ê²ƒì´ ì¤‘ìš”
- ì•„í‚¤í…ì²˜ë³„ë¡œ ì ì ˆí•œ ë„êµ¬ ì„ íƒ í•„ìš”
- `llm_finetuning.py`ì— ìë™ ì„ íƒ ë¡œì§ êµ¬í˜„ìœ¼ë¡œ ì‚¬ìš©ì í¸ì˜ì„± í–¥ìƒ

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### Q: Java/konlpy ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤
**A**: ì´ í”„ë¡œì íŠ¸ëŠ” Javaë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. `src/evaluation/metrics.py`ê°€ rouge ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

### Q: ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤
**A**:
```bash
# ëª¨ë¸ëª… í™•ì¸
model_name: "digit82/kobart-summarization"

# ë˜ëŠ” ì›ë³¸ ëª¨ë¸
model_name: "gogamza/kobart-base-v2"
```

### Q: ROUGE ì ìˆ˜ê°€ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤
**A**:
- configì—ì„œ `num_train_epochs`ë¥¼ 20ìœ¼ë¡œ ì„¤ì •í–ˆëŠ”ì§€ í™•ì¸
- í•™ìŠµë¥ (`learning_rate: 1e-5`)ì´ ì ì ˆí•œì§€ í™•ì¸
- Early stopping patience(3)ê°€ ë„ˆë¬´ ì‘ì§€ ì•Šì€ì§€ í™•ì¸

## ğŸ“š ì°¸ê³  ìë£Œ

- ì›ë³¸ baseline: `/Competition/NLP/naturallanguageprocessingcompetition-nlp-1/code/baseline.ipynb`
- ëŒ€íšŒ ë¬¸ì„œ: `/Competition/NLP/docs/`
- Claude ê°€ì´ë“œ: `/Competition/NLP/CLAUDE.md`

## ğŸ”„ ë‹¤ìŒ ë‹¨ê³„

- [ ] WandB í†µí•© (ì„ íƒ)
- [ ] Hyperparameter tuning (Optuna)
- [ ] Advanced models (mBART, KoGPT ë“±)
- [ ] Ensemble ì „ëµ

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” ê²½ì§„ëŒ€íšŒ ì œì¶œìš©ì…ë‹ˆë‹¤.

---

**Built with** â¤ï¸ **by Claude Code**
