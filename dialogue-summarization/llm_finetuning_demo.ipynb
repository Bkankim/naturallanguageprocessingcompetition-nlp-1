{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ğŸ¤– LLM Fine-tuning Demo - ëŒ€í™” ìš”ì•½ ëª¨ë¸ í•™ìŠµ ë° í™œìš© ê°€ì´ë“œ\n",
    "\n",
    "> QLoRA ê¸°ë°˜ í•œêµ­ì–´ LLM íŒŒì¸íŠœë‹ ì™„ë²½ ê°€ì´ë“œ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•©ë‹ˆë‹¤:\n",
    "- âœ… Fine-tuned ëª¨ë¸ ë¡œë”© ë° ì¶”ë¡ \n",
    "- âœ… í•™ìŠµ ì„¤ì • ë° ê²°ê³¼ ë¶„ì„\n",
    "- âœ… QLoRA ì ‘ê·¼ë²• ì„¤ëª…\n",
    "- âœ… Chat Template ì´ìŠˆ ë° í•´ê²°\n",
    "- âœ… ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\n",
    "\n",
    "**í•™ìŠµ ì™„ë£Œ ëª¨ë¸**: koBART-summarization (ROUGE Sum: 94.51)\n",
    "\n",
    "**ì§„í–‰ ì¤‘ ëª¨ë¸**: Llama-3.2-Korean-3B (ì˜ˆìƒ ì™„ë£Œ: 1ì‹œê°„)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toc",
   "metadata": {},
   "source": [
    "## ğŸ“‹ ëª©ì°¨\n",
    "\n",
    "1. [í™˜ê²½ ì„¤ì •](#1-í™˜ê²½-ì„¤ì •)\n",
    "2. [QLoRAë€? - íš¨ìœ¨ì ì¸ LLM íŒŒì¸íŠœë‹](#2-qloraë€)\n",
    "3. [í˜„ì¬ í•™ìŠµ ì„¤ì • í™•ì¸](#3-í˜„ì¬-í•™ìŠµ-ì„¤ì •-í™•ì¸)\n",
    "4. [Fine-tuned ëª¨ë¸ ë¡œë”©](#4-fine-tuned-ëª¨ë¸-ë¡œë”©)\n",
    "5. [ì¶”ë¡  ì‹¤í–‰ - ìš”ì•½ ìƒì„±](#5-ì¶”ë¡ -ì‹¤í–‰)\n",
    "6. [ROUGE ì ìˆ˜ ê³„ì‚°](#6-rouge-ì ìˆ˜-ê³„ì‚°)\n",
    "7. [Chat Template ì´ìŠˆì™€ í•´ê²°](#7-chat-template-ì´ìŠˆì™€-í•´ê²°)\n",
    "8. [ì‹¤í—˜ ê²°ê³¼ ìš”ì•½](#8-ì‹¤í—˜-ê²°ê³¼-ìš”ì•½)\n",
    "9. [ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ](#9-ëª¨ë¸-ì„±ëŠ¥-ë¹„êµ)\n",
    "10. [ë‹¤ìŒ ë‹¨ê³„](#10-ë‹¤ìŒ-ë‹¨ê³„)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-path",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •\n",
    "project_root = '/Competition/NLP/naturallanguageprocessingcompetition-nlp-1/dialogue-summarization'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "os.chdir(project_root)\n",
    "print(f\"âœ… Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ëª¨ë“ˆ\n",
    "from src.evaluation.metrics import calculate_rouge_scores\n",
    "\n",
    "print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
    "print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. QLoRAë€? - íš¨ìœ¨ì ì¸ LLM íŒŒì¸íŠœë‹\n",
    "\n",
    "### ğŸ¯ QLoRAë€?\n",
    "\n",
    "**QLoRA (Quantized Low-Rank Adaptation)**ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.\n",
    "\n",
    "### ì£¼ìš” ê°œë…\n",
    "\n",
    "#### 1. **4bit ì–‘ìí™” (Quantization)**\n",
    "- ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ 32bit â†’ 4bitë¡œ ì••ì¶•\n",
    "- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ **1/8ë¡œ ê°ì†Œ** (8B ëª¨ë¸: 32GB â†’ 4GB)\n",
    "- RTX 3090 24GBì—ì„œ 8B ëª¨ë¸ í•™ìŠµ ê°€ëŠ¥\n",
    "\n",
    "```python\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # NormalFloat4\n",
    "    bnb_4bit_use_double_quant=True  # 2ë‹¨ê³„ ì–‘ìí™”\n",
    ")\n",
    "```\n",
    "\n",
    "#### 2. **LoRA (Low-Rank Adaptation)**\n",
    "- ì „ì²´ ëª¨ë¸ì„ í•™ìŠµí•˜ì§€ ì•Šê³  **ì‘ì€ ì–´ëŒ‘í„°ë§Œ í•™ìŠµ**\n",
    "- í•™ìŠµ íŒŒë¼ë¯¸í„°: ì „ì²´ì˜ **0.75%ë§Œ** ì—…ë°ì´íŠ¸\n",
    "- ì˜ˆ: Llama-3.2-3B (3.2B íŒŒë¼ë¯¸í„°) â†’ **24M íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ**\n",
    "\n",
    "```python\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank (ì €ì°¨ì› ë¶„í•´ ì°¨ì›)\n",
    "    lora_alpha=32,  # ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„°\n",
    "    target_modules=[  # ì ìš©í•  ë ˆì´ì–´\n",
    "        \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",  # Attention\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"  # MLP\n",
    "    ],\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "```\n",
    "\n",
    "#### 3. **ì™œ QLoRAë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€?**\n",
    "\n",
    "| ë°©ì‹ | ë©”ëª¨ë¦¬ | í•™ìŠµ ì‹œê°„ | ì„±ëŠ¥ |\n",
    "|------|--------|----------|------|\n",
    "| Full Fine-tuning | 32GB (8B ëª¨ë¸) | 8ì‹œê°„ | 100% |\n",
    "| QLoRA | 4GB | 1.5ì‹œê°„ | **~98%** |\n",
    "\n",
    "**ì¥ì **:\n",
    "- âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨: 8ë°° ê°ì†Œ\n",
    "- âœ… ì†ë„: 5ë°° ë¹ ë¦„\n",
    "- âœ… ì„±ëŠ¥: Full fine-tuningì˜ 98% ìœ ì§€\n",
    "- âœ… ë””ìŠ¤í¬: ì–´ëŒ‘í„°ë§Œ ì €ì¥ (100MB vs 16GB)\n",
    "\n",
    "**ë‹¨ì **:\n",
    "- âš ï¸ ì–‘ìí™” ì˜¤ë²„í—¤ë“œ: ì²« ë¡œë”© ì‹œê°„ ì¦ê°€\n",
    "- âš ï¸ ë³µì¡ì„±: ì„¤ì •ì´ Full fine-tuningë³´ë‹¤ ë³µì¡\n",
    "\n",
    "### ğŸ“Š ìš°ë¦¬ í”„ë¡œì íŠ¸ì˜ QLoRA ì„¤ì •\n",
    "\n",
    "```yaml\n",
    "# QLoRA 4bit ì–‘ìí™”\n",
    "qlora:\n",
    "  load_in_4bit: true\n",
    "  bnb_4bit_compute_dtype: \"bfloat16\"\n",
    "  bnb_4bit_quant_type: \"nf4\"\n",
    "  bnb_4bit_use_double_quant: true\n",
    "\n",
    "# LoRA ì„¤ì • (QLoRA ë…¼ë¬¸ ê¸°ë°˜)\n",
    "lora:\n",
    "  r: 16\n",
    "  lora_alpha: 32\n",
    "  target_modules: [q_proj, v_proj, k_proj, o_proj, gate_proj, up_proj, down_proj]\n",
    "  lora_dropout: 0.1  # 13B ì´í•˜ ëª¨ë¸\n",
    "\n",
    "# í•™ìŠµ ì„¤ì •\n",
    "training:\n",
    "  learning_rate: 2e-4  # QLoRA í‘œì¤€ê°’\n",
    "  lr_scheduler_type: \"constant\"  # QLoRA ë…¼ë¬¸ ê¶Œì¥\n",
    "  optim: \"paged_adamw_32bit\"\n",
    "  max_grad_norm: 0.3\n",
    "```\n",
    "\n",
    "### ğŸ“š ì°¸ê³  ìë£Œ\n",
    "\n",
    "- [QLoRA ë…¼ë¬¸](https://arxiv.org/abs/2305.14314) - Dettmers et al. (2023)\n",
    "- [Lightning AI: QLoRA ì‹¤í—˜](https://lightning.ai/pages/community/lora-insights/)\n",
    "- [Hugging Face PEFT ë¬¸ì„œ](https://huggingface.co/docs/peft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. í˜„ì¬ í•™ìŠµ ì„¤ì • í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì„¤ì • ë¡œë“œ\n",
    "config_path = 'configs/finetune_config.yaml'\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"ğŸ“‹ Fine-tuning ì„¤ì • ìš”ì•½\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nâœ… QLoRA ì„¤ì •:\")\n",
    "print(f\"   - 4bit ì–‘ìí™”: {config['qlora']['load_in_4bit']}\")\n",
    "print(f\"   - Quant type: {config['qlora']['bnb_4bit_quant_type']}\")\n",
    "print(f\"   - Compute dtype: {config['qlora']['bnb_4bit_compute_dtype']}\")\n",
    "\n",
    "print(f\"\\nâœ… LoRA ì„¤ì •:\")\n",
    "print(f\"   - Rank (r): {config['lora']['r']}\")\n",
    "print(f\"   - Alpha: {config['lora']['lora_alpha']}\")\n",
    "print(f\"   - Target modules: {', '.join(config['lora']['target_modules'])}\")\n",
    "\n",
    "print(f\"\\nâœ… í•™ìŠµ ì„¤ì •:\")\n",
    "print(f\"   - Epochs: {config['training']['num_train_epochs']}\")\n",
    "print(f\"   - Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"   - LR scheduler: {config['training']['lr_scheduler_type']}\")\n",
    "print(f\"   - Optimizer: {config['training']['optim']}\")\n",
    "\n",
    "print(f\"\\nâœ… íŒŒì¸íŠœë‹ ëŒ€ìƒ ëª¨ë¸ ({len(config['models'])}ê°œ):\")\n",
    "for i, model in enumerate(config['models'], 1):\n",
    "    print(f\"   {i}. {model['nickname']} - {model['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. Fine-tuned ëª¨ë¸ ë¡œë”©\n",
    "\n",
    "### 4.1 koBART ëª¨ë¸ (í•™ìŠµ ì™„ë£Œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-kobart",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kobart_model(checkpoint_path: str = \"checkpoints/llm_finetuning/koBART-summarization\"):\n",
    "    \"\"\"\n",
    "    Fine-tuned koBART ëª¨ë¸ ë¡œë”© (Encoder-Decoder)\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ\n",
    "    \n",
    "    Returns:\n",
    "        model, tokenizer\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“¥ koBART ëª¨ë¸ ë¡œë”©: {checkpoint_path}\")\n",
    "    \n",
    "    # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”©\n",
    "    base_model = \"digit82/kobart-summarization\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        base_model,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… koBART ëª¨ë¸ ë¡œë”© ì™„ë£Œ\")\n",
    "    print(f\"   - íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters():,}\")\n",
    "    print(f\"   - Vocab í¬ê¸°: {len(tokenizer):,}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# ëª¨ë¸ ë¡œë”©\n",
    "kobart_model, kobart_tokenizer = load_kobart_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qlora-loading",
   "metadata": {},
   "source": [
    "### 4.2 QLoRA ëª¨ë¸ ë¡œë”© (Llama-3.2-Korean-3B)\n",
    "\n",
    "QLoRA ëª¨ë¸ì€ **ë² ì´ìŠ¤ ëª¨ë¸ + LoRA ì–´ëŒ‘í„°**ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-qlora",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qlora_model(\n",
    "    base_model: str = \"Bllossom/llama-3.2-Korean-Bllossom-3B\",\n",
    "    adapter_path: str = \"checkpoints/llm_finetuning/Llama-3.2-Korean-3B\"\n",
    "):\n",
    "    \"\"\"\n",
    "    QLoRA fine-tuned ëª¨ë¸ ë¡œë”© (ë² ì´ìŠ¤ + ì–´ëŒ‘í„°)\n",
    "    \n",
    "    Args:\n",
    "        base_model: ë² ì´ìŠ¤ ëª¨ë¸ ì´ë¦„\n",
    "        adapter_path: LoRA ì–´ëŒ‘í„° ê²½ë¡œ\n",
    "    \n",
    "    Returns:\n",
    "        model, tokenizer\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“¥ QLoRA ëª¨ë¸ ë¡œë”©\")\n",
    "    print(f\"   - ë² ì´ìŠ¤: {base_model}\")\n",
    "    print(f\"   - ì–´ëŒ‘í„°: {adapter_path}\")\n",
    "    \n",
    "    # 4bit ì–‘ìí™” ì„¤ì •\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    \n",
    "    # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© (4bit ì–‘ìí™”)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # LoRA ì–´ëŒ‘í„° ë¡œë”© (ì²´í¬í¬ì¸íŠ¸ê°€ ìˆëŠ” ê²½ìš°)\n",
    "    if Path(adapter_path).exists():\n",
    "        model = PeftModel.from_pretrained(base, adapter_path)\n",
    "        print(f\"âœ… LoRA ì–´ëŒ‘í„° ë¡œë”© ì™„ë£Œ\")\n",
    "    else:\n",
    "        model = base\n",
    "        print(f\"âš ï¸  ì–´ëŒ‘í„° ì—†ìŒ, ë² ì´ìŠ¤ ëª¨ë¸ë§Œ ì‚¬ìš©\")\n",
    "    \n",
    "    # í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„° í™•ì¸\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"âœ… QLoRA ëª¨ë¸ ë¡œë”© ì™„ë£Œ\")\n",
    "    print(f\"   - ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}\")\n",
    "    print(f\"   - í•™ìŠµ íŒŒë¼ë¯¸í„°: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# ëª¨ë¸ ë¡œë”© (ì²´í¬í¬ì¸íŠ¸ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ)\n",
    "qlora_checkpoint = \"checkpoints/llm_finetuning/Llama-3.2-Korean-3B\"\n",
    "if Path(qlora_checkpoint).exists():\n",
    "    llama_model, llama_tokenizer = load_qlora_model()\n",
    "else:\n",
    "    print(f\"âš ï¸  QLoRA ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {qlora_checkpoint}\")\n",
    "    print(f\"   í•™ìŠµì´ ì™„ë£Œë˜ë©´ ì´ ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. ì¶”ë¡  ì‹¤í–‰ - ìš”ì•½ ìƒì„±\n",
    "\n",
    "### 5.1 koBART ì¶”ë¡  (Encoder-Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference-kobart",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_kobart(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dialogue: str,\n",
    "    max_length: int = 100,\n",
    "    num_beams: int = 4\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    koBARTë¡œ ìš”ì•½ ìƒì„± (Encoder-Decoder)\n",
    "    \n",
    "    Args:\n",
    "        model: koBART ëª¨ë¸\n",
    "        tokenizer: í† í¬ë‚˜ì´ì €\n",
    "        dialogue: ëŒ€í™” í…ìŠ¤íŠ¸\n",
    "        max_length: ìµœëŒ€ ìš”ì•½ ê¸¸ì´\n",
    "        num_beams: Beam search í¬ê¸°\n",
    "    \n",
    "    Returns:\n",
    "        ìš”ì•½ í…ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    # ì…ë ¥ ì „ì²˜ë¦¬\n",
    "    input_text = f\"<s>{dialogue}</s>\"\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # ìƒì„±\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "    \n",
    "    # ë””ì½”ë”©\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    # í›„ì²˜ë¦¬ (baseline ë°©ì‹)\n",
    "    summary = summary.replace('<usr>', '').replace('</s>', '').replace('<s>', '')\n",
    "    summary = summary.replace('<pad>', '').replace('<unk>', '').strip()\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ\n",
    "dev_data = pd.read_csv('../data/dev.csv')\n",
    "sample = dev_data.iloc[0]\n",
    "\n",
    "print(\"ğŸ” ìƒ˜í”Œ ëŒ€í™”:\")\n",
    "print(\"=\" * 80)\n",
    "print(sample['dialogue'][:200] + \"...\")\n",
    "print(\"\\nğŸ“ ì‹¤ì œ ìš”ì•½:\")\n",
    "print(sample['summary'])\n",
    "\n",
    "# koBART ì¶”ë¡ \n",
    "kobart_summary = generate_summary_kobart(kobart_model, kobart_tokenizer, sample['dialogue'])\n",
    "print(\"\\nğŸ¤– koBART ìš”ì•½:\")\n",
    "print(kobart_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference-qlora",
   "metadata": {},
   "source": [
    "### 5.2 QLoRA ì¶”ë¡  (Decoder-only with Chat Template)\n",
    "\n",
    "Decoder-only ëª¨ë¸ì€ **Chat Template**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference-llama",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_llama(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dialogue: str,\n",
    "    max_length: int = 100,\n",
    "    num_beams: int = 4\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Llamaë¡œ ìš”ì•½ ìƒì„± (Decoder-only with Chat Template)\n",
    "    \n",
    "    Args:\n",
    "        model: Llama ëª¨ë¸\n",
    "        tokenizer: í† í¬ë‚˜ì´ì €\n",
    "        dialogue: ëŒ€í™” í…ìŠ¤íŠ¸\n",
    "        max_length: ìµœëŒ€ ìš”ì•½ ê¸¸ì´\n",
    "        num_beams: Beam search í¬ê¸°\n",
    "    \n",
    "    Returns:\n",
    "        ìš”ì•½ í…ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    # Chat template ì ìš©\n",
    "    system_prompt = (\n",
    "        \"ë‹¹ì‹ ì€ ëŒ€í™” ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\\n\"\n",
    "        \"- ë°˜ë“œì‹œ í•œêµ­ì–´ë§Œ ì‚¬ìš©í•˜ì„¸ìš” (ì˜ë¬¸/ì¼ë¬¸/ë² íŠ¸ë‚¨ì–´/ì´ëª¨ì§€/URL ê¸ˆì§€).\\n\"\n",
    "        \"- 2~3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.\\n\"\n",
    "        \"- ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´, ì°½ì‘ì€ í•˜ì§€ ë§ˆì„¸ìš”.\"\n",
    "    )\n",
    "    \n",
    "    user_message = f\"ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ì„¸ìš”:\\n---\\n{dialogue}\\n---\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    # Tokenizerì˜ chat template ì‚¬ìš©\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # ì™¸êµ­ì–´ ì°¨ë‹¨ (bad_words_ids)\n",
    "    bad_words_ids = []\n",
    "    for token_id in range(len(tokenizer)):\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "        for ch in token_str:\n",
    "            code = ord(ch)\n",
    "            # ë¼í‹´/íˆë¼ê°€ë‚˜/ê°€íƒ€ì¹´ë‚˜/CJK í•œì\n",
    "            if (0x41 <= code <= 0x5A or 0x61 <= code <= 0x7A or 0x00C0 <= code <= 0x024F or\n",
    "                0x3040 <= code <= 0x30FF or 0x4E00 <= code <= 0x9FFF):\n",
    "                bad_words_ids.append([token_id])\n",
    "                break\n",
    "    \n",
    "    # ìƒì„±\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2,\n",
    "            bad_words_ids=bad_words_ids[:10000]  # ì¼ë¶€ë§Œ ì‚¬ìš© (ì†ë„)\n",
    "        )\n",
    "    \n",
    "    # ë””ì½”ë”© (ì…ë ¥ ì œì™¸)\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    summary = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)\n",
    "    \n",
    "    return summary.strip()\n",
    "\n",
    "# Llama ì¶”ë¡  (ì²´í¬í¬ì¸íŠ¸ê°€ ìˆëŠ” ê²½ìš°)\n",
    "if Path(qlora_checkpoint).exists():\n",
    "    llama_summary = generate_summary_llama(llama_model, llama_tokenizer, sample['dialogue'])\n",
    "    print(\"\\nğŸ¦™ Llama-3.2-Korean-3B ìš”ì•½:\")\n",
    "    print(llama_summary)\n",
    "else:\n",
    "    print(\"âš ï¸  Llama ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ë©´ ì´ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. ROUGE ì ìˆ˜ ê³„ì‚°\n",
    "\n",
    "### Mecab í˜•íƒœì†Œ ê¸°ë°˜ ROUGE (ëŒ€íšŒ ê³µì‹ í‰ê°€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rouge-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_for_model(model, tokenizer, data: pd.DataFrame, model_type: str = \"kobart\"):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ì˜ ROUGE ì ìˆ˜ ê³„ì‚°\n",
    "    \n",
    "    Args:\n",
    "        model: ëª¨ë¸\n",
    "        tokenizer: í† í¬ë‚˜ì´ì €\n",
    "        data: í‰ê°€ ë°ì´í„°\n",
    "        model_type: 'kobart' ë˜ëŠ” 'llama'\n",
    "    \n",
    "    Returns:\n",
    "        ROUGE ì ìˆ˜ ë”•ì…”ë„ˆë¦¬\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    print(f\"ğŸ”„ {len(data)}ê°œ ìƒ˜í”Œ ì¶”ë¡  ì¤‘...\")\n",
    "    \n",
    "    for idx, row in data.iterrows():\n",
    "        if model_type == \"kobart\":\n",
    "            pred = generate_summary_kobart(model, tokenizer, row['dialogue'])\n",
    "        else:\n",
    "            pred = generate_summary_llama(model, tokenizer, row['dialogue'])\n",
    "        \n",
    "        predictions.append(pred)\n",
    "        references.append(row['summary'])\n",
    "        \n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"   ì§„í–‰ë¥ : {idx + 1}/{len(data)}\")\n",
    "    \n",
    "    # ROUGE ê³„ì‚° (Mecab)\n",
    "    scores = calculate_rouge_scores(\n",
    "        predictions=predictions,\n",
    "        references=references,\n",
    "        tokenization_mode='mecab'\n",
    "    )\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Dev set ì¼ë¶€ë¡œ í‰ê°€ (ì‹œê°„ ì ˆì•½)\n",
    "eval_data = dev_data.sample(50, random_state=42)\n",
    "\n",
    "print(\"ğŸ“Š koBART ROUGE í‰ê°€ (50 samples)\")\n",
    "print(\"=\" * 80)\n",
    "kobart_scores = calculate_rouge_for_model(kobart_model, kobart_tokenizer, eval_data, model_type=\"kobart\")\n",
    "\n",
    "print(f\"\\nâœ… koBART ROUGE ì ìˆ˜:\")\n",
    "print(f\"   ROUGE-1: {kobart_scores['rouge-1-f1']:.4f}\")\n",
    "print(f\"   ROUGE-2: {kobart_scores['rouge-2-f1']:.4f}\")\n",
    "print(f\"   ROUGE-L: {kobart_scores['rouge-l-f1']:.4f}\")\n",
    "print(f\"   ROUGE Sum: {kobart_scores['rouge-1-f1'] + kobart_scores['rouge-2-f1'] + kobart_scores['rouge-l-f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "## 7. Chat Template ì´ìŠˆì™€ í•´ê²°\n",
    "\n",
    "### ğŸ› ë°œê²¬í•œ ë¬¸ì œ: Chat Template ë¯¸ì ìš©\n",
    "\n",
    "#### ë¬¸ì œ ìƒí™©\n",
    "\n",
    "ì´ˆê¸° LLM ìŠ¤í¬ë¦¬ë‹ì—ì„œ ROUGE Sumì´ 1~2ì ëŒ€ë¡œ **ë¹„ì •ìƒì ìœ¼ë¡œ ë‚®ì•˜ìŠµë‹ˆë‹¤**.\n",
    "\n",
    "```python\n",
    "# âŒ ì˜ëª»ëœ ë°©ì‹ (Chat Template ì—†ìŒ)\n",
    "prompt = f\"ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ì„¸ìš”:\\n{dialogue}\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "```\n",
    "\n",
    "**ê²°ê³¼**: \n",
    "- Llama-3.2-Korean-3B: ROUGE Sum **1.84** (ê·¹ë„ë¡œ ë‚®ìŒ)\n",
    "- ì˜ì–´/ì¼ë³¸ì–´ í˜¼í•© ì¶œë ¥\n",
    "- ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¬´ì‹œ\n",
    "\n",
    "#### ê·¼ë³¸ ì›ì¸\n",
    "\n",
    "**Instruction-tuned ëª¨ë¸ì€ Chat Templateì´ í•„ìˆ˜ì…ë‹ˆë‹¤!**\n",
    "\n",
    "Chat Templateì€ ëª¨ë¸ì´ í•™ìŠµí•œ **íŠ¹ì • ëŒ€í™” í˜•ì‹**ì…ë‹ˆë‹¤:\n",
    "\n",
    "```python\n",
    "# Llama-3 Chat Template\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_message}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{assistant_response}<|eot_id|>\n",
    "```\n",
    "\n",
    "ëª¨ë¸ì€ ì´ í˜•ì‹ìœ¼ë¡œ **ìˆ˜ë°±ë§Œ ëŒ€í™”**ë¥¼ í•™ìŠµí–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í˜•ì‹ì„ ì£¼ë©´:\n",
    "- âŒ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì¸ì‹\n",
    "- âŒ Role êµ¬ë¶„ ì‹¤íŒ¨\n",
    "- âŒ ì˜ë„í•˜ì§€ ì•Šì€ ì¶œë ¥ (ì˜ì–´/ì¼ë³¸ì–´)\n",
    "\n",
    "### âœ… í•´ê²° ë°©ë²•\n",
    "\n",
    "```python\n",
    "# âœ… ì˜¬ë°”ë¥¸ ë°©ì‹ (Chat Template ì ìš©)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "]\n",
    "\n",
    "# Tokenizerì˜ apply_chat_template ì‚¬ìš©\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True  # assistant í—¤ë” ì¶”ê°€\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "```\n",
    "\n",
    "### ğŸ“Š ì„±ëŠ¥ ê°œì„ \n",
    "\n",
    "Chat Template ì ìš© í›„:\n",
    "\n",
    "| ëª¨ë¸ | Before | After | ê°œì„ ìœ¨ |\n",
    "|------|--------|-------|--------|\n",
    "| Llama-3.2-Korean-3B | 1.84 | **49.52** | **26.9ë°°** |\n",
    "| Llama-3-Korean-8B | 1.18 | **48.61** | **41.2ë°°** |\n",
    "| Qwen2.5-7B | 0.61 | **46.84** | **76.8ë°°** |\n",
    "\n",
    "### ğŸ’¡ êµí›ˆ\n",
    "\n",
    "1. **Instruction-tuned ëª¨ë¸ = Chat Template í•„ìˆ˜**\n",
    "2. **`tokenizer.apply_chat_template()` ì‚¬ìš©** (ëª¨ë¸ë³„ ìë™ ì ìš©)\n",
    "3. **ëª¨ë¸ ë¬¸ì„œ í™•ì¸** (Llama vs Qwenì€ í…œí”Œë¦¿ì´ ë‹¤ë¦„)\n",
    "4. **Zero-shot í‰ê°€ ì‹œ ì£¼ì˜** (í…œí”Œë¦¿ ì—†ìœ¼ë©´ ì„±ëŠ¥ 1/40)\n",
    "\n",
    "### ğŸ“š ì°¸ê³ \n",
    "\n",
    "- [Llama-3 Chat Template](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/)\n",
    "- [Qwen Chat Template](https://qwen.readthedocs.io/en/latest/chat_template.html)\n",
    "- [Hugging Face Chat Templates](https://huggingface.co/docs/transformers/chat_templating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {},
   "source": [
    "## 8. ì‹¤í—˜ ê²°ê³¼ ìš”ì•½\n",
    "\n",
    "### 8.1 í•™ìŠµ ì™„ë£Œ ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ë¡œë“œ\n",
    "results_path = 'checkpoints/llm_finetuning/finetuning_results.csv'\n",
    "\n",
    "if Path(results_path).exists():\n",
    "    results = pd.read_csv(results_path)\n",
    "    \n",
    "    print(\"ğŸ“Š Fine-tuning ê²°ê³¼\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"\\n{'Model':<25} {'ROUGE-1':<12} {'ROUGE-2':<12} {'ROUGE-L':<12} {'ROUGE Sum':<12} {'Timestamp':<25}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for _, row in results.iterrows():\n",
    "        rouge_sum = row['rouge_1_f1'] + row['rouge_2_f1'] + row['rouge_l_f1']\n",
    "        print(f\"{row['model']:<25} {row['rouge_1_f1']:<12.4f} {row['rouge_2_f1']:<12.4f} \"\n",
    "              f\"{row['rouge_l_f1']:<12.4f} {rouge_sum:<12.4f} {row['timestamp']:<25}\")\n",
    "    \n",
    "    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸\n",
    "    results['rouge_sum'] = results['rouge_1_f1'] + results['rouge_2_f1'] + results['rouge_l_f1']\n",
    "    best_model = results.loc[results['rouge_sum'].idxmax()]\n",
    "    \n",
    "    print(f\"\\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['model']}\")\n",
    "    print(f\"   ROUGE Sum: {best_model['rouge_sum']:.4f}\")\n",
    "    print(f\"   ROUGE-1: {best_model['rouge_1_f1']:.4f}\")\n",
    "    print(f\"   ROUGE-2: {best_model['rouge_2_f1']:.4f}\")\n",
    "    print(f\"   ROUGE-L: {best_model['rouge_l_f1']:.4f}\")\n",
    "else:\n",
    "    print(f\"âš ï¸  ê²°ê³¼ íŒŒì¼ ì—†ìŒ: {results_path}\")\n",
    "    print(\"   í•™ìŠµì´ ì™„ë£Œë˜ë©´ ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9",
   "metadata": {},
   "source": [
    "## 9. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\n",
    "\n",
    "### 9.1 Baseline vs Fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¹„êµ í…Œì´ë¸” ìƒì„±\n",
    "comparison_data = [\n",
    "    {\n",
    "        'Model': 'baseline.ipynb',\n",
    "        'Architecture': 'koBART',\n",
    "        'Type': 'Encoder-Decoder',\n",
    "        'Fine-tuning': 'Full',\n",
    "        'ROUGE Sum': 75.77,\n",
    "        'ROUGE-1': 32.28,\n",
    "        'ROUGE-2': 13.46,\n",
    "        'ROUGE-L': 30.03\n",
    "    },\n",
    "    {\n",
    "        'Model': 'koBART-summarization',\n",
    "        'Architecture': 'koBART',\n",
    "        'Type': 'Encoder-Decoder',\n",
    "        'Fine-tuning': 'Full (3 epochs)',\n",
    "        'ROUGE Sum': 94.51,\n",
    "        'ROUGE-1': 56.20,\n",
    "        'ROUGE-2': 24.35,\n",
    "        'ROUGE-L': 13.96\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Llama-3.2-Korean-3B (Zero-shot)',\n",
    "        'Architecture': 'Llama-3.2',\n",
    "        'Type': 'Decoder-only',\n",
    "        'Fine-tuning': 'None',\n",
    "        'ROUGE Sum': 49.52,\n",
    "        'ROUGE-1': 24.72,\n",
    "        'ROUGE-2': 3.73,\n",
    "        'ROUGE-L': 21.07\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Llama-3.2-Korean-3B (Fine-tuned)',\n",
    "        'Architecture': 'Llama-3.2',\n",
    "        'Type': 'Decoder-only',\n",
    "        'Fine-tuning': 'QLoRA 4bit (1 epoch)',\n",
    "        'ROUGE Sum': 'í•™ìŠµ ì¤‘...',\n",
    "        'ROUGE-1': '-',\n",
    "        'ROUGE-2': '-',\n",
    "        'ROUGE-L': '-'\n",
    "    }\n",
    "]\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\")\n",
    "print(\"=\" * 120)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nğŸ’¡ ì£¼ìš” ë°œê²¬:\")\n",
    "print(\"   1. koBART Fine-tuning: Baseline ëŒ€ë¹„ +24.7% í–¥ìƒ (75.77 â†’ 94.51)\")\n",
    "print(\"   2. Llama-3.2-3B Zero-shot: 49.52 (QLoRA Fine-tuning ì˜ˆì •)\")\n",
    "print(\"   3. Encoder-Decoder vs Decoder-only: ìš”ì•½ íƒœìŠ¤í¬ëŠ” Encoder-Decoderê°€ ìœ ë¦¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zero-shot-comparison",
   "metadata": {},
   "source": [
    "### 9.2 Zero-shot LLM Screening ê²°ê³¼\n",
    "\n",
    "5ê°œ LLMì˜ Zero-shot ì„±ëŠ¥ (Chat Template + Mecab ROUGE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zero-shot-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_data = [\n",
    "    {'Rank': 'ğŸ¥‡', 'Model': 'Llama-3.2-Korean-3B', 'Params': '3.21B', 'ROUGE Sum': 49.52, 'R1': 24.72, 'R2': 3.73, 'RL': 21.07},\n",
    "    {'Rank': 'ğŸ¥ˆ', 'Model': 'Llama-3-Korean-8B', 'Params': '8.03B', 'ROUGE Sum': 48.61, 'R1': 23.95, 'R2': 4.01, 'RL': 20.65},\n",
    "    {'Rank': 'ğŸ¥‰', 'Model': 'Qwen2.5-7B', 'Params': '7.61B', 'ROUGE Sum': 46.84, 'R1': 23.34, 'R2': 4.05, 'RL': 19.45},\n",
    "    {'Rank': '4', 'Model': 'Qwen3-4B-Instruct', 'Params': '4.02B', 'ROUGE Sum': 45.02, 'R1': 22.60, 'R2': 3.54, 'RL': 18.88},\n",
    "    {'Rank': '5', 'Model': 'Llama-3.2-AICA-5B', 'Params': '4.31B', 'ROUGE Sum': 41.99, 'R1': 21.22, 'R2': 2.91, 'RL': 17.86},\n",
    "]\n",
    "\n",
    "zero_shot_df = pd.DataFrame(zero_shot_data)\n",
    "\n",
    "print(\"ğŸ“Š Zero-shot LLM Screening (Dev Set, 499 samples)\")\n",
    "print(\"=\" * 100)\n",
    "print(zero_shot_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nğŸ’¡ í•µì‹¬ ë°œê²¬:\")\n",
    "print(\"   1. ëª¨ë¸ í¬ê¸° â‰  ì„±ëŠ¥: 3.2B ëª¨ë¸ì´ 8B ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜\")\n",
    "print(\"   2. Task Alignment ì¤‘ìš”: Instruction-tuned > Conversation-specialized\")\n",
    "print(\"   3. Llama-3.2-Korean-3Bê°€ Fine-tuning 1ìˆœìœ„ í›„ë³´\")\n",
    "print(\"\\nâš ï¸  SOLAR-10.7BëŠ” Depth Upscalingìœ¼ë¡œ ì¸í•´ 40ë°° ëŠë ¤ì„œ ì œì™¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-10",
   "metadata": {},
   "source": [
    "## 10. ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### 10.1 í˜„ì¬ ì§„í–‰ ìƒí™©\n",
    "\n",
    "âœ… **ì™„ë£Œ**:\n",
    "- koBART Fine-tuning (ROUGE Sum: 94.51)\n",
    "- 5ê°œ LLM Zero-shot Screening\n",
    "- QLoRA ì„¤ì • ìµœì í™” (QLoRA ë…¼ë¬¸ ê¸°ë°˜)\n",
    "- Chat Template ì ìš© ë° ê²€ì¦\n",
    "- W&B ë¡œê¹… êµ¬ì¡° êµ¬ì¶•\n",
    "\n",
    "ğŸ”„ **ì§„í–‰ ì¤‘**:\n",
    "- Llama-3.2-Korean-3B QLoRA Fine-tuning (ì˜ˆìƒ ì™„ë£Œ: 1ì‹œê°„)\n",
    "- Qwen3-4B-Instruct Fine-tuning (ëŒ€ê¸°)\n",
    "- Qwen2.5-7B Fine-tuning (ëŒ€ê¸°)\n",
    "- Llama-3-Korean-8B Fine-tuning (ëŒ€ê¸°)\n",
    "\n",
    "### 10.2 ì‹¤í—˜ ê³„íš\n",
    "\n",
    "#### Phase 1: QLoRA Fine-tuning ì™„ë£Œ (ì§„í–‰ ì¤‘)\n",
    "- [ ] Llama-3.2-Korean-3B (1ìˆœìœ„, ~1ì‹œê°„)\n",
    "- [ ] Qwen3-4B-Instruct (~1.5ì‹œê°„)\n",
    "- [ ] Qwen2.5-7B (~3ì‹œê°„)\n",
    "- [ ] Llama-3-Korean-8B (~3ì‹œê°„)\n",
    "\n",
    "**ì˜ˆìƒ ê²°ê³¼**: Zero-shot 49.52 â†’ Fine-tuned **70~80** (ëª©í‘œ)\n",
    "\n",
    "#### Phase 2: Test Set ì œì¶œ\n",
    "- [ ] ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë¡œ Test Set ì¶”ë¡ \n",
    "- [ ] ì œì¶œ íŒŒì¼ ìƒì„± (CSV)\n",
    "- [ ] ê²½ì§„ëŒ€íšŒ í”Œë«í¼ ì œì¶œ\n",
    "\n",
    "**ëª©í‘œ ì ìˆ˜**: **> 50.0** (í˜„ì¬ Baseline 46.85)\n",
    "\n",
    "#### Phase 3: ì•™ìƒë¸” (ì„ íƒ)\n",
    "- [ ] koBART + Llama-3.2-3B ì•™ìƒë¸”\n",
    "- [ ] Voting or Weighted Average\n",
    "\n",
    "### 10.3 ì¶”ê°€ ê°œì„  ì•„ì´ë””ì–´\n",
    "\n",
    "1. **Hyperparameter Tuning**\n",
    "   - Learning rate: 1e-4, 2e-4, 3e-4\n",
    "   - LoRA rank: 16, 32, 64\n",
    "   - Epochs: 1, 2, 3\n",
    "\n",
    "2. **Advanced LoRA**\n",
    "   - DoRA (Weight-Decomposed LoRA)\n",
    "   - LoRA+ (differential learning rates)\n",
    "\n",
    "3. **Post-processing**\n",
    "   - Beam search tuning (num_beams, length_penalty)\n",
    "   - Top-k/Top-p sampling\n",
    "\n",
    "### 10.4 í•™ìŠµ ëª¨ë‹ˆí„°ë§\n",
    "\n",
    "**W&B Dashboard**: https://wandb.ai/bkan-ai/dialogue-summarization-finetuning\n",
    "\n",
    "ì‹¤ì‹œê°„ í•™ìŠµ ë¡œê·¸ í™•ì¸:\n",
    "```bash\n",
    "tail -f llm_finetuning.log\n",
    "```\n",
    "\n",
    "### 10.5 ì¬í˜„ ê°€ì´ë“œ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì„ ì²˜ìŒë¶€í„° ì‹¤í–‰í•˜ë ¤ë©´:\n",
    "\n",
    "1. **í™˜ê²½ ì„¤ì •**\n",
    "   ```bash\n",
    "   cd /Competition/NLP/naturallanguageprocessingcompetition-nlp-1/dialogue-summarization\n",
    "   python -m pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "2. **W&B ë¡œê·¸ì¸** (ì„ íƒ)\n",
    "   ```bash\n",
    "   wandb login\n",
    "   ```\n",
    "\n",
    "3. **Fine-tuning ì‹¤í–‰**\n",
    "   ```bash\n",
    "   python scripts/llm_finetuning.py --config configs/finetune_config.yaml\n",
    "   ```\n",
    "\n",
    "4. **ì´ ë…¸íŠ¸ë¶ ì‹¤í–‰**\n",
    "   - ëª¨ë“  ì…€ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰\n",
    "   - Fine-tuned ëª¨ë¸ ë¡œë”© ë° ì¶”ë¡ \n",
    "\n",
    "### ğŸ“š ì°¸ê³  ìë£Œ\n",
    "\n",
    "- [QLoRA ë…¼ë¬¸](https://arxiv.org/abs/2305.14314)\n",
    "- [Llama-3-Korean-Bllossom](https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B)\n",
    "- [Qwen2.5 Documentation](https://qwen.readthedocs.io/)\n",
    "- [PEFT (LoRA) ë¬¸ì„œ](https://huggingface.co/docs/peft)\n",
    "- [í”„ë¡œì íŠ¸ README](./README.md)\n",
    "- [ì‹¤í—˜ ë¡œê·¸](./EXPERIMENT_LOG.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "footer",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ ìš”ì•½\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ”:\n",
    "\n",
    "1. âœ… **QLoRA ê°œë… ì´í•´**: 4bit ì–‘ìí™” + LoRAë¡œ íš¨ìœ¨ì ì¸ LLM í•™ìŠµ\n",
    "2. âœ… **Fine-tuned ëª¨ë¸ ë¡œë”©**: koBART (ì™„ë£Œ), Llama-3.2-3B (ì§„í–‰ ì¤‘)\n",
    "3. âœ… **ì¶”ë¡  ì‹¤í–‰**: Encoder-Decoder vs Decoder-only ë°©ì‹\n",
    "4. âœ… **ROUGE ê³„ì‚°**: Mecab í˜•íƒœì†Œ ê¸°ë°˜ í‰ê°€\n",
    "5. âœ… **Chat Template ì´ìŠˆ í•´ê²°**: Instruction-tuned ëª¨ë¸ í•„ìˆ˜ ìš”ì†Œ\n",
    "6. âœ… **ì‹¤í—˜ ê²°ê³¼ ë¶„ì„**: koBART ROUGE Sum 94.51 ë‹¬ì„±\n",
    "7. âœ… **ëª¨ë¸ ë¹„êµ**: Zero-shot vs Fine-tuned ì„±ëŠ¥\n",
    "\n",
    "**ë‹¤ìŒ ëª©í‘œ**: Llama-3.2-Korean-3B Fine-tuning ì™„ë£Œ í›„ Test Set ì œì¶œ\n",
    "\n",
    "---\n",
    "\n",
    "**Built with â¤ï¸ by Claude Code**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
