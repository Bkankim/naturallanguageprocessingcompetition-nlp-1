{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# 🤖 LLM Fine-tuning Demo - 대화 요약 모델 학습 및 활용 가이드\n",
    "\n",
    "> QLoRA 기반 한국어 LLM 파인튜닝 완벽 가이드\n",
    "\n",
    "이 노트북은 다음 내용을 포함합니다:\n",
    "- ✅ Fine-tuned 모델 로딩 및 추론\n",
    "- ✅ 학습 설정 및 결과 분석\n",
    "- ✅ QLoRA 접근법 설명\n",
    "- ✅ Chat Template 이슈 및 해결\n",
    "- ✅ 모델 성능 비교\n",
    "\n",
    "**학습 완료 모델**: koBART-summarization (ROUGE Sum: 94.51)\n",
    "\n",
    "**진행 중 모델**: Llama-3.2-Korean-3B (예상 완료: 1시간)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toc",
   "metadata": {},
   "source": [
    "## 📋 목차\n",
    "\n",
    "1. [환경 설정](#1-환경-설정)\n",
    "2. [QLoRA란? - 효율적인 LLM 파인튜닝](#2-qlora란)\n",
    "3. [현재 학습 설정 확인](#3-현재-학습-설정-확인)\n",
    "4. [Fine-tuned 모델 로딩](#4-fine-tuned-모델-로딩)\n",
    "5. [추론 실행 - 요약 생성](#5-추론-실행)\n",
    "6. [ROUGE 점수 계산](#6-rouge-점수-계산)\n",
    "7. [Chat Template 이슈와 해결](#7-chat-template-이슈와-해결)\n",
    "8. [실험 결과 요약](#8-실험-결과-요약)\n",
    "9. [모델 성능 비교](#9-모델-성능-비교)\n",
    "10. [다음 단계](#10-다음-단계)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-path",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 프로젝트 루트 설정\n",
    "project_root = '/Competition/NLP/naturallanguageprocessingcompetition-nlp-1/dialogue-summarization'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "os.chdir(project_root)\n",
    "print(f\"✅ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "# 프로젝트 모듈\n",
    "from src.evaluation.metrics import calculate_rouge_scores\n",
    "\n",
    "print(f\"✅ PyTorch version: {torch.__version__}\")\n",
    "print(f\"✅ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. QLoRA란? - 효율적인 LLM 파인튜닝\n",
    "\n",
    "### 🎯 QLoRA란?\n",
    "\n",
    "**QLoRA (Quantized Low-Rank Adaptation)**는 대형 언어 모델을 효율적으로 파인튜닝하는 기법입니다.\n",
    "\n",
    "### 주요 개념\n",
    "\n",
    "#### 1. **4bit 양자화 (Quantization)**\n",
    "- 모델 가중치를 32bit → 4bit로 압축\n",
    "- 메모리 사용량 **1/8로 감소** (8B 모델: 32GB → 4GB)\n",
    "- RTX 3090 24GB에서 8B 모델 학습 가능\n",
    "\n",
    "```python\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # NormalFloat4\n",
    "    bnb_4bit_use_double_quant=True  # 2단계 양자화\n",
    ")\n",
    "```\n",
    "\n",
    "#### 2. **LoRA (Low-Rank Adaptation)**\n",
    "- 전체 모델을 학습하지 않고 **작은 어댑터만 학습**\n",
    "- 학습 파라미터: 전체의 **0.75%만** 업데이트\n",
    "- 예: Llama-3.2-3B (3.2B 파라미터) → **24M 파라미터만 학습**\n",
    "\n",
    "```python\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank (저차원 분해 차원)\n",
    "    lora_alpha=32,  # 스케일링 파라미터\n",
    "    target_modules=[  # 적용할 레이어\n",
    "        \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",  # Attention\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"  # MLP\n",
    "    ],\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "```\n",
    "\n",
    "#### 3. **왜 QLoRA를 사용하는가?**\n",
    "\n",
    "| 방식 | 메모리 | 학습 시간 | 성능 |\n",
    "|------|--------|----------|------|\n",
    "| Full Fine-tuning | 32GB (8B 모델) | 8시간 | 100% |\n",
    "| QLoRA | 4GB | 1.5시간 | **~98%** |\n",
    "\n",
    "**장점**:\n",
    "- ✅ 메모리 효율: 8배 감소\n",
    "- ✅ 속도: 5배 빠름\n",
    "- ✅ 성능: Full fine-tuning의 98% 유지\n",
    "- ✅ 디스크: 어댑터만 저장 (100MB vs 16GB)\n",
    "\n",
    "**단점**:\n",
    "- ⚠️ 양자화 오버헤드: 첫 로딩 시간 증가\n",
    "- ⚠️ 복잡성: 설정이 Full fine-tuning보다 복잡\n",
    "\n",
    "### 📊 우리 프로젝트의 QLoRA 설정\n",
    "\n",
    "```yaml\n",
    "# QLoRA 4bit 양자화\n",
    "qlora:\n",
    "  load_in_4bit: true\n",
    "  bnb_4bit_compute_dtype: \"bfloat16\"\n",
    "  bnb_4bit_quant_type: \"nf4\"\n",
    "  bnb_4bit_use_double_quant: true\n",
    "\n",
    "# LoRA 설정 (QLoRA 논문 기반)\n",
    "lora:\n",
    "  r: 16\n",
    "  lora_alpha: 32\n",
    "  target_modules: [q_proj, v_proj, k_proj, o_proj, gate_proj, up_proj, down_proj]\n",
    "  lora_dropout: 0.1  # 13B 이하 모델\n",
    "\n",
    "# 학습 설정\n",
    "training:\n",
    "  learning_rate: 2e-4  # QLoRA 표준값\n",
    "  lr_scheduler_type: \"constant\"  # QLoRA 논문 권장\n",
    "  optim: \"paged_adamw_32bit\"\n",
    "  max_grad_norm: 0.3\n",
    "```\n",
    "\n",
    "### 📚 참고 자료\n",
    "\n",
    "- [QLoRA 논문](https://arxiv.org/abs/2305.14314) - Dettmers et al. (2023)\n",
    "- [Lightning AI: QLoRA 실험](https://lightning.ai/pages/community/lora-insights/)\n",
    "- [Hugging Face PEFT 문서](https://huggingface.co/docs/peft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. 현재 학습 설정 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 설정 로드\n",
    "config_path = 'configs/finetune_config.yaml'\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"📋 Fine-tuning 설정 요약\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n✅ QLoRA 설정:\")\n",
    "print(f\"   - 4bit 양자화: {config['qlora']['load_in_4bit']}\")\n",
    "print(f\"   - Quant type: {config['qlora']['bnb_4bit_quant_type']}\")\n",
    "print(f\"   - Compute dtype: {config['qlora']['bnb_4bit_compute_dtype']}\")\n",
    "\n",
    "print(f\"\\n✅ LoRA 설정:\")\n",
    "print(f\"   - Rank (r): {config['lora']['r']}\")\n",
    "print(f\"   - Alpha: {config['lora']['lora_alpha']}\")\n",
    "print(f\"   - Target modules: {', '.join(config['lora']['target_modules'])}\")\n",
    "\n",
    "print(f\"\\n✅ 학습 설정:\")\n",
    "print(f\"   - Epochs: {config['training']['num_train_epochs']}\")\n",
    "print(f\"   - Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"   - LR scheduler: {config['training']['lr_scheduler_type']}\")\n",
    "print(f\"   - Optimizer: {config['training']['optim']}\")\n",
    "\n",
    "print(f\"\\n✅ 파인튜닝 대상 모델 ({len(config['models'])}개):\")\n",
    "for i, model in enumerate(config['models'], 1):\n",
    "    print(f\"   {i}. {model['nickname']} - {model['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. Fine-tuned 모델 로딩\n",
    "\n",
    "### 4.1 koBART 모델 (학습 완료)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-kobart",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kobart_model(checkpoint_path: str = \"checkpoints/llm_finetuning/koBART-summarization\"):\n",
    "    \"\"\"\n",
    "    Fine-tuned koBART 모델 로딩 (Encoder-Decoder)\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: 체크포인트 경로\n",
    "    \n",
    "    Returns:\n",
    "        model, tokenizer\n",
    "    \"\"\"\n",
    "    print(f\"📥 koBART 모델 로딩: {checkpoint_path}\")\n",
    "    \n",
    "    # 베이스 모델 로딩\n",
    "    base_model = \"digit82/kobart-summarization\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        base_model,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ koBART 모델 로딩 완료\")\n",
    "    print(f\"   - 파라미터 수: {model.num_parameters():,}\")\n",
    "    print(f\"   - Vocab 크기: {len(tokenizer):,}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# 모델 로딩\n",
    "kobart_model, kobart_tokenizer = load_kobart_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qlora-loading",
   "metadata": {},
   "source": [
    "### 4.2 QLoRA 모델 로딩 (Llama-3.2-Korean-3B)\n",
    "\n",
    "QLoRA 모델은 **베이스 모델 + LoRA 어댑터**로 구성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-qlora",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qlora_model(\n",
    "    base_model: str = \"Bllossom/llama-3.2-Korean-Bllossom-3B\",\n",
    "    adapter_path: str = \"checkpoints/llm_finetuning/Llama-3.2-Korean-3B\"\n",
    "):\n",
    "    \"\"\"\n",
    "    QLoRA fine-tuned 모델 로딩 (베이스 + 어댑터)\n",
    "    \n",
    "    Args:\n",
    "        base_model: 베이스 모델 이름\n",
    "        adapter_path: LoRA 어댑터 경로\n",
    "    \n",
    "    Returns:\n",
    "        model, tokenizer\n",
    "    \"\"\"\n",
    "    print(f\"📥 QLoRA 모델 로딩\")\n",
    "    print(f\"   - 베이스: {base_model}\")\n",
    "    print(f\"   - 어댑터: {adapter_path}\")\n",
    "    \n",
    "    # 4bit 양자화 설정\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    \n",
    "    # 베이스 모델 로딩 (4bit 양자화)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # LoRA 어댑터 로딩 (체크포인트가 있는 경우)\n",
    "    if Path(adapter_path).exists():\n",
    "        model = PeftModel.from_pretrained(base, adapter_path)\n",
    "        print(f\"✅ LoRA 어댑터 로딩 완료\")\n",
    "    else:\n",
    "        model = base\n",
    "        print(f\"⚠️  어댑터 없음, 베이스 모델만 사용\")\n",
    "    \n",
    "    # 학습 가능 파라미터 확인\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"✅ QLoRA 모델 로딩 완료\")\n",
    "    print(f\"   - 전체 파라미터: {total_params:,}\")\n",
    "    print(f\"   - 학습 파라미터: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# 모델 로딩 (체크포인트가 존재하는 경우에만)\n",
    "qlora_checkpoint = \"checkpoints/llm_finetuning/Llama-3.2-Korean-3B\"\n",
    "if Path(qlora_checkpoint).exists():\n",
    "    llama_model, llama_tokenizer = load_qlora_model()\n",
    "else:\n",
    "    print(f\"⚠️  QLoRA 체크포인트 없음: {qlora_checkpoint}\")\n",
    "    print(f\"   학습이 완료되면 이 셀을 다시 실행하세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. 추론 실행 - 요약 생성\n",
    "\n",
    "### 5.1 koBART 추론 (Encoder-Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference-kobart",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_kobart(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dialogue: str,\n",
    "    max_length: int = 100,\n",
    "    num_beams: int = 4\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    koBART로 요약 생성 (Encoder-Decoder)\n",
    "    \n",
    "    Args:\n",
    "        model: koBART 모델\n",
    "        tokenizer: 토크나이저\n",
    "        dialogue: 대화 텍스트\n",
    "        max_length: 최대 요약 길이\n",
    "        num_beams: Beam search 크기\n",
    "    \n",
    "    Returns:\n",
    "        요약 텍스트\n",
    "    \"\"\"\n",
    "    # 입력 전처리\n",
    "    input_text = f\"<s>{dialogue}</s>\"\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # 생성\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "    \n",
    "    # 디코딩\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    # 후처리 (baseline 방식)\n",
    "    summary = summary.replace('<usr>', '').replace('</s>', '').replace('<s>', '')\n",
    "    summary = summary.replace('<pad>', '').replace('<unk>', '').strip()\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# 샘플 데이터 로드\n",
    "dev_data = pd.read_csv('../data/dev.csv')\n",
    "sample = dev_data.iloc[0]\n",
    "\n",
    "print(\"🔍 샘플 대화:\")\n",
    "print(\"=\" * 80)\n",
    "print(sample['dialogue'][:200] + \"...\")\n",
    "print(\"\\n📝 실제 요약:\")\n",
    "print(sample['summary'])\n",
    "\n",
    "# koBART 추론\n",
    "kobart_summary = generate_summary_kobart(kobart_model, kobart_tokenizer, sample['dialogue'])\n",
    "print(\"\\n🤖 koBART 요약:\")\n",
    "print(kobart_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference-qlora",
   "metadata": {},
   "source": [
    "### 5.2 QLoRA 추론 (Decoder-only with Chat Template)\n",
    "\n",
    "Decoder-only 모델은 **Chat Template**을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference-llama",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_llama(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dialogue: str,\n",
    "    max_length: int = 100,\n",
    "    num_beams: int = 4\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Llama로 요약 생성 (Decoder-only with Chat Template)\n",
    "    \n",
    "    Args:\n",
    "        model: Llama 모델\n",
    "        tokenizer: 토크나이저\n",
    "        dialogue: 대화 텍스트\n",
    "        max_length: 최대 요약 길이\n",
    "        num_beams: Beam search 크기\n",
    "    \n",
    "    Returns:\n",
    "        요약 텍스트\n",
    "    \"\"\"\n",
    "    # Chat template 적용\n",
    "    system_prompt = (\n",
    "        \"당신은 대화 요약 전문가입니다.\\n\"\n",
    "        \"- 반드시 한국어만 사용하세요 (영문/일문/베트남어/이모지/URL 금지).\\n\"\n",
    "        \"- 2~3문장으로 간결하게 요약하세요.\\n\"\n",
    "        \"- 불필요한 수식어, 창작은 하지 마세요.\"\n",
    "    )\n",
    "    \n",
    "    user_message = f\"다음 대화를 요약하세요:\\n---\\n{dialogue}\\n---\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    # Tokenizer의 chat template 사용\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # 외국어 차단 (bad_words_ids)\n",
    "    bad_words_ids = []\n",
    "    for token_id in range(len(tokenizer)):\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "        for ch in token_str:\n",
    "            code = ord(ch)\n",
    "            # 라틴/히라가나/가타카나/CJK 한자\n",
    "            if (0x41 <= code <= 0x5A or 0x61 <= code <= 0x7A or 0x00C0 <= code <= 0x024F or\n",
    "                0x3040 <= code <= 0x30FF or 0x4E00 <= code <= 0x9FFF):\n",
    "                bad_words_ids.append([token_id])\n",
    "                break\n",
    "    \n",
    "    # 생성\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2,\n",
    "            bad_words_ids=bad_words_ids[:10000]  # 일부만 사용 (속도)\n",
    "        )\n",
    "    \n",
    "    # 디코딩 (입력 제외)\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    summary = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)\n",
    "    \n",
    "    return summary.strip()\n",
    "\n",
    "# Llama 추론 (체크포인트가 있는 경우)\n",
    "if Path(qlora_checkpoint).exists():\n",
    "    llama_summary = generate_summary_llama(llama_model, llama_tokenizer, sample['dialogue'])\n",
    "    print(\"\\n🦙 Llama-3.2-Korean-3B 요약:\")\n",
    "    print(llama_summary)\n",
    "else:\n",
    "    print(\"⚠️  Llama 모델 학습이 완료되면 이 셀을 실행하세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. ROUGE 점수 계산\n",
    "\n",
    "### Mecab 형태소 기반 ROUGE (대회 공식 평가)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rouge-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_for_model(model, tokenizer, data: pd.DataFrame, model_type: str = \"kobart\"):\n",
    "    \"\"\"\n",
    "    모델의 ROUGE 점수 계산\n",
    "    \n",
    "    Args:\n",
    "        model: 모델\n",
    "        tokenizer: 토크나이저\n",
    "        data: 평가 데이터\n",
    "        model_type: 'kobart' 또는 'llama'\n",
    "    \n",
    "    Returns:\n",
    "        ROUGE 점수 딕셔너리\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    print(f\"🔄 {len(data)}개 샘플 추론 중...\")\n",
    "    \n",
    "    for idx, row in data.iterrows():\n",
    "        if model_type == \"kobart\":\n",
    "            pred = generate_summary_kobart(model, tokenizer, row['dialogue'])\n",
    "        else:\n",
    "            pred = generate_summary_llama(model, tokenizer, row['dialogue'])\n",
    "        \n",
    "        predictions.append(pred)\n",
    "        references.append(row['summary'])\n",
    "        \n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"   진행률: {idx + 1}/{len(data)}\")\n",
    "    \n",
    "    # ROUGE 계산 (Mecab)\n",
    "    scores = calculate_rouge_scores(\n",
    "        predictions=predictions,\n",
    "        references=references,\n",
    "        tokenization_mode='mecab'\n",
    "    )\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Dev set 일부로 평가 (시간 절약)\n",
    "eval_data = dev_data.sample(50, random_state=42)\n",
    "\n",
    "print(\"📊 koBART ROUGE 평가 (50 samples)\")\n",
    "print(\"=\" * 80)\n",
    "kobart_scores = calculate_rouge_for_model(kobart_model, kobart_tokenizer, eval_data, model_type=\"kobart\")\n",
    "\n",
    "print(f\"\\n✅ koBART ROUGE 점수:\")\n",
    "print(f\"   ROUGE-1: {kobart_scores['rouge-1-f1']:.4f}\")\n",
    "print(f\"   ROUGE-2: {kobart_scores['rouge-2-f1']:.4f}\")\n",
    "print(f\"   ROUGE-L: {kobart_scores['rouge-l-f1']:.4f}\")\n",
    "print(f\"   ROUGE Sum: {kobart_scores['rouge-1-f1'] + kobart_scores['rouge-2-f1'] + kobart_scores['rouge-l-f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "## 7. Chat Template 이슈와 해결\n",
    "\n",
    "### 🐛 발견한 문제: Chat Template 미적용\n",
    "\n",
    "#### 문제 상황\n",
    "\n",
    "초기 LLM 스크리닝에서 ROUGE Sum이 1~2점대로 **비정상적으로 낮았습니다**.\n",
    "\n",
    "```python\n",
    "# ❌ 잘못된 방식 (Chat Template 없음)\n",
    "prompt = f\"다음 대화를 요약하세요:\\n{dialogue}\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "```\n",
    "\n",
    "**결과**: \n",
    "- Llama-3.2-Korean-3B: ROUGE Sum **1.84** (극도로 낮음)\n",
    "- 영어/일본어 혼합 출력\n",
    "- 시스템 프롬프트 무시\n",
    "\n",
    "#### 근본 원인\n",
    "\n",
    "**Instruction-tuned 모델은 Chat Template이 필수입니다!**\n",
    "\n",
    "Chat Template은 모델이 학습한 **특정 대화 형식**입니다:\n",
    "\n",
    "```python\n",
    "# Llama-3 Chat Template\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_message}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{assistant_response}<|eot_id|>\n",
    "```\n",
    "\n",
    "모델은 이 형식으로 **수백만 대화**를 학습했습니다. 다른 형식을 주면:\n",
    "- ❌ 시스템 프롬프트를 일반 텍스트로 인식\n",
    "- ❌ Role 구분 실패\n",
    "- ❌ 의도하지 않은 출력 (영어/일본어)\n",
    "\n",
    "### ✅ 해결 방법\n",
    "\n",
    "```python\n",
    "# ✅ 올바른 방식 (Chat Template 적용)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "]\n",
    "\n",
    "# Tokenizer의 apply_chat_template 사용\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True  # assistant 헤더 추가\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "```\n",
    "\n",
    "### 📊 성능 개선\n",
    "\n",
    "Chat Template 적용 후:\n",
    "\n",
    "| 모델 | Before | After | 개선율 |\n",
    "|------|--------|-------|--------|\n",
    "| Llama-3.2-Korean-3B | 1.84 | **49.52** | **26.9배** |\n",
    "| Llama-3-Korean-8B | 1.18 | **48.61** | **41.2배** |\n",
    "| Qwen2.5-7B | 0.61 | **46.84** | **76.8배** |\n",
    "\n",
    "### 💡 교훈\n",
    "\n",
    "1. **Instruction-tuned 모델 = Chat Template 필수**\n",
    "2. **`tokenizer.apply_chat_template()` 사용** (모델별 자동 적용)\n",
    "3. **모델 문서 확인** (Llama vs Qwen은 템플릿이 다름)\n",
    "4. **Zero-shot 평가 시 주의** (템플릿 없으면 성능 1/40)\n",
    "\n",
    "### 📚 참고\n",
    "\n",
    "- [Llama-3 Chat Template](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/)\n",
    "- [Qwen Chat Template](https://qwen.readthedocs.io/en/latest/chat_template.html)\n",
    "- [Hugging Face Chat Templates](https://huggingface.co/docs/transformers/chat_templating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {},
   "source": [
    "## 8. 실험 결과 요약\n",
    "\n",
    "### 8.1 학습 완료 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 로드\n",
    "results_path = 'checkpoints/llm_finetuning/finetuning_results.csv'\n",
    "\n",
    "if Path(results_path).exists():\n",
    "    results = pd.read_csv(results_path)\n",
    "    \n",
    "    print(\"📊 Fine-tuning 결과\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"\\n{'Model':<25} {'ROUGE-1':<12} {'ROUGE-2':<12} {'ROUGE-L':<12} {'ROUGE Sum':<12} {'Timestamp':<25}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for _, row in results.iterrows():\n",
    "        rouge_sum = row['rouge_1_f1'] + row['rouge_2_f1'] + row['rouge_l_f1']\n",
    "        print(f\"{row['model']:<25} {row['rouge_1_f1']:<12.4f} {row['rouge_2_f1']:<12.4f} \"\n",
    "              f\"{row['rouge_l_f1']:<12.4f} {rouge_sum:<12.4f} {row['timestamp']:<25}\")\n",
    "    \n",
    "    # 최고 성능 모델\n",
    "    results['rouge_sum'] = results['rouge_1_f1'] + results['rouge_2_f1'] + results['rouge_l_f1']\n",
    "    best_model = results.loc[results['rouge_sum'].idxmax()]\n",
    "    \n",
    "    print(f\"\\n🏆 최고 성능 모델: {best_model['model']}\")\n",
    "    print(f\"   ROUGE Sum: {best_model['rouge_sum']:.4f}\")\n",
    "    print(f\"   ROUGE-1: {best_model['rouge_1_f1']:.4f}\")\n",
    "    print(f\"   ROUGE-2: {best_model['rouge_2_f1']:.4f}\")\n",
    "    print(f\"   ROUGE-L: {best_model['rouge_l_f1']:.4f}\")\n",
    "else:\n",
    "    print(f\"⚠️  결과 파일 없음: {results_path}\")\n",
    "    print(\"   학습이 완료되면 자동으로 생성됩니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9",
   "metadata": {},
   "source": [
    "## 9. 모델 성능 비교\n",
    "\n",
    "### 9.1 Baseline vs Fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비교 테이블 생성\n",
    "comparison_data = [\n",
    "    {\n",
    "        'Model': 'baseline.ipynb',\n",
    "        'Architecture': 'koBART',\n",
    "        'Type': 'Encoder-Decoder',\n",
    "        'Fine-tuning': 'Full',\n",
    "        'ROUGE Sum': 75.77,\n",
    "        'ROUGE-1': 32.28,\n",
    "        'ROUGE-2': 13.46,\n",
    "        'ROUGE-L': 30.03\n",
    "    },\n",
    "    {\n",
    "        'Model': 'koBART-summarization',\n",
    "        'Architecture': 'koBART',\n",
    "        'Type': 'Encoder-Decoder',\n",
    "        'Fine-tuning': 'Full (3 epochs)',\n",
    "        'ROUGE Sum': 94.51,\n",
    "        'ROUGE-1': 56.20,\n",
    "        'ROUGE-2': 24.35,\n",
    "        'ROUGE-L': 13.96\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Llama-3.2-Korean-3B (Zero-shot)',\n",
    "        'Architecture': 'Llama-3.2',\n",
    "        'Type': 'Decoder-only',\n",
    "        'Fine-tuning': 'None',\n",
    "        'ROUGE Sum': 49.52,\n",
    "        'ROUGE-1': 24.72,\n",
    "        'ROUGE-2': 3.73,\n",
    "        'ROUGE-L': 21.07\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Llama-3.2-Korean-3B (Fine-tuned)',\n",
    "        'Architecture': 'Llama-3.2',\n",
    "        'Type': 'Decoder-only',\n",
    "        'Fine-tuning': 'QLoRA 4bit (1 epoch)',\n",
    "        'ROUGE Sum': '학습 중...',\n",
    "        'ROUGE-1': '-',\n",
    "        'ROUGE-2': '-',\n",
    "        'ROUGE-L': '-'\n",
    "    }\n",
    "]\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"📊 모델 성능 비교\")\n",
    "print(\"=\" * 120)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n💡 주요 발견:\")\n",
    "print(\"   1. koBART Fine-tuning: Baseline 대비 +24.7% 향상 (75.77 → 94.51)\")\n",
    "print(\"   2. Llama-3.2-3B Zero-shot: 49.52 (QLoRA Fine-tuning 예정)\")\n",
    "print(\"   3. Encoder-Decoder vs Decoder-only: 요약 태스크는 Encoder-Decoder가 유리\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zero-shot-comparison",
   "metadata": {},
   "source": [
    "### 9.2 Zero-shot LLM Screening 결과\n",
    "\n",
    "5개 LLM의 Zero-shot 성능 (Chat Template + Mecab ROUGE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zero-shot-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_data = [\n",
    "    {'Rank': '🥇', 'Model': 'Llama-3.2-Korean-3B', 'Params': '3.21B', 'ROUGE Sum': 49.52, 'R1': 24.72, 'R2': 3.73, 'RL': 21.07},\n",
    "    {'Rank': '🥈', 'Model': 'Llama-3-Korean-8B', 'Params': '8.03B', 'ROUGE Sum': 48.61, 'R1': 23.95, 'R2': 4.01, 'RL': 20.65},\n",
    "    {'Rank': '🥉', 'Model': 'Qwen2.5-7B', 'Params': '7.61B', 'ROUGE Sum': 46.84, 'R1': 23.34, 'R2': 4.05, 'RL': 19.45},\n",
    "    {'Rank': '4', 'Model': 'Qwen3-4B-Instruct', 'Params': '4.02B', 'ROUGE Sum': 45.02, 'R1': 22.60, 'R2': 3.54, 'RL': 18.88},\n",
    "    {'Rank': '5', 'Model': 'Llama-3.2-AICA-5B', 'Params': '4.31B', 'ROUGE Sum': 41.99, 'R1': 21.22, 'R2': 2.91, 'RL': 17.86},\n",
    "]\n",
    "\n",
    "zero_shot_df = pd.DataFrame(zero_shot_data)\n",
    "\n",
    "print(\"📊 Zero-shot LLM Screening (Dev Set, 499 samples)\")\n",
    "print(\"=\" * 100)\n",
    "print(zero_shot_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n💡 핵심 발견:\")\n",
    "print(\"   1. 모델 크기 ≠ 성능: 3.2B 모델이 8B 모델보다 우수\")\n",
    "print(\"   2. Task Alignment 중요: Instruction-tuned > Conversation-specialized\")\n",
    "print(\"   3. Llama-3.2-Korean-3B가 Fine-tuning 1순위 후보\")\n",
    "print(\"\\n⚠️  SOLAR-10.7B는 Depth Upscaling으로 인해 40배 느려서 제외\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-10",
   "metadata": {},
   "source": [
    "## 10. 다음 단계\n",
    "\n",
    "### 10.1 현재 진행 상황\n",
    "\n",
    "✅ **완료**:\n",
    "- koBART Fine-tuning (ROUGE Sum: 94.51)\n",
    "- 5개 LLM Zero-shot Screening\n",
    "- QLoRA 설정 최적화 (QLoRA 논문 기반)\n",
    "- Chat Template 적용 및 검증\n",
    "- W&B 로깅 구조 구축\n",
    "\n",
    "🔄 **진행 중**:\n",
    "- Llama-3.2-Korean-3B QLoRA Fine-tuning (예상 완료: 1시간)\n",
    "- Qwen3-4B-Instruct Fine-tuning (대기)\n",
    "- Qwen2.5-7B Fine-tuning (대기)\n",
    "- Llama-3-Korean-8B Fine-tuning (대기)\n",
    "\n",
    "### 10.2 실험 계획\n",
    "\n",
    "#### Phase 1: QLoRA Fine-tuning 완료 (진행 중)\n",
    "- [ ] Llama-3.2-Korean-3B (1순위, ~1시간)\n",
    "- [ ] Qwen3-4B-Instruct (~1.5시간)\n",
    "- [ ] Qwen2.5-7B (~3시간)\n",
    "- [ ] Llama-3-Korean-8B (~3시간)\n",
    "\n",
    "**예상 결과**: Zero-shot 49.52 → Fine-tuned **70~80** (목표)\n",
    "\n",
    "#### Phase 2: Test Set 제출\n",
    "- [ ] 최고 성능 모델로 Test Set 추론\n",
    "- [ ] 제출 파일 생성 (CSV)\n",
    "- [ ] 경진대회 플랫폼 제출\n",
    "\n",
    "**목표 점수**: **> 50.0** (현재 Baseline 46.85)\n",
    "\n",
    "#### Phase 3: 앙상블 (선택)\n",
    "- [ ] koBART + Llama-3.2-3B 앙상블\n",
    "- [ ] Voting or Weighted Average\n",
    "\n",
    "### 10.3 추가 개선 아이디어\n",
    "\n",
    "1. **Hyperparameter Tuning**\n",
    "   - Learning rate: 1e-4, 2e-4, 3e-4\n",
    "   - LoRA rank: 16, 32, 64\n",
    "   - Epochs: 1, 2, 3\n",
    "\n",
    "2. **Advanced LoRA**\n",
    "   - DoRA (Weight-Decomposed LoRA)\n",
    "   - LoRA+ (differential learning rates)\n",
    "\n",
    "3. **Post-processing**\n",
    "   - Beam search tuning (num_beams, length_penalty)\n",
    "   - Top-k/Top-p sampling\n",
    "\n",
    "### 10.4 학습 모니터링\n",
    "\n",
    "**W&B Dashboard**: https://wandb.ai/bkan-ai/dialogue-summarization-finetuning\n",
    "\n",
    "실시간 학습 로그 확인:\n",
    "```bash\n",
    "tail -f llm_finetuning.log\n",
    "```\n",
    "\n",
    "### 10.5 재현 가이드\n",
    "\n",
    "이 노트북을 처음부터 실행하려면:\n",
    "\n",
    "1. **환경 설정**\n",
    "   ```bash\n",
    "   cd /Competition/NLP/naturallanguageprocessingcompetition-nlp-1/dialogue-summarization\n",
    "   python -m pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "2. **W&B 로그인** (선택)\n",
    "   ```bash\n",
    "   wandb login\n",
    "   ```\n",
    "\n",
    "3. **Fine-tuning 실행**\n",
    "   ```bash\n",
    "   python scripts/llm_finetuning.py --config configs/finetune_config.yaml\n",
    "   ```\n",
    "\n",
    "4. **이 노트북 실행**\n",
    "   - 모든 셀을 순차적으로 실행\n",
    "   - Fine-tuned 모델 로딩 및 추론\n",
    "\n",
    "### 📚 참고 자료\n",
    "\n",
    "- [QLoRA 논문](https://arxiv.org/abs/2305.14314)\n",
    "- [Llama-3-Korean-Bllossom](https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B)\n",
    "- [Qwen2.5 Documentation](https://qwen.readthedocs.io/)\n",
    "- [PEFT (LoRA) 문서](https://huggingface.co/docs/peft)\n",
    "- [프로젝트 README](./README.md)\n",
    "- [실험 로그](./EXPERIMENT_LOG.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "footer",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📝 요약\n",
    "\n",
    "이 노트북에서는:\n",
    "\n",
    "1. ✅ **QLoRA 개념 이해**: 4bit 양자화 + LoRA로 효율적인 LLM 학습\n",
    "2. ✅ **Fine-tuned 모델 로딩**: koBART (완료), Llama-3.2-3B (진행 중)\n",
    "3. ✅ **추론 실행**: Encoder-Decoder vs Decoder-only 방식\n",
    "4. ✅ **ROUGE 계산**: Mecab 형태소 기반 평가\n",
    "5. ✅ **Chat Template 이슈 해결**: Instruction-tuned 모델 필수 요소\n",
    "6. ✅ **실험 결과 분석**: koBART ROUGE Sum 94.51 달성\n",
    "7. ✅ **모델 비교**: Zero-shot vs Fine-tuned 성능\n",
    "\n",
    "**다음 목표**: Llama-3.2-Korean-3B Fine-tuning 완료 후 Test Set 제출\n",
    "\n",
    "---\n",
    "\n",
    "**Built with ❤️ by Claude Code**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
