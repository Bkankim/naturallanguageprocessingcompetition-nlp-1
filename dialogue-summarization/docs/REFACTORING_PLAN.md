# ğŸ”„ ë¦¬íŒ©í† ë§ ê³„íš: ë² ì´ìŠ¤ë¼ì¸ í†µí•© ì „ëµ

**ì‘ì„±ì¼**: 2025-10-05
**ëª©ì **: Korean_DCS_2024 ë² ì´ìŠ¤ë¼ì¸ì˜ í•µì‹¬ ìš”ì†Œë¥¼ í˜„ì¬ ì½”ë“œë² ì´ìŠ¤ì— í†µí•©í•˜ì—¬ ì„±ëŠ¥ ë° íš¨ìœ¨ì„± ê°œì„ 

---

## ğŸ“‹ Executive Summary

### í˜„ì¬ ìƒí™©
- **ìš°ë¦¬ ì„±ëŠ¥**: Llama-3.2-Korean-3B LR=1e-4 í•™ìŠµ ì™„ë£Œ, submission ìƒì„±
- **ëª©í‘œ**: koBART 94.51 ROUGE ìˆ˜ì¤€ ë‹¬ì„±
- **ë¬¸ì œì **:
  - LR 5ë°° ê³¼ë‹¤ (1e-4 vs ë² ì´ìŠ¤ë¼ì¸ 2e-5)
  - Grad_norm ë¶ˆì•ˆì • (2.76 spike)
  - Packing ë¯¸ì§€ì› (40-60% í† í° ë‚­ë¹„)
  - System prompt ì—­ì„¤ (ì™¸êµ­ì–´ ê¸ˆì§€ ì–¸ê¸‰ì´ ì˜¤íˆë ¤ íŠ¸ë¦¬ê±°)

### ì „ëµ
**í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼**: ì™„ì „ ì¬ì‘ì„± ëŒ€ì‹  **ì ì§„ì  í†µí•©**ìœ¼ë¡œ ë¦¬ìŠ¤í¬ ìµœì†Œí™”

---

## ğŸ¯ Phase 1: Config ìµœì í™” (ì¦‰ì‹œ ì ìš© ê°€ëŠ¥)

**ì†Œìš” ì‹œê°„**: 30ë¶„
**ë¦¬ìŠ¤í¬**: ë‚®ìŒ
**ì˜ˆìƒ íš¨ê³¼**: ê·¸ë˜ë””ì–¸íŠ¸ ì•ˆì •ì„± í™•ë³´, ì™¸êµ­ì–´ í˜¼ì… 50% ê°ì†Œ

### 1.1 Learning Rate ì¡°ì •

**ë³€ê²½ ì „** (`configs/finetune_config.yaml`):
```yaml
training:
  learning_rate: 1.0e-4  # âŒ 5ë°° ê³¼ë‹¤
```

**ë³€ê²½ í›„**:
```yaml
training:
  learning_rate: 2.0e-5  # âœ… ë² ì´ìŠ¤ë¼ì¸ ê¸°ì¤€
```

**ê·¼ê±°**:
- Korean_DCS_2024: `2e-5` ì‚¬ìš©
- í˜„ì¬ grad_norm spike (2.76) â†’ ë¶ˆì•ˆì •ì„± ì§€í‘œ
- 5ë°° ê°ì†Œë¡œ ìˆ˜ë ´ ì•ˆì •í™”

---

### 1.2 LR Scheduler ë³€ê²½

**ë³€ê²½ ì „**:
```yaml
training:
  lr_scheduler_type: "constant"  # âŒ ìˆ˜ë ´ í’ˆì§ˆ ì €í•˜
```

**ë³€ê²½ í›„**:
```yaml
training:
  lr_scheduler_type: "cosine"  # âœ… í›„ë°˜ ìˆ˜ë ´ ê°œì„ 
```

**ê·¼ê±°**:
- Cosine annealing: ë§ˆì§€ë§‰ ì—í­ì—ì„œ LRì„ 0ì— ê°€ê¹ê²Œ ê°ì†Œ
- Fine-tuningì—ì„œ í‘œì¤€ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥

---

### 1.3 Weight Decay ì¶”ê°€

**ë³€ê²½ ì „**:
```yaml
training:
  weight_decay: 0.0  # âŒ ê³¼ì í•© ìœ„í—˜
```

**ë³€ê²½ í›„**:
```yaml
training:
  weight_decay: 0.1  # âœ… ì •ê·œí™” ê°•í™”
```

---

### 1.4 Effective Batch Size ì¦ê°€

**ë³€ê²½ ì „**:
```yaml
training:
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 4
  # Effective batch = 8 * 4 = 32
```

**ë³€ê²½ í›„**:
```yaml
training:
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 8
  # Effective batch = 8 * 8 = 64 (ë² ì´ìŠ¤ë¼ì¸ê³¼ ë™ì¼)
```

**ê·¼ê±°**:
- ë” í° ë°°ì¹˜ í¬ê¸° â†’ ê·¸ë˜ë””ì–¸íŠ¸ ì¶”ì • ì•ˆì •ì„± í–¥ìƒ
- GPU ë©”ëª¨ë¦¬ í—ˆìš© ë²”ìœ„ ë‚´ (Llama 19.15GB, Qwen 23.04GB)

---

### 1.5 System Prompt ë‹¨ìˆœí™”

**ë³€ê²½ ì „** (âŒ ì—­ì„¤ì  ë¬¸ì œ):
```yaml
system_prompt: |
  ë‹¹ì‹ ì€ ëŒ€í™” ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
  ë‹¤ìŒ ì§€ì¹¨ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì„¸ìš”:

  - ë°˜ë“œì‹œ í•œêµ­ì–´ë§Œ ì‚¬ìš©í•˜ì„¸ìš” (ì˜ë¬¸/ì¼ë¬¸/ë² íŠ¸ë‚¨ì–´/ì´ëª¨ì§€/URL ê¸ˆì§€).  # â† ë¬¸ì œ!
  - ëŒ€í™”ì˜ í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.
  ...
```

**ë³€ê²½ í›„** (âœ… í—ˆìš©ì  ì ‘ê·¼):
```yaml
system_prompt: |
  ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
  ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
```

**ê·¼ê±°**:
- ë² ì´ìŠ¤ë¼ì¸ê³¼ ë™ì¼í•œ ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸
- "ê¸ˆì§€" ì–¸ê¸‰ì´ multilingual token í™œì„±í™” ê°€ëŠ¥ì„± ì œê±°
- ì‘ì—… ì¸ì‹ì€ User promptì˜ [Question]ì—ì„œ ì²˜ë¦¬

---

## ğŸ”§ Phase 2: Prompt êµ¬ì¡°í™” (1-2ì‹œê°„)

**ì†Œìš” ì‹œê°„**: 1-2ì‹œê°„
**ë¦¬ìŠ¤í¬**: ì¤‘ê°„ (ì½”ë“œ ìˆ˜ì • í•„ìš”)
**ì˜ˆìƒ íš¨ê³¼**: ROUGE +5~10

### 2.1 ë² ì´ìŠ¤ë¼ì¸ Prompt Format ì ìš©

**í˜„ì¬ êµ¬ì¡°**:
```
[System]
ë‹¹ì‹ ì€ ëŒ€í™” ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤...

[User]
#Person1#: ì•ˆë…•í•˜ì„¸ìš”.
#Person2#: ë°˜ê°‘ìŠµë‹ˆë‹¤.

ìœ„ ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.
```

**ë² ì´ìŠ¤ë¼ì¸ êµ¬ì¡°**:
```
[System]
ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

[User]
[Conversation]
í™”ìSD2000001: ì €ëŠ” ì—¬í–‰ ë‹¤ë‹ˆëŠ” ê²ƒì„ êµ‰ì¥íˆ ì¢‹ì•„í•˜ëŠ”ë°ìš”...
í™”ìSD2000002: ì € ì—¬í–‰ ë‹¤ë‹ˆëŠ” ê±° ë˜ê²Œ ì¢‹ì•„í•´ì„œ...

[Question]
ìœ„ í•´ì™¸ì—¬í–‰ ì£¼ì œì— ëŒ€í•œ ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.
```

### 2.2 êµ¬í˜„ ì½”ë“œ

**íŒŒì¼**: `scripts/llm_finetuning.py`

**ë³€ê²½ ìœ„ì¹˜**: `prepare_causal_lm_data()` í•¨ìˆ˜ ë‚´ prompt ìƒì„± ë¶€ë¶„

```python
# ê¸°ì¡´ ì½”ë“œ (ë‹¨ìˆœ í¬ë§·)
def format_dialogue_prompt(sample):
    # ë‹¨ìˆœíˆ dialogue í…ìŠ¤íŠ¸ë§Œ ì „ë‹¬
    return sample["dialogue"]

# ìƒˆ ì½”ë“œ (êµ¬ì¡°í™” í¬ë§·)
def format_dialogue_prompt(sample):
    # [Conversation] ì„¹ì…˜ ìƒì„±
    conversation_lines = ["[Conversation]"]

    # í™”ìë³„ ë°œí™” ì¶”ì¶œ
    dialogue = sample["dialogue"]
    turns = re.split(r'(#Person\d+#:)', dialogue)

    for i in range(1, len(turns), 2):
        speaker = turns[i].strip(':').strip()
        utterance = turns[i+1].strip()
        conversation_lines.append(f"{speaker}: {utterance}")

    conversation_text = "\n".join(conversation_lines)

    # [Question] ì„¹ì…˜ ìƒì„±
    # subject_keywordê°€ ìˆìœ¼ë©´ í™œìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ë¬¸êµ¬
    subject = sample.get("subject_keyword", ["ëŒ€í™”"])[0]  # ì²« ë²ˆì§¸ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
    question = f"[Question]\nìœ„ {subject} ì£¼ì œì— ëŒ€í•œ ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”."

    return conversation_text + "\n\n" + question
```

**ì£¼ì˜ì‚¬í•­**:
- ë² ì´ìŠ¤ë¼ì¸ì€ `subject_keyword` í•„ë“œë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ìš°ë¦¬ ë°ì´í„°ì—ëŠ” ì—†ì„ ìˆ˜ ìˆìŒ
- ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ "ëŒ€í™”"ë¡œ ëŒ€ì²´

---

## âš¡ Phase 3: TRL SFTTrainer ì „í™˜ (2-3ì‹œê°„)

**ì†Œìš” ì‹œê°„**: 2-3ì‹œê°„
**ë¦¬ìŠ¤í¬**: ì¤‘ê°„
**ì˜ˆìƒ íš¨ê³¼**: í•™ìŠµ ì‹œê°„ 40-60% ë‹¨ì¶• (packing íš¨ê³¼)

### 3.1 TRL ì„¤ì¹˜

```bash
pip install trl==0.9.4
```

### 3.2 ì½”ë“œ ë³€ê²½

**íŒŒì¼**: `scripts/llm_finetuning.py`

**ë³€ê²½ ì „**:
```python
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=training_config["per_device_train_batch_size"],
    ...
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=dev_dataset,
    data_collator=data_collator,
    callbacks=callbacks,
)
```

**ë³€ê²½ í›„**:
```python
from trl import SFTTrainer, SFTConfig

training_args = SFTConfig(
    # ê¸°ì¡´ TrainingArguments íŒŒë¼ë¯¸í„° ëª¨ë‘ ìœ ì§€
    output_dir=output_dir,
    per_device_train_batch_size=training_config["per_device_train_batch_size"],
    ...

    # TRL ì „ìš© íŒŒë¼ë¯¸í„° ì¶”ê°€
    max_seq_length=tokenizer_config.get("encoder_max_len", 1024) +
                   tokenizer_config.get("decoder_max_len", 200),
    packing=True,  # âœ… í•µì‹¬ ê¸°ëŠ¥!
    dataset_text_field="text",  # ë°ì´í„°ì…‹ì—ì„œ í…ìŠ¤íŠ¸ í•„ë“œëª… (ì¡°ì • í•„ìš”)
)

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,  # âœ… ëª…ì‹œì  ì „ë‹¬
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=dev_dataset,
    # data_collatorëŠ” SFTTrainerê°€ ìë™ ì²˜ë¦¬ (packing ì§€ì›)
    callbacks=callbacks,
)
```

### 3.3 ë°ì´í„°ì…‹ í¬ë§· ì¡°ì •

**ë¬¸ì œ**: SFTTrainerëŠ” `dataset_text_field`ë¡œ raw textë¥¼ ê¸°ëŒ€

**í•´ê²°ì±…**:

**Option A**: ë°ì´í„°ì…‹ í¬ë§· ë³€ê²½
```python
# ê¸°ì¡´: {"input_ids": [...], "labels": [...]}
# ë³€ê²½: {"text": "<full_prompt_with_response>"}

def prepare_sft_dataset(samples):
    texts = []
    for sample in samples:
        # Full conversation text ìƒì„±
        prompt = format_dialogue_prompt(sample)
        response = sample["summary"]

        # Chat template ì ìš©
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt},
            {"role": "assistant", "content": response}
        ]

        # Tokenizerì˜ chat template í™œìš©
        full_text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,  # í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜
            add_generation_prompt=False
        )

        texts.append({"text": full_text})

    return Dataset.from_dict({"text": texts})
```

**Option B**: `formatting_func` ì‚¬ìš©
```python
def formatting_func(example):
    """SFTTrainerì— ì „ë‹¬í•  í¬ë§·íŒ… í•¨ìˆ˜"""
    prompt = format_dialogue_prompt(example)
    response = example["summary"]

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt},
        {"role": "assistant", "content": response}
    ]

    return tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=False
    )

trainer = SFTTrainer(
    ...,
    formatting_func=formatting_func,  # âœ… ì´ ë°©ì‹ì´ ë” ê¹”ë”
)
```

---

## ğŸ“Š Phase 4: ê²€ì¦ ë° ì„±ëŠ¥ ë¹„êµ

### 4.1 A/B í…ŒìŠ¤íŠ¸ êµ¬ì„±

| ì„¤ì • | Phase 1 | Phase 2 | Phase 3 |
|------|---------|---------|---------|
| LR | 2e-5 | 2e-5 | 2e-5 |
| Scheduler | cosine | cosine | cosine |
| Weight Decay | 0.1 | 0.1 | 0.1 |
| Batch Size | 64 | 64 | 64 |
| Prompt | ë‹¨ìˆœ | **êµ¬ì¡°í™”** | êµ¬ì¡°í™” |
| Trainer | Trainer | Trainer | **SFTTrainer** |
| Packing | âŒ | âŒ | **âœ…** |

### 4.2 ì˜ˆìƒ ì„±ëŠ¥

| Metric | Phase 1 | Phase 2 | Phase 3 |
|--------|---------|---------|---------|
| ROUGE Sum | 60-70 | **70-80** | **75-85** |
| í•™ìŠµ ì‹œê°„/epoch | 60ë¶„ | 60ë¶„ | **25-35ë¶„** |
| Grad Norm Spike | <1.5 | <1.5 | <1.5 |
| ì™¸êµ­ì–´ í˜¼ì… | <3% | <1% | <1% |

---

## ğŸ›¡ï¸ ë¦¬ìŠ¤í¬ ê´€ë¦¬

### Phase 1 ì‹¤í–‰ ë¦¬ìŠ¤í¬
- **ë¦¬ìŠ¤í¬**: ê±°ì˜ ì—†ìŒ (configë§Œ ë³€ê²½)
- **ë¡¤ë°±**: config íŒŒì¼ git revert
- **ê²€ì¦**: 1 epoch í•™ìŠµ í›„ grad_norm, loss ì¶”ì´ í™•ì¸

### Phase 2 ì‹¤í–‰ ë¦¬ìŠ¤í¬
- **ë¦¬ìŠ¤í¬**: ì¤‘ê°„ (prompt íŒŒì‹± ë²„ê·¸ ê°€ëŠ¥)
- **ë¡¤ë°±**: prompt ìƒì„± í•¨ìˆ˜ë§Œ ë˜ëŒë¦¬ê¸°
- **ê²€ì¦**: ì†Œìˆ˜ ìƒ˜í”Œë¡œ prompt ìƒì„± í…ŒìŠ¤íŠ¸

### Phase 3 ì‹¤í–‰ ë¦¬ìŠ¤í¬
- **ë¦¬ìŠ¤í¬**: ì¤‘ê°„ (Trainer â†’ SFTTrainer í˜¸í™˜ì„±)
- **ë¡¤ë°±**: ê¸°ì¡´ Trainer ì½”ë“œ ë³´ì¡´ (branch ìƒì„±)
- **ê²€ì¦**:
  - packing=Falseë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸
  - í•™ìŠµ loss ê³¡ì„  ë¹„êµ

---

## ğŸ“… ì‹¤í–‰ ìˆœì„œ

### ìš°ì„ ìˆœìœ„ 1: Phase 1 (ì¦‰ì‹œ)
```bash
# 1. Config ìˆ˜ì •
vim configs/finetune_config.yaml

# 2. 1 epoch í…ŒìŠ¤íŠ¸ í•™ìŠµ
python scripts/llm_finetuning.py --config configs/finetune_config.yaml

# 3. W&Bì—ì„œ grad_norm, loss í™•ì¸
# ì˜ˆìƒ: grad_norm < 1.5, loss ì•ˆì •ì  ê°ì†Œ
```

**íŒë‹¨ ê¸°ì¤€**:
- âœ… grad_norm < 1.5 ìœ ì§€ â†’ Phase 2 ì§„í–‰
- âŒ grad_norm ì—¬ì „íˆ spike â†’ LR ì¶”ê°€ ê°ì†Œ (1e-5)

---

### ìš°ì„ ìˆœìœ„ 2: Phase 2 (Phase 1 ì„±ê³µ ì‹œ)
```bash
# 1. Prompt ìƒì„± í•¨ìˆ˜ ìˆ˜ì •
vim scripts/llm_finetuning.py

# 2. ì†Œìˆ˜ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
python -c "
from scripts.llm_finetuning import format_dialogue_prompt
# ìƒ˜í”Œë¡œ prompt ìƒì„± í™•ì¸
"

# 3. ì „ì²´ í•™ìŠµ
python scripts/llm_finetuning.py --config configs/finetune_config.yaml
```

**íŒë‹¨ ê¸°ì¤€**:
- âœ… ROUGE > 70 â†’ Phase 3 ì§„í–‰
- âŒ ROUGE < 70 â†’ Prompt ë””ë²„ê¹… ë˜ëŠ” Phase 1ë¡œ ì œì¶œ

---

### ìš°ì„ ìˆœìœ„ 3: Phase 3 (Phase 2 ì„±ê³µ ì‹œ)
```bash
# 1. TRL ì„¤ì¹˜
pip install trl==0.9.4

# 2. ì½”ë“œ ìˆ˜ì • (branch ìƒì„±)
git checkout -b feature/sft-trainer
vim scripts/llm_finetuning.py

# 3. Packing ë¹„í™œì„±í™” í…ŒìŠ¤íŠ¸
# packing=Falseë¡œ ë¨¼ì € í•™ìŠµí•˜ì—¬ í˜¸í™˜ì„± í™•ì¸

# 4. Packing í™œì„±í™”
# packing=Trueë¡œ í•™ìŠµ ì†ë„ ê°œì„  í™•ì¸
```

---

## ğŸ¯ ì„±ê³µ ê¸°ì¤€

### Phase 1
- [x] Grad_norm < 1.5 ì•ˆì • ìœ ì§€
- [x] Lossê°€ epochë§ˆë‹¤ ê°ì†Œ
- [x] ì™¸êµ­ì–´ í˜¼ì… < 3%

### Phase 2
- [x] ROUGE-1 > 35
- [x] ROUGE-L > 30
- [x] ROUGE Sum > 70

### Phase 3
- [x] í•™ìŠµ ì‹œê°„ 40% ì´ìƒ ë‹¨ì¶•
- [x] ROUGE ì„±ëŠ¥ ìœ ì§€ ë˜ëŠ” í–¥ìƒ
- [x] Packingìœ¼ë¡œ ì¸í•œ í’ˆì§ˆ ì €í•˜ ì—†ìŒ

---

## ğŸ“ ë¬¸ì„œí™” ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Phase 1 ì ìš© í›„ W&B ë§í¬ ê¸°ë¡
- [ ] Phase 2 ì ìš© í›„ ìƒ˜í”Œ ìƒì„± ê²°ê³¼ ë¹„êµ
- [ ] Phase 3 ì ìš© í›„ í•™ìŠµ ì‹œê°„ ì¸¡ì •
- [ ] ìµœì¢… ì„±ëŠ¥ ë¹„êµí‘œ ì—…ë°ì´íŠ¸
- [ ] CLAUDE.mdì— Critical Learnings ì¶”ê°€

---

## ğŸ”— ì°¸ê³  ìë£Œ

- Korean_DCS_2024 ë² ì´ìŠ¤ë¼ì¸: `/Competition/NLP/Korean_DCS_2024/`
- TRL ë¬¸ì„œ: https://huggingface.co/docs/trl
- í˜„ì¬ ì½”ë“œ: `/Competition/NLP/naturallanguageprocessingcompetition-nlp-1/dialogue-summarization/`
- W&B í”„ë¡œì íŠ¸: https://wandb.ai/bkan-ai/dialogue-summarization-finetuning

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-10-05
**ì‘ì„±ì**: Claude Code
**ìƒíƒœ**: Phase 1 ì¤€ë¹„ ì™„ë£Œ
