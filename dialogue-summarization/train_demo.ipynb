{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ—¨ï¸ Dialogue Summarization - Training Demo\n",
    "\n",
    "> baseline.ipynb ì¬í˜„ì„ ìœ„í•œ ë°ëª¨ ë…¸íŠ¸ë¶\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ëª¨ë“ˆí™”ëœ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ baseline.ipynbì™€ ë™ì¼í•œ í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Competition/NLP/dialogue-summarization\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€\n",
    "project_root = '/Competition/NLP/dialogue-summarization'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "os.chdir(project_root)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas                    2.1.4\n",
      "pytorch-lightning         2.1.2\n",
      "rouge                     1.0.1\n",
      "rouge-score               0.1.2\n",
      "torch                     2.7.1+cu118\n",
      "torchaudio                2.7.1+cu118\n",
      "torchelastic              0.2.2\n",
      "torchmetrics              1.2.1\n",
      "torchvision               0.22.1+cu118\n",
      "transformers              4.56.2\n"
     ]
    }
   ],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸\n",
    "!pip list | grep -E \"torch|transformers|rouge|pandas|pyyaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ëª¨ë“ˆ Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import torch\n",
    "from transformers import (\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "# ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë“ˆë“¤\n",
    "from src.data.preprocessor import Preprocess\n",
    "from src.data.dataset import DatasetForTrain, DatasetForVal\n",
    "from src.models.model_loader import load_tokenizer_and_model\n",
    "from src.evaluation.metrics import compute_metrics_for_trainer\n",
    "from src.utils.seed import set_seed\n",
    "\n",
    "print(\"âœ… All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì„¤ì • ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Configuration:\n",
      "  Model: digit82/kobart-summarization\n",
      "  Epochs: 20\n",
      "  Batch size: 50\n",
      "  Learning rate: 1e-05\n",
      "  Output: /Competition/NLP/dialogue-summarization/checkpoints/baseline_run\n"
     ]
    }
   ],
   "source": [
    "# Config íŒŒì¼ ë¡œë“œ\n",
    "with open('configs/train_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# ì£¼ìš” ì„¤ì • ì¶œë ¥\n",
    "print(\"ğŸ“‹ Configuration:\")\n",
    "print(f\"  Model: {config['general']['model_name']}\")\n",
    "print(f\"  Epochs: {config['training']['num_train_epochs']}\")\n",
    "print(f\"  Batch size: {config['training']['per_device_train_batch_size']}\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Output: {config['general']['output_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Seed ê³ ì •\nset_seed(config['training']['seed'])\nprint(f\"âœ… Random seed set to {config['training']['seed']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"ğŸ–¥ï¸  Device: {device}\")\n",
    "\n",
    "model, tokenizer = load_tokenizer_and_model(\n",
    "    model_name=config['general']['model_name'],\n",
    "    special_tokens=config['tokenizer']['special_tokens'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"âœ… Model loaded: {config['general']['model_name']}\")\n",
    "print(f\"   Vocab size: {len(tokenizer)}\")\n",
    "print(f\"   Special tokens: {len(config['tokenizer']['special_tokens'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë°ì´í„° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessor ì´ˆê¸°í™”\n",
    "preprocessor = Preprocess(\n",
    "    bos_token=tokenizer.bos_token,\n",
    "    eos_token=tokenizer.eos_token\n",
    ")\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "data_path = config['general']['data_path']\n",
    "train_data = preprocessor.make_set_as_df(f\"{data_path}/train.csv\", is_train=True)\n",
    "val_data = preprocessor.make_set_as_df(f\"{data_path}/dev.csv\", is_train=True)\n",
    "\n",
    "print(f\"ğŸ“Š Data loaded:\")\n",
    "print(f\"   Train: {len(train_data)} samples\")\n",
    "print(f\"   Val: {len(val_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒ˜í”Œ í™•ì¸\n",
    "print(\"\\nğŸ” Sample data:\")\n",
    "sample = train_data.iloc[0]\n",
    "print(f\"Dialogue: {sample['dialogue'][:100]}...\")\n",
    "print(f\"Summary: {sample['summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²˜ë¦¬ ë° í† í¬ë‚˜ì´ì§•\n",
    "encoder_input_train, decoder_input_train, decoder_output_train = preprocessor.make_input(\n",
    "    train_data, is_test=False\n",
    ")\n",
    "encoder_input_val, decoder_input_val, decoder_output_val = preprocessor.make_input(\n",
    "    val_data, is_test=False\n",
    ")\n",
    "\n",
    "print(\"âœ… Preprocessing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í† í¬ë‚˜ì´ì§•\n",
    "tokenizer_config = config['tokenizer']\n",
    "\n",
    "tokenized_encoder_inputs_train = tokenizer(\n",
    "    encoder_input_train,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    max_length=tokenizer_config['encoder_max_len'],\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "tokenized_decoder_inputs_train = tokenizer(\n",
    "    decoder_input_train,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    max_length=tokenizer_config['decoder_max_len'],\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "tokenized_decoder_outputs_train = tokenizer(\n",
    "    decoder_output_train,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    max_length=tokenizer_config['decoder_max_len'],\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "print(f\"âœ… Tokenization completed\")\n",
    "print(f\"   Encoder shape: {tokenized_encoder_inputs_train['input_ids'].shape}\")\n",
    "print(f\"   Decoder shape: {tokenized_decoder_inputs_train['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation ë°ì´í„°ë„ í† í¬ë‚˜ì´ì§•\n",
    "tokenized_encoder_inputs_val = tokenizer(\n",
    "    encoder_input_val,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    max_length=tokenizer_config['encoder_max_len'],\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "tokenized_decoder_inputs_val = tokenizer(\n",
    "    decoder_input_val,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    max_length=tokenizer_config['decoder_max_len'],\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "tokenized_decoder_outputs_val = tokenizer(\n",
    "    decoder_output_val,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    max_length=tokenizer_config['decoder_max_len'],\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "print(\"âœ… Validation data tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset ìƒì„±\n",
    "train_dataset = DatasetForTrain(\n",
    "    tokenized_encoder_inputs_train,\n",
    "    tokenized_decoder_inputs_train,\n",
    "    tokenized_decoder_outputs_train,\n",
    "    len(train_data)\n",
    ")\n",
    "\n",
    "val_dataset = DatasetForVal(\n",
    "    tokenized_encoder_inputs_val,\n",
    "    tokenized_decoder_inputs_val,\n",
    "    tokenized_decoder_outputs_val,\n",
    "    len(val_data)\n",
    ")\n",
    "\n",
    "print(f\"âœ… Datasets created\")\n",
    "print(f\"   Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"   Val dataset: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í•™ìŠµ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training Arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=config['general']['output_dir'],\n    overwrite_output_dir=True,\n    num_train_epochs=config['training']['num_train_epochs'],\n    per_device_train_batch_size=config['training']['per_device_train_batch_size'],\n    per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],\n    learning_rate=config['training']['learning_rate'],\n    warmup_ratio=config['training']['warmup_ratio'],\n    lr_scheduler_type=config['training']['lr_scheduler_type'],\n    optim=config['training']['optim'],\n    eval_strategy=config['training']['eval_strategy'],\n    save_strategy=config['training']['save_strategy'],\n    save_total_limit=config['training']['save_total_limit'],\n    load_best_model_at_end=config['training']['load_best_model_at_end'],\n    seed=config['training']['seed'],\n    logging_dir=config['training']['logging_dir'],\n    predict_with_generate=True,\n    generation_max_length=tokenizer_config['decoder_max_len'],\n    fp16=torch.cuda.is_available(),\n)\n\nprint(\"âœ… Training arguments configured\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Compute Metrics\n",
    "compute_metrics = compute_metrics_for_trainer(\n",
    "    tokenizer=tokenizer,\n",
    "    remove_tokens=['<usr>', tokenizer.bos_token, tokenizer.eos_token, tokenizer.pad_token]\n",
    ")\n",
    "\n",
    "print(\"âœ… Data collator and metrics configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer ìƒì„±\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. í•™ìŠµ ì‹¤í–‰\n",
    "\n",
    "âš ï¸ **ì£¼ì˜**: ì „ì²´ í•™ìŠµì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤ (ìˆ˜ ì‹œê°„ ~ í•˜ë£¨).\n",
    "\n",
    "í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œëŠ” `num_train_epochs`ë¥¼ 1ë¡œ ì¤„ì—¬ì„œ ì‹¤í–‰í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì‹œì‘\n",
    "print(\"ğŸš€ Training started...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nâœ… Training completed!\")\n",
    "print(f\"   Train loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"   Train time: {train_result.metrics['train_runtime']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation ì…‹ í‰ê°€\n",
    "print(\"ğŸ“Š Evaluating on validation set...\")\n",
    "eval_result = trainer.evaluate()\n",
    "\n",
    "print(\"\\nâœ… Evaluation completed!\")\n",
    "print(f\"   ROUGE-1: {eval_result['eval_rouge-1']:.2f}\")\n",
    "print(f\"   ROUGE-2: {eval_result['eval_rouge-2']:.2f}\")\n",
    "print(f\"   ROUGE-L: {eval_result['eval_rouge-l']:.2f}\")\n",
    "print(f\"   ROUGE Sum: {eval_result['eval_rouge-1'] + eval_result['eval_rouge-2'] + eval_result['eval_rouge-l']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ëª¨ë¸ ì €ì¥\n",
    "final_model_path = f\"{config['general']['output_dir']}/final_model\"\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"âœ… Model saved to: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ìƒ˜í”Œ ì˜ˆì¸¡ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª‡ ê°œ ìƒ˜í”Œì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "sample_indices = random.sample(range(len(val_data)), min(3, len(val_data)))\n",
    "\n",
    "for idx in sample_indices:\n",
    "    sample = val_data.iloc[idx]\n",
    "    \n",
    "    # ì…ë ¥ í† í¬ë‚˜ì´ì§•\n",
    "    input_ids = tokenizer(\n",
    "        sample['dialogue'],\n",
    "        return_tensors='pt',\n",
    "        max_length=tokenizer_config['encoder_max_len'],\n",
    "        truncation=True\n",
    "    ).input_ids.to(device)\n",
    "    \n",
    "    # ì˜ˆì¸¡\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=tokenizer_config['decoder_max_len'],\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    predicted_summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Sample {idx + 1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n[Dialogue]\\n{sample['dialogue'][:200]}...\")\n",
    "    print(f\"\\n[Ground Truth]\\n{sample['summary']}\")\n",
    "    print(f\"\\n[Predicted]\\n{predicted_summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì™„ë£Œ!\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„:\n",
    "1. **ì¶”ë¡ **: `generate_predictions.py` ì‹¤í–‰í•˜ì—¬ test ì…‹ ì˜ˆì¸¡\n",
    "2. **ì œì¶œ**: ìƒì„±ëœ CSV íŒŒì¼ì„ ê²½ì§„ëŒ€íšŒ í”Œë«í¼ì— ì œì¶œ\n",
    "3. **ê°œì„ **: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹, ë‹¤ë¥¸ ëª¨ë¸ ì‹œë„"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}