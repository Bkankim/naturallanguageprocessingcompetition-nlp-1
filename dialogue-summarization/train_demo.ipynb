{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🗨️ Dialogue Summarization - Training Demo\n",
    "\n",
    "> baseline.ipynb 재현을 위한 데모 노트북\n",
    "\n",
    "이 노트북은 모듈화된 코드를 사용하여 baseline.ipynb와 동일한 학습을 수행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Competition/NLP/dialogue-summarization\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 프로젝트 루트를 Python path에 추가\n",
    "project_root = '/Competition/NLP/dialogue-summarization'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "os.chdir(project_root)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas                    2.1.4\n",
      "pytorch-lightning         2.1.2\n",
      "rouge                     1.0.1\n",
      "rouge-score               0.1.2\n",
      "torch                     2.7.1+cu118\n",
      "torchaudio                2.7.1+cu118\n",
      "torchelastic              0.2.2\n",
      "torchmetrics              1.2.1\n",
      "torchvision               0.22.1+cu118\n",
      "transformers              4.56.2\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 확인\n",
    "!pip list | grep -E \"torch|transformers|rouge|pandas|pyyaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모듈 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import torch\n",
    "from transformers import (\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "# 우리가 만든 모듈들\n",
    "from src.data.preprocessor import Preprocess\n",
    "from src.data.dataset import DatasetForTrain, DatasetForVal\n",
    "from src.models.model_loader import load_tokenizer_and_model\n",
    "from src.evaluation.metrics import compute_metrics_for_trainer\n",
    "from src.utils.seed import set_seed\n",
    "\n",
    "print(\"✅ All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 설정 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Configuration:\n",
      "  Model: digit82/kobart-summarization\n",
      "  Epochs: 20\n",
      "  Batch size: 50\n",
      "  Learning rate: 1e-05\n",
      "  Output: /Competition/NLP/dialogue-summarization/checkpoints/baseline_run\n"
     ]
    }
   ],
   "source": [
    "# Config 파일 로드\n",
    "with open('configs/train_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# 주요 설정 출력\n",
    "print(\"📋 Configuration:\")\n",
    "print(f\"  Model: {config['general']['model_name']}\")\n",
    "print(f\"  Epochs: {config['training']['num_train_epochs']}\")\n",
    "print(f\"  Batch size: {config['training']['per_device_train_batch_size']}\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Output: {config['general']['output_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Seed 고정\nset_seed(config['training']['seed'])\nprint(f\"✅ Random seed set to {config['training']['seed']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 & 토크나이저 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"🖥️  Device: {device}\")\n",
    "\n",
    "model, tokenizer = load_tokenizer_and_model(\n",
    "    model_name=config['general']['model_name'],\n",
    "    special_tokens=config['tokenizer']['special_tokens'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"✅ Model loaded: {config['general']['model_name']}\")\n",
    "print(f\"   Vocab size: {len(tokenizer)}\")\n",
    "print(f\"   Special tokens: {len(config['tokenizer']['special_tokens'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessor 초기화\n",
    "preprocessor = Preprocess(\n",
    "    bos_token=tokenizer.bos_token,\n",
    "    eos_token=tokenizer.eos_token\n",
    ")\n",
    "\n",
    "# 데이터 로드\n",
    "data_path = config['general']['data_path']\n",
    "train_data = preprocessor.make_set_as_df(f\"{data_path}/train.csv\", is_train=True)\n",
    "val_data = preprocessor.make_set_as_df(f\"{data_path}/dev.csv\", is_train=True)\n",
    "\n",
    "print(f\"📊 Data loaded:\")\n",
    "print(f\"   Train: {len(train_data)} samples\")\n",
    "print(f\"   Val: {len(val_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 확인\n",
    "print(\"\\n🔍 Sample data:\")\n",
    "sample = train_data.iloc[0]\n",
    "print(f\"Dialogue: {sample['dialogue'][:100]}...\")\n",
    "print(f\"Summary: {sample['summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 및 토크나이징\n",
    "encoder_input_train, decoder_input_train, decoder_output_train = preprocessor.make_input(\n",
    "    train_data, is_test=False\n",
    ")\n",
    "encoder_input_val, decoder_input_val, decoder_output_val = preprocessor.make_input(\n",
    "    val_data, is_test=False\n",
    ")\n",
    "\n",
    "print(\"✅ Preprocessing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이징\n",
    "tokenizer_config = config['tokenizer']\n",
    "\n",
    "tokenized_encoder_inputs_train = tokenizer(\n",
    "    encoder_input_train,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    max_length=tokenizer_config['encoder_max_len'],\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "tokenized_decoder_inputs_train = tokenizer(\n",
    "    decoder_input_train,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    max_length=tokenizer_config['decoder_max_len'],\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "tokenized_decoder_outputs_train = tokenizer(\n",
    "    decoder_output_train,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    max_length=tokenizer_config['decoder_max_len'],\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "print(f\"✅ Tokenization completed\")\n",
    "print(f\"   Encoder shape: {tokenized_encoder_inputs_train['input_ids'].shape}\")\n",
    "print(f\"   Decoder shape: {tokenized_decoder_inputs_train['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation 데이터도 토크나이징\n",
    "tokenized_encoder_inputs_val = tokenizer(\n",
    "    encoder_input_val,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    max_length=tokenizer_config['encoder_max_len'],\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "tokenized_decoder_inputs_val = tokenizer(\n",
    "    decoder_input_val,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    max_length=tokenizer_config['decoder_max_len'],\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "tokenized_decoder_outputs_val = tokenizer(\n",
    "    decoder_output_val,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    max_length=tokenizer_config['decoder_max_len'],\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "print(\"✅ Validation data tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 생성\n",
    "train_dataset = DatasetForTrain(\n",
    "    tokenized_encoder_inputs_train,\n",
    "    tokenized_decoder_inputs_train,\n",
    "    tokenized_decoder_outputs_train,\n",
    "    len(train_data)\n",
    ")\n",
    "\n",
    "val_dataset = DatasetForVal(\n",
    "    tokenized_encoder_inputs_val,\n",
    "    tokenized_decoder_inputs_val,\n",
    "    tokenized_decoder_outputs_val,\n",
    "    len(val_data)\n",
    ")\n",
    "\n",
    "print(f\"✅ Datasets created\")\n",
    "print(f\"   Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"   Val dataset: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training Arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=config['general']['output_dir'],\n    overwrite_output_dir=True,\n    num_train_epochs=config['training']['num_train_epochs'],\n    per_device_train_batch_size=config['training']['per_device_train_batch_size'],\n    per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],\n    learning_rate=config['training']['learning_rate'],\n    warmup_ratio=config['training']['warmup_ratio'],\n    lr_scheduler_type=config['training']['lr_scheduler_type'],\n    optim=config['training']['optim'],\n    eval_strategy=config['training']['eval_strategy'],\n    save_strategy=config['training']['save_strategy'],\n    save_total_limit=config['training']['save_total_limit'],\n    load_best_model_at_end=config['training']['load_best_model_at_end'],\n    seed=config['training']['seed'],\n    logging_dir=config['training']['logging_dir'],\n    predict_with_generate=True,\n    generation_max_length=tokenizer_config['decoder_max_len'],\n    fp16=torch.cuda.is_available(),\n)\n\nprint(\"✅ Training arguments configured\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Compute Metrics\n",
    "compute_metrics = compute_metrics_for_trainer(\n",
    "    tokenizer=tokenizer,\n",
    "    remove_tokens=['<usr>', tokenizer.bos_token, tokenizer.eos_token, tokenizer.pad_token]\n",
    ")\n",
    "\n",
    "print(\"✅ Data collator and metrics configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 생성\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"✅ Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 학습 실행\n",
    "\n",
    "⚠️ **주의**: 전체 학습은 시간이 오래 걸립니다 (수 시간 ~ 하루).\n",
    "\n",
    "테스트용으로는 `num_train_epochs`를 1로 줄여서 실행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 시작\n",
    "print(\"🚀 Training started...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n✅ Training completed!\")\n",
    "print(f\"   Train loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"   Train time: {train_result.metrics['train_runtime']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation 셋 평가\n",
    "print(\"📊 Evaluating on validation set...\")\n",
    "eval_result = trainer.evaluate()\n",
    "\n",
    "print(\"\\n✅ Evaluation completed!\")\n",
    "print(f\"   ROUGE-1: {eval_result['eval_rouge-1']:.2f}\")\n",
    "print(f\"   ROUGE-2: {eval_result['eval_rouge-2']:.2f}\")\n",
    "print(f\"   ROUGE-L: {eval_result['eval_rouge-l']:.2f}\")\n",
    "print(f\"   ROUGE Sum: {eval_result['eval_rouge-1'] + eval_result['eval_rouge-2'] + eval_result['eval_rouge-l']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 모델 저장\n",
    "final_model_path = f\"{config['general']['output_dir']}/final_model\"\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"✅ Model saved to: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 샘플 예측 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 몇 개 샘플에 대해 예측 수행\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "sample_indices = random.sample(range(len(val_data)), min(3, len(val_data)))\n",
    "\n",
    "for idx in sample_indices:\n",
    "    sample = val_data.iloc[idx]\n",
    "    \n",
    "    # 입력 토크나이징\n",
    "    input_ids = tokenizer(\n",
    "        sample['dialogue'],\n",
    "        return_tensors='pt',\n",
    "        max_length=tokenizer_config['encoder_max_len'],\n",
    "        truncation=True\n",
    "    ).input_ids.to(device)\n",
    "    \n",
    "    # 예측\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=tokenizer_config['decoder_max_len'],\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    predicted_summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Sample {idx + 1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n[Dialogue]\\n{sample['dialogue'][:200]}...\")\n",
    "    print(f\"\\n[Ground Truth]\\n{sample['summary']}\")\n",
    "    print(f\"\\n[Predicted]\\n{predicted_summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 완료!\n",
    "\n",
    "### 다음 단계:\n",
    "1. **추론**: `generate_predictions.py` 실행하여 test 셋 예측\n",
    "2. **제출**: 생성된 CSV 파일을 경진대회 플랫폼에 제출\n",
    "3. **개선**: 하이퍼파라미터 튜닝, 다른 모델 시도"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}