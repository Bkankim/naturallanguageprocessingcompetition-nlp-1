# í•œêµ­ì–´ ëŒ€í™” ìš”ì•½ ëŒ€íšŒ ì§„í–‰ ìƒí™© ë³´ê³ ì„œ

**ì‘ì„±ì¼ì‹œ**: 2025-10-04 14:43 KST
**í”„ë¡œì íŠ¸**: Korean Dialogue Summarization Competition
**í˜„ì¬ ë‹¨ê³„**: LLM Fine-tuning with QLoRA

---

## ğŸ“Š Executive Summary

### í”„ë¡œì íŠ¸ ê°œìš”
- **ê³¼ì œ**: í•œêµ­ì–´ ë‹¤ìê°„ ëŒ€í™” ë°ì´í„°ë¡œë¶€í„° ìš”ì•½ë¬¸ ìƒì„±
- **í‰ê°€ ì§€í‘œ**: ROUGE-1, ROUGE-2, ROUGE-L F1 ì ìˆ˜ í‰ê·  (í•œêµ­ì–´ í˜•íƒœì†Œ í† í¬ë‚˜ì´ì € ì‚¬ìš©)
- **í˜„ì¬ ë°©ë²•ë¡ **: QLoRA 4bit ì–‘ìí™”ë¥¼ í™œìš©í•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ íŒŒì¸íŠœë‹

### ì§„í–‰ í˜„í™©
- âœ… **ì™„ë£Œ**: 1/5 ëª¨ë¸ (koBART)
- ğŸ”„ **ì§„í–‰ ì¤‘**: 1/5 ëª¨ë¸ (Llama-3.2-Korean-3B)
- â³ **ëŒ€ê¸° ì¤‘**: 3/5 ëª¨ë¸ (Qwen3-4B, Qwen2.5-7B, Llama-3-Korean-8B)

### ì „ì²´ ì§„í–‰ë¥ 
**20%** (1/5 ëª¨ë¸ ì™„ë£Œ)

---

## âœ… Completed Work

### 1. koBART Fine-tuning
**ìƒíƒœ**: âœ… Complete
**ì™„ë£Œ ì‹œê°„**: 2025-10-04 13:30 KST
**ì†Œìš” ì‹œê°„**: ~30ë¶„

#### ì„±ëŠ¥ ì§€í‘œ
| Metric | Score |
|--------|-------|
| **ROUGE-1 F1** | 56.20% |
| **ROUGE-2 F1** | 24.35% |
| **ROUGE-L F1** | 13.96% |
| **ROUGE SUM** | **94.51** |

#### ëª¨ë¸ êµ¬ì„±
- **ë² ì´ìŠ¤ ëª¨ë¸**: gogamza/kobart-base-v2
- **íŒŒë¼ë¯¸í„° ìˆ˜**: ~123M
- **ì–‘ìí™”**: ì—†ìŒ (Full precision)
- **í•™ìŠµ ì„¤ì •**:
  - Epochs: 20
  - Batch size: 8
  - Learning rate: 5e-5
  - Optimizer: AdamW

#### ì‚°ì¶œë¬¼
- ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸: `/Competition/NLP/naturallanguageprocessingcompetition-nlp-1/dialogue-summarization/training/kobart-baseline-finetuned/`
- ì˜ˆì¸¡ ê²°ê³¼: `baseline_kobart_predictions.csv`
- W&B ë¡œê·¸: [ë§í¬ í™•ì¸ í•„ìš”]

---

### 2. Critical Bug Fixes

#### 2.1 Metric Configuration Error ìˆ˜ì •
**ë¬¸ì œ**: `metric_for_best_model` ì„¤ì • ì˜¤ë¥˜ë¡œ ì¸í•œ í•™ìŠµ ì¤‘ë‹¨
```python
# Before (ì˜ëª»ëœ ì„¤ì •)
metric_for_best_model="rouge-1"

# After (ì˜¬ë°”ë¥¸ ì„¤ì •)
metric_for_best_model="rouge1"
```

#### 2.2 Chat Template Tokens ì¶”ê°€
**ë¬¸ì œ**: Llama/Qwen ëª¨ë¸ì˜ chat template í† í° ëˆ„ë½
```python
# ì¶”ê°€ëœ special tokens
special_tokens = {
    'additional_special_tokens': [
        '<|begin_of_text|>',
        '<|end_of_text|>',
        '<|start_header_id|>',
        '<|end_header_id|>',
        '<|eot_id|>',
        # ... PII ë° Person í† í°ë“¤
    ]
}
```

#### 2.3 QLoRA Compute Dtype ì •ë ¬
**ë¬¸ì œ**: ëª¨ë¸ dtype(bfloat16)ê³¼ compute dtype(float16) ë¶ˆì¼ì¹˜
```python
# Before
bnb_config = BitsAndBytesConfig(
    bnb_4bit_compute_dtype=torch.float16
)

# After
bnb_config = BitsAndBytesConfig(
    bnb_4bit_compute_dtype=torch.bfloat16  # ëª¨ë¸ dtypeê³¼ ì¼ì¹˜
)
```

---

## ğŸ”„ Current Activities

### Llama-3.2-Korean-3B Fine-tuning (ì§„í–‰ ì¤‘)

**ì‹œì‘ ì‹œê°„**: 2025-10-04 14:22 KST (ì¬ì‹œì‘)
**í˜„ì¬ ì§„í–‰ë¥ **: Step 127/390 (33%)
**ì²˜ë¦¬ ì†ë„**: ~8.7ì´ˆ/step
**ì˜ˆìƒ ì™„ë£Œ ì‹œê°„**: ~40ë¶„ ë‚¨ìŒ (ì•½ 15:02 KST)

#### ì‹¤ì‹œê°„ í•™ìŠµ ë©”íŠ¸ë¦­
- **í˜„ì¬ Step**: 127/390
- **Loss**: ëª¨ë‹ˆí„°ë§ ì¤‘
- **í•™ìŠµë¥ **: 2e-4 (constant scheduler)

#### W&B ëª¨ë‹ˆí„°ë§
- **Run URL**: https://wandb.ai/bkan-ai/dialogue-summarization-finetuning/runs/hlpzuzzs
- **ì‹¤ì‹œê°„ ì¶”ì **: Loss, Learning Rate, GPU Utilization

#### ëª¨ë¸ êµ¬ì„±
- **ë² ì´ìŠ¤ ëª¨ë¸**: beomi/Llama-3.2-Korean-3B-Instruct
- **íŒŒë¼ë¯¸í„° ìˆ˜**: ~3.21B
- **ì–‘ìí™”**: QLoRA 4bit (NF4)
- **LoRA ì„¤ì •**:
  - Rank: 16
  - Alpha: 32
  - Target modules: ëª¨ë“  Linear ë ˆì´ì–´
  - Dropout: 0.05

#### í•™ìŠµ ì„¤ì •
```yaml
Training Arguments:
  learning_rate: 2e-4
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 2
  effective_batch_size: 8
  num_train_epochs: 10
  warmup_ratio: 0.03
  lr_scheduler_type: cosine

Generation Config:
  max_new_tokens: 512
  do_sample: True
  temperature: 0.7
  top_p: 0.9
```

---

## â³ Pending Tasks

### 1. Qwen3-4B-Instruct Fine-tuning
**ìƒíƒœ**: â³ Pending
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: ~40-50ë¶„
**ìš°ì„ ìˆœìœ„**: High

#### ê³„íšëœ êµ¬ì„±
- **ë² ì´ìŠ¤ ëª¨ë¸**: Qwen/Qwen3-4B-Instruct-2507
- **íŒŒë¼ë¯¸í„° ìˆ˜**: ~4.02B
- **LoRA Rank**: 16
- **Batch Size**: 4
- **Learning Rate**: 2e-4

---

### 2. Qwen2.5-7B-Instruct Fine-tuning
**ìƒíƒœ**: â³ Pending
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: ~60-80ë¶„
**ìš°ì„ ìˆœìœ„**: Medium

#### ê³„íšëœ êµ¬ì„±
- **ë² ì´ìŠ¤ ëª¨ë¸**: Qwen/Qwen2.5-7B-Instruct
- **íŒŒë¼ë¯¸í„° ìˆ˜**: ~7.61B
- **LoRA Rank**: 16
- **Batch Size**: 2 (ë©”ëª¨ë¦¬ ì œì•½)
- **Learning Rate**: 1e-4

---

### 3. Llama-3-Korean-8B Fine-tuning
**ìƒíƒœ**: â³ Pending
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: ~80-100ë¶„
**ìš°ì„ ìˆœìœ„**: High

#### ê³„íšëœ êµ¬ì„±
- **ë² ì´ìŠ¤ ëª¨ë¸**: beomi/Llama-3-Open-Ko-8B-Instruct-preview
- **íŒŒë¼ë¯¸í„° ìˆ˜**: ~8B
- **LoRA Rank**: 16
- **Batch Size**: 1 (ë©”ëª¨ë¦¬ ì œì•½)
- **Learning Rate**: 1e-4

---

## ğŸ›  Technical Configuration

### Hardware Setup
```yaml
GPU: NVIDIA RTX 3090
VRAM: 24GB
CUDA: 12.1
PyTorch: 2.5.1
Precision: TF32 (ìµœì í™” í™œì„±í™”)
```

### QLoRA Configuration
```python
BitsAndBytesConfig:
  load_in_4bit: True
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: torch.bfloat16
  bnb_4bit_use_double_quant: True

LoRAConfig:
  r: 16
  lora_alpha: 32
  target_modules: "all-linear"
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"
```

### Optimization Features
- âœ… Gradient Checkpointing (ë©”ëª¨ë¦¬ ì ˆì•½)
- âœ… TF32 Precision (RTX 3090 ìµœì í™”)
- âœ… Mixed Precision Training (BF16)
- âœ… Flash Attention 2 (ì¼ë¶€ ëª¨ë¸)
- âœ… Gradient Accumulation (íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸° ì¦ê°€)

---

## ğŸ“ˆ Performance Tracking

### ëª¨ë¸ ë¹„êµí‘œ

| Model | Status | Params | ROUGE-1 | ROUGE-2 | ROUGE-L | ROUGE SUM | Training Time |
|-------|--------|--------|---------|---------|---------|-----------|---------------|
| **koBART** | âœ… Complete | 123M | 56.20% | 24.35% | 13.96% | **94.51** | ~30ë¶„ |
| **Llama-3.2-Korean-3B** | ğŸ”„ Training | 3.2B | - | - | - | - | ~40ë¶„ (33% ì™„ë£Œ) |
| **Qwen3-4B-Instruct** | â³ Pending | 4.0B | - | - | - | - | ~40ë¶„ (ì˜ˆìƒ) |
| **Qwen2.5-7B-Instruct** | â³ Pending | 7.6B | - | - | - | - | ~75ë¶„ (ì˜ˆìƒ) |
| **Llama-3-Korean-8B** | â³ Pending | 8.0B | - | - | - | - | ~90ë¶„ (ì˜ˆìƒ) |

### ê¸°ëŒ€ ì„±ëŠ¥
- **Baseline (koBART)**: 94.51 ROUGE SUM
- **Target (LLM)**: 100+ ROUGE SUM
- **Competition Benchmark**: Random reference selection â‰ˆ 70ì 

---

## âš ï¸ Risk Management

### 1. Disk Space Monitoring
```bash
í˜„ì¬ ì‚¬ìš©ëŸ‰: 110GB / 150GB
ê°€ìš© ê³µê°„: 40GB
ì•ˆì „ ë²„í¼: ì¶©ë¶„ âœ…
```

**ëª¨ë‹ˆí„°ë§ ì „ëµ**:
- í•™ìŠµ ì „ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ì²´í¬
- ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹œ ê³µê°„ í™•ì¸
- ë¶ˆí•„ìš”í•œ ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ

### 2. Memory Management
**RTX 3090 24GB VRAM**:
- 3B ëª¨ë¸: Batch size 4 (ì•ˆì „)
- 7B ëª¨ë¸: Batch size 2 (ì£¼ì˜)
- 8B ëª¨ë¸: Batch size 1 (í•œê³„)

**ëŒ€ì‘ ì „ëµ**:
- Gradient accumulation í™œìš©
- Gradient checkpointing í™œì„±í™”
- í•„ìš”ì‹œ max_length ì¶•ì†Œ

### 3. Training Stability
**ì ì¬ì  ì´ìŠˆ**:
- OOM (Out of Memory) ì—ëŸ¬
- Gradient explosion
- Learning rate ë¶€ì í•©

**ëŒ€ì‘ ë°©ì•ˆ**:
- Automatic mixed precision
- Gradient clipping (max_grad_norm=1.0)
- Warmup ratio 0.03 ì ìš©

### 4. GitHub Backup
**ë™ê¸°í™” ì „ëµ**:
- ê° ëª¨ë¸ í•™ìŠµ ì™„ë£Œ í›„ ì»¤ë°‹
- ì£¼ìš” ë²„ê·¸ ìˆ˜ì • ì‹œ ì¦‰ì‹œ í‘¸ì‹œ
- ì‹¤í—˜ ê²°ê³¼ CSV íŒŒì¼ ë°±ì—…

---

## ğŸ“… Timeline

### ì™„ë£Œëœ ì‘ì—…
- âœ… **2025-10-03**: í”„ë¡œì íŠ¸ ì´ˆê¸° ì„¤ì • ë° ë°ì´í„° ì¤€ë¹„
- âœ… **2025-10-04 10:00**: koBART baseline í•™ìŠµ ì‹œì‘
- âœ… **2025-10-04 13:30**: koBART í•™ìŠµ ì™„ë£Œ (94.51 ROUGE SUM)
- âœ… **2025-10-04 14:00**: QLoRA ë²„ê·¸ ìˆ˜ì • ì™„ë£Œ (compute dtype alignment)
- âœ… **2025-10-04 14:22**: Llama-3.2-Korean-3B í•™ìŠµ ì¬ì‹œì‘

### ì§„í–‰ ì¤‘
- ğŸ”„ **2025-10-04 14:22-15:02**: Llama-3.2-Korean-3B í•™ìŠµ ì¤‘ (33% ì™„ë£Œ)

### ì˜ˆì •ëœ ì‘ì—…
- â³ **2025-10-04 15:05**: Qwen3-4B-Instruct í•™ìŠµ ì‹œì‘
- â³ **2025-10-04 15:45**: Qwen2.5-7B-Instruct í•™ìŠµ ì‹œì‘
- â³ **2025-10-04 17:00**: Llama-3-Korean-8B í•™ìŠµ ì‹œì‘
- â³ **2025-10-04 18:30**: ì „ì²´ ëª¨ë¸ í‰ê°€ ë° ë¹„êµ ë¶„ì„
- â³ **2025-10-04 19:00**: ìµœì¢… ì œì¶œ ëª¨ë¸ ì„ ì •

### ì „ì²´ íƒ€ì„ë¼ì¸
```
10:00 â”â”â”â”â”â”â”â”â”â” koBART (30ë¶„) â”â”â”â”â”â”â”â”â”â”â”â”â”â–º 13:30 âœ…
14:22 â”â”â”â”â”â”â”â” Llama-3.2-3B (40ë¶„) â”â”â”â”â”â”â”â”â”â–º 15:02 ğŸ”„ 33%
15:05 â”â”â”â”â”â”â”â” Qwen3-4B (40ë¶„) â”â”â”â”â”â”â”â”â”â”â”â”â–º 15:45 â³
15:45 â”â”â”â”â”â”â”â” Qwen2.5-7B (75ë¶„) â”â”â”â”â”â”â”â”â”â”â–º 17:00 â³
17:00 â”â”â”â”â”â”â”â” Llama-3-KO-8B (90ë¶„) â”â”â”â”â”â”â”â–º 18:30 â³
18:30 â”â”â”â”â”â”â”â” í‰ê°€ ë° ë¶„ì„ (30ë¶„) â”â”â”â”â”â”â”â”â”â–º 19:00 â³
```

**ì˜ˆìƒ ì „ì²´ ì™„ë£Œ ì‹œê°„**: 2025-10-04 19:00 KST

---

## ğŸ¯ Next Steps

### ì¦‰ì‹œ ìˆ˜í–‰ (0-1ì‹œê°„)
1. **Llama-3.2-Korean-3B í•™ìŠµ ëª¨ë‹ˆí„°ë§**
   - W&B ë¡œê·¸ ì‹¤ì‹œê°„ í™•ì¸
   - Loss ìˆ˜ë ´ ì—¬ë¶€ ê´€ì°°
   - ì™„ë£Œ ì‹œ í‰ê°€ ìˆ˜í–‰

2. **Qwen2.5-3B í•™ìŠµ ì¤€ë¹„**
   - ë””ìŠ¤í¬ ê³µê°„ ì¬í™•ì¸
   - í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ìµœì¢… ì ê²€
   - W&B run ì„¤ì •

### ë‹¨ê¸° ëª©í‘œ (1-4ì‹œê°„)
3. **ë‚¨ì€ 3ê°œ ëª¨ë¸ ìˆœì°¨ í•™ìŠµ**
   - Qwen2.5-3B â†’ Qwen2.5-7B â†’ Llama-3-KO-8B
   - ê° ëª¨ë¸ë³„ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš©
   - í•™ìŠµ ì¤‘ ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§

4. **ëª¨ë¸ í‰ê°€ ë° ë¹„êµ**
   - ëª¨ë“  ëª¨ë¸ì˜ dev set ì„±ëŠ¥ ì¸¡ì •
   - ROUGE ì ìˆ˜ ë¹„êµ ë¶„ì„
   - ìƒì„± í’ˆì§ˆ ì •ì„± í‰ê°€

### ì¤‘ê¸° ëª©í‘œ (4-8ì‹œê°„)
5. **ì•™ìƒë¸” ì „ëµ ìˆ˜ë¦½**
   - ìµœê³  ì„±ëŠ¥ 2-3ê°œ ëª¨ë¸ ì„ ì •
   - ì•™ìƒë¸” ë°©ë²•ë¡  ê²€í† 
   - í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ìƒì„±

6. **ì œì¶œ ì¤€ë¹„**
   - submission.csv ìƒì„±
   - í˜•ì‹ ê²€ì¦
   - ìµœì¢… ì œì¶œ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸ í™•ì¸

### ì¥ê¸° ëª©í‘œ (1-2ì¼)
7. **ì¶”ê°€ ìµœì í™”**
   - í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì •
   - Prompt engineering ê°œì„ 
   - Inference íŒŒë¼ë¯¸í„° íŠœë‹ (temperature, top_p)

8. **ë¬¸ì„œí™” ë° ë°±ì—…**
   - ìµœì¢… ì‹¤í—˜ ê²°ê³¼ ì •ë¦¬
   - GitHub ì €ì¥ì†Œ ë™ê¸°í™”
   - ì¬í˜„ ê°€ëŠ¥ì„± í™•ë³´

---

## ğŸ“ Lessons Learned

### ê¸°ìˆ ì  êµí›ˆ
1. **Chat Template í† í° ì¤‘ìš”ì„±**
   - Instruction-tuned ëª¨ë¸ì€ chat template í† í°ì´ í•„ìˆ˜
   - ëˆ„ë½ ì‹œ í•™ìŠµ ë¶ˆì•ˆì • ë° ì„±ëŠ¥ ì €í•˜

2. **Dtype ì¼ê´€ì„±**
   - ëª¨ë¸ dtypeê³¼ compute dtype ë¶ˆì¼ì¹˜ ì‹œ ì„±ëŠ¥ ì €í•˜
   - BF16 ì§€ì› í•˜ë“œì›¨ì–´ì—ì„œëŠ” BF16 í†µì¼ ê¶Œì¥

3. **Metric Configuration**
   - Hugging Face Trainerì˜ metric ì´ë¦„ ê·œì¹™ ì¤€ìˆ˜ í•„ìš”
   - `rouge-1` âŒ â†’ `rouge1` âœ…

### í”„ë¡œì„¸ìŠ¤ ê°œì„ 
1. **ì²´ê³„ì ì¸ ë²„ê·¸ ì¶”ì **
   - ì—ëŸ¬ ë°œìƒ ì‹œ ì¦‰ì‹œ ë¬¸ì„œí™”
   - í•´ê²° ë°©ë²• ê³µìœ  ë° ì¬ì‚¬ìš©

2. **ì‹¤í—˜ ë¡œê¹…**
   - W&B í™œìš©ìœ¼ë¡œ ëª¨ë“  ì‹¤í—˜ ì¶”ì 
   - ì¬í˜„ ê°€ëŠ¥í•œ ì„¤ì • ê´€ë¦¬

3. **ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§**
   - ë””ìŠ¤í¬/ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì§€ì† í™•ì¸
   - ì‚¬ì „ ì˜ˆë°©ì  ê´€ë¦¬

---

## ğŸ”— Resources

### ì½”ë“œ ë° ë°ì´í„°
- **í”„ë¡œì íŠ¸ ë£¨íŠ¸**: `/Competition/NLP/naturallanguageprocessingcompetition-nlp-1/dialogue-summarization/`
- **í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸**: `scripts/llm_finetuning/`
- **ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸**: `training/`
- **ì˜ˆì¸¡ ê²°ê³¼**: `predictions/`

### ì™¸ë¶€ ë§í¬
- **W&B Dashboard**: https://wandb.ai/bkan-ai/dialogue-summarization-finetuning
- **GitHub Repo**: https://github.com/Bkankim/naturallanguageprocessingcompetition-nlp-1.git
- **Competition Platform**: [ëŒ€íšŒ ë§í¬]

### ë¬¸ì„œ
- **CLAUDE.md**: í”„ë¡œì íŠ¸ ê°€ì´ë“œë¼ì¸
- **README.md**: í”„ë¡œì íŠ¸ ê°œìš”
- **docs/**: ëŒ€íšŒ ê·œì¹™ ë° ê¸°ìˆ  ë¬¸ì„œ

---

## ğŸ“ Contact & Support

**í”„ë¡œì íŠ¸ ë‹´ë‹¹**: Claude Code Agent
**ì‘ì„±ì¼**: 2025-10-04 14:43 KST
**ìµœì¢… ì—…ë°ì´íŠ¸**: ì§„í–‰ ì¤‘ (ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸)

---

**Note**: ì´ ë³´ê³ ì„œëŠ” Llama-3.2-Korean-3B í•™ìŠµì´ ì™„ë£Œë˜ë©´ ì—…ë°ì´íŠ¸ë  ì˜ˆì •ì…ë‹ˆë‹¤. ìµœì‹  ì •ë³´ëŠ” W&B ëŒ€ì‹œë³´ë“œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.