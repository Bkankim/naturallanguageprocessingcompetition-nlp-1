model_name,nickname,rouge-1,rouge-2,rouge-l,rouge_sum,status
Qwen/Qwen2.5-7B-Instruct,Qwen2.5-7B,0.0,0.0,0.0,0.0,"failed: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
Bllossom/llama-3-Korean-Bllossom-8B,Llama-3-Korean-8B,0.0,0.0,0.0,0.0,"failed: Bllossom/llama-3-Korean-Bllossom-8B is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`"
Bllossom/llama-3.2-Korean-Bllossom-3B,Llama-3.2-Korean-3B,0.0,0.0,0.0,0.0,"failed: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
upstage/SOLAR-10.7B-Instruct-v1.0,SOLAR-10.7B,0.0,0.0,0.0,0.0,"failed: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
